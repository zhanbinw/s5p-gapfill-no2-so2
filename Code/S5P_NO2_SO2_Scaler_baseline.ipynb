{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNPb8O98uvdyGtaple+ORR0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6nQ97sYJYT7","executionInfo":{"status":"ok","timestamp":1758162212537,"user_tz":-120,"elapsed":8679,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"5c324674-5685-40d1-ca4a-2b24e726e565"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# --- 1. å®‰è£…ä¾èµ– ---\n","!pip install xarray netCDF4 matplotlib geopandas rasterio rioxarray --quiet\n","\n","# --- 2. æŒ‚è½½Google Drive ---\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# --- 3. å¯¼å…¥åº“ ---\n","import os\n","import numpy as np\n","import pandas as pd\n","import xarray as xr\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","source":["# 1. Verification: NO2 and SO2 File Structure Check"],"metadata":{"id":"OVIKlerodYIA"}},{"cell_type":"code","source":["# Complete Verification: NO2 and SO2 File Structure Check\n","import os, numpy as np, glob\n","\n","def check_file_structures():\n","    \"\"\"Complete verification of NO2 and SO2 file structures\"\"\"\n","    print(\"ğŸ” Complete File Structure Verification\")\n","    print(\"=\"*60)\n","\n","    base_path = \"/content/drive/MyDrive/Feature_Stacks\"\n","\n","    # Check NO2 structure\n","    print(\"\\nğŸ“ Checking NO2 File Structure:\")\n","    print(\"-\" * 40)\n","\n","    no2_dir = os.path.join(base_path, \"NO2_2019\")\n","    if not os.path.exists(no2_dir):\n","        print(f\"âŒ NO2 directory not found: {no2_dir}\")\n","        return\n","\n","    no2_files = sorted(glob.glob(os.path.join(no2_dir, \"NO2_stack_*.npz\")))\n","    if len(no2_files) == 0:\n","        print(f\"âŒ No NO2 files found in: {no2_dir}\")\n","        return\n","\n","    print(f\"âœ… Found {len(no2_files)} NO2 files\")\n","\n","    # Check first NO2 file\n","    no2_file = no2_files[0]\n","    print(f\"ğŸ“„ Analyzing: {os.path.basename(no2_file)}\")\n","\n","    try:\n","        with np.load(no2_file, allow_pickle=True) as data:\n","            print(f\" NO2 file keys: {list(data.files)}\")\n","\n","            # Check for X matrix\n","            if 'X' in data.files:\n","                X = data['X']\n","                print(f\"âœ… NO2 has X matrix: shape={X.shape}\")\n","            else:\n","                print(f\"âŒ NO2 missing X matrix\")\n","                print(f\"   Available keys: {[k for k in data.files if k not in ['coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']]}\")\n","\n","            # Check for target and mask\n","            if 'no2_target' in data.files:\n","                target = data['no2_target']\n","                print(f\"âœ… NO2 has target: shape={target.shape}\")\n","            else:\n","                print(f\"âŒ NO2 missing target\")\n","\n","            if 'no2_mask' in data.files:\n","                mask = data['no2_mask']\n","                print(f\"âœ… NO2 has mask: shape={mask.shape}\")\n","            else:\n","                print(f\"âŒ NO2 missing mask\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error loading NO2 file: {e}\")\n","\n","    # Check SO2 structure\n","    print(\"\\nğŸ“ Checking SO2 File Structure:\")\n","    print(\"-\" * 40)\n","\n","    so2_dir = os.path.join(base_path, \"SO2_2019\")\n","    if not os.path.exists(so2_dir):\n","        print(f\"âŒ SO2 directory not found: {so2_dir}\")\n","        return\n","\n","    so2_files = sorted(glob.glob(os.path.join(so2_dir, \"SO2_stack_*.npz\")))\n","    if len(so2_files) == 0:\n","        print(f\"âŒ No SO2 files found in: {so2_dir}\")\n","        return\n","\n","    print(f\"âœ… Found {len(so2_files)} SO2 files\")\n","\n","    # Check first SO2 file\n","    so2_file = so2_files[0]\n","    print(f\"ğŸ“„ Analyzing: {os.path.basename(so2_file)}\")\n","\n","    try:\n","        with np.load(so2_file, allow_pickle=True) as data:\n","            print(f\" SO2 file keys: {list(data.files)}\")\n","\n","            # Check for X matrix\n","            if 'X' in data.files:\n","                X = data['X']\n","                print(f\"âœ… SO2 has X matrix: shape={X.shape}\")\n","            else:\n","                print(f\"âŒ SO2 missing X matrix\")\n","\n","            # Check for target and mask\n","            if 'y' in data.files:\n","                target = data['y']\n","                print(f\"âœ… SO2 has target: shape={target.shape}\")\n","            else:\n","                print(f\"âŒ SO2 missing target\")\n","\n","            if 'mask' in data.files:\n","                mask = data['mask']\n","                print(f\"âœ… SO2 has mask: shape={mask.shape}\")\n","            else:\n","                print(f\"âŒ SO2 missing mask\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error loading SO2 file: {e}\")\n","\n","    # Summary\n","    print(\"\\nğŸ“‹ Summary:\")\n","    print(\"-\" * 40)\n","    print(\"File structure comparison:\")\n","    print(\"  NO2: Dictionary structure (no X matrix)\")\n","    print(\"  SO2: Matrix structure (has X matrix)\")\n","    print(\"\\nRecommendation:\")\n","    print(\"  Need to modify NO2 scaler to handle dictionary structure\")\n","\n","def check_no2_feature_availability():\n","    \"\"\"Check if NO2 has all required features for scaler\"\"\"\n","    print(\"\\nğŸ” Checking NO2 Feature Availability:\")\n","    print(\"-\" * 40)\n","\n","    base_path = \"/content/drive/MyDrive/Feature_Stacks\"\n","    no2_dir = os.path.join(base_path, \"NO2_2019\")\n","    no2_files = sorted(glob.glob(os.path.join(no2_dir, \"NO2_stack_*.npz\")))\n","\n","    if len(no2_files) == 0:\n","        print(\"âŒ No NO2 files found\")\n","        return\n","\n","    # Required features for scaler\n","    required_features = [\n","        'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","        'no2_lag_1day', 'no2_neighbor',\n","        'dem', 'slope', 'pop'\n","    ]\n","\n","    try:\n","        with np.load(no2_files[0], allow_pickle=True) as data:\n","            available_keys = list(data.files)\n","            print(f\"ğŸ“Š Available features: {len(available_keys)}\")\n","\n","            missing_features = []\n","            available_features = []\n","\n","            for feature in required_features:\n","                if feature in available_keys:\n","                    available_features.append(feature)\n","                else:\n","                    missing_features.append(feature)\n","\n","            print(f\"âœ… Available required features: {len(available_features)}\")\n","            print(f\"   {available_features}\")\n","\n","            if missing_features:\n","                print(f\"âŒ Missing required features: {len(missing_features)}\")\n","                print(f\"   {missing_features}\")\n","            else:\n","                print(\"âœ… All required features available for scaler\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error checking features: {e}\")\n","\n","def suggest_no2_scaler_fix():\n","    \"\"\"Suggest how to fix NO2 scaler for dictionary structure\"\"\"\n","    print(\"\\nğŸ› ï¸ Suggested NO2 Scaler Fix:\")\n","    print(\"-\" * 40)\n","\n","    print(\"Current NO2 scaler expects:\")\n","    print(\"  - data['X'] (matrix structure)\")\n","    print(\"  - data['mask'] (unified mask)\")\n","\n","    print(\"\\nActual NO2 structure:\")\n","    print(\"  - Dictionary of individual features\")\n","    print(\"  - data['no2_mask'] (specific mask)\")\n","\n","    print(\"\\nRequired changes:\")\n","    print(\"1. Build X matrix from dictionary features\")\n","    print(\"2. Use data['no2_mask'] instead of data['mask']\")\n","    print(\"3. Handle feature ordering manually\")\n","\n","    print(\"\\nCode changes needed:\")\n","    print(\"\"\"\n","    # Instead of:\n","    X = data['X']\n","    valid_mask = data['mask'] == 1\n","\n","    # Use:\n","    feature_order = ['dem', 'slope', 'pop', 'u10', 'v10', ...]\n","    X = np.stack([data[name] for name in feature_order], axis=0)\n","    valid_mask = data['no2_mask'] == 1\n","    \"\"\")\n","\n","# Run all checks\n","print(\" Starting Complete File Structure Verification\")\n","print(\"=\"*80)\n","\n","check_file_structures()\n","check_no2_feature_availability()\n","suggest_no2_scaler_fix()\n","\n","print(\"\\n Next Steps:\")\n","print(\"1. If NO2 missing X matrix â†’ Modify NO2 scaler code\")\n","print(\"2. If NO2 has X matrix â†’ Run scaler directly\")\n","print(\"3. Ensure mask semantics are correct (1=valid, 0=invalid)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXj1uPXDVVHp","executionInfo":{"status":"ok","timestamp":1758057610887,"user_tz":-120,"elapsed":196,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"6b5a10ab-157b-4484-fbad-3f3ad2d9d625"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Starting Complete File Structure Verification\n","================================================================================\n","ğŸ” Complete File Structure Verification\n","============================================================\n","\n","ğŸ“ Checking NO2 File Structure:\n","----------------------------------------\n","âœ… Found 365 NO2 files\n","ğŸ“„ Analyzing: NO2_stack_20190101.npz\n"," NO2 file keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","âŒ NO2 missing X matrix\n","   Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor']\n","âœ… NO2 has target: shape=(300, 621)\n","âœ… NO2 has mask: shape=(300, 621)\n","\n","ğŸ“ Checking SO2 File Structure:\n","----------------------------------------\n","âœ… Found 365 SO2 files\n","ğŸ“„ Analyzing: SO2_stack_20190101.npz\n"," SO2 file keys: ['X', 'y', 'mask', 'feature_names', 'cont_idx', 'onehot_idx', 'noscale_idx', 'coverage', 'trainable', 'pollutant', 'season', 'date', 'doy', 'weekday', 'year_len', 'grid_height', 'grid_width', 'lag1_fill_ratio', 'neighbor_fill_ratio', 'file_version']\n","âœ… SO2 has X matrix: shape=(30, 300, 621)\n","âœ… SO2 has target: shape=(300, 621)\n","âœ… SO2 has mask: shape=(300, 621)\n","\n","ğŸ“‹ Summary:\n","----------------------------------------\n","File structure comparison:\n","  NO2: Dictionary structure (no X matrix)\n","  SO2: Matrix structure (has X matrix)\n","\n","Recommendation:\n","  Need to modify NO2 scaler to handle dictionary structure\n","\n","ğŸ” Checking NO2 Feature Availability:\n","----------------------------------------\n","ğŸ“Š Available features: 37\n","âœ… Available required features: 13\n","   ['u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'no2_lag_1day', 'no2_neighbor', 'dem', 'slope', 'pop']\n","âœ… All required features available for scaler\n","\n","ğŸ› ï¸ Suggested NO2 Scaler Fix:\n","----------------------------------------\n","Current NO2 scaler expects:\n","  - data['X'] (matrix structure)\n","  - data['mask'] (unified mask)\n","\n","Actual NO2 structure:\n","  - Dictionary of individual features\n","  - data['no2_mask'] (specific mask)\n","\n","Required changes:\n","1. Build X matrix from dictionary features\n","2. Use data['no2_mask'] instead of data['mask']\n","3. Handle feature ordering manually\n","\n","Code changes needed:\n","\n","    # Instead of:\n","    X = data['X']\n","    valid_mask = data['mask'] == 1\n","    \n","    # Use:\n","    feature_order = ['dem', 'slope', 'pop', 'u10', 'v10', ...]\n","    X = np.stack([data[name] for name in feature_order], axis=0)\n","    valid_mask = data['no2_mask'] == 1\n","    \n","\n"," Next Steps:\n","1. If NO2 missing X matrix â†’ Modify NO2 scaler code\n","2. If NO2 has X matrix â†’ Run scaler directly\n","3. Ensure mask semantics are correct (1=valid, 0=invalid)\n"]}]},{"cell_type":"markdown","source":["# 2. Generate NO2 standardization parameters"],"metadata":{"id":"pSN8d8lLddkI"}},{"cell_type":"code","source":["# Generate NO2 standardization parameters (FINAL VERIFIED VERSION)\n","import os, numpy as np\n","from collections import defaultdict\n","\n","# Create output directory\n","os.makedirs(\"/content/drive/MyDrive/Scalers\", exist_ok=True)\n","\n","BASE_NO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","OUT_DIR = \"/content/drive/MyDrive/Scalers\"\n","\n","# Continuous features that need standardization (NO2 version)\n","CONTINUOUS_FEATURES = [\n","    'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","    'no2_lag_1day', 'no2_neighbor',  # NO2 related features\n","    'dem', 'slope', 'pop',\n","    'ws'  # Wind speed (if not already standardized)\n","]\n","\n","# Features that should not be standardized\n","NON_STANDARDIZED = ['sin_doy', 'cos_doy', 'weekday_weight', 'wd_sin', 'wd_cos']\n","\n","# Categorical features (one-hot encoded)\n","CATEGORICAL_FEATURES = [\n","    'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4',\n","    'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9'\n","]\n","\n","def compute_no2_scalers(years=[2019, 2020, 2021], sample_per_file=1000):\n","    \"\"\"Compute standardization parameters for NO2\"\"\"\n","    print(\"ğŸš€ Starting NO2 standardization parameter computation...\")\n","    print(f\"Data path: {BASE_NO2}\")\n","    print(f\"Training years: {years}\")\n","    print(f\"Samples per file: {sample_per_file}\")\n","\n","    # Accumulate statistics\n","    stats = defaultdict(lambda: {'sum': 0.0, 'sumsq': 0.0, 'count': 0})\n","    feature_order = None\n","\n","    total_files = 0\n","    processed_files = 0\n","\n","    for year in years:\n","        year_dir = os.path.join(BASE_NO2, f\"NO2_{year}\")\n","        if not os.path.exists(year_dir):\n","            print(f\"âš ï¸ Year directory not found: {year_dir}\")\n","            continue\n","\n","        files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","        total_files += len(files)\n","        print(f\"\\n Processing {year}: {len(files)} files\")\n","\n","        for i, fname in enumerate(files):\n","            if i % 50 == 0:\n","                print(f\"  {i}/{len(files)} ({i/len(files)*100:.1f}%)\")\n","\n","            try:\n","                with np.load(os.path.join(year_dir, fname)) as data:\n","                    # NO2 uses dictionary structure, need to build X matrix\n","                    if feature_order is None:\n","                        # Define feature order for NO2 (based on verification results)\n","                        feature_order = [\n","                            'dem', 'slope', 'pop',\n","                            'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4',\n","                            'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9',\n","                            'sin_doy', 'cos_doy', 'weekday_weight',\n","                            'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","                            'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor'\n","                        ]\n","\n","                    # Build X matrix from dictionary features with robustness checks\n","                    # Strategy: Fill missing non-critical features with appropriate defaults\n","                    X_list = []\n","                    missing_critical = False\n","\n","                    for name in feature_order:\n","                        if name in data.files:\n","                            X_list.append(data[name].astype(np.float32))\n","                        else:\n","                            print(f\"   âš ï¸ Missing feature: {name} in {fname}\")\n","                            if name in CONTINUOUS_FEATURES:\n","                                # Critical feature missing - skip this file\n","                                print(f\"   âŒ Critical continuous feature missing, skipping file\")\n","                                missing_critical = True\n","                                break\n","                            else:\n","                                # Non-critical feature missing - fill with appropriate default\n","                                if name.startswith('lulc_class_'):\n","                                    fill_value = 0.0  # One-hot encoded, 0 is appropriate for missing class\n","                                elif name in ['sin_doy', 'cos_doy']:\n","                                    fill_value = 0.0  # Default to 0 for missing temporal features\n","                                elif name == 'weekday_weight':\n","                                    fill_value = 1.0  # Default to weekday weight\n","                                elif name in ['wd_sin', 'wd_cos']:\n","                                    fill_value = 0.0  # Default wind direction components\n","                                else:\n","                                    fill_value = 0.0  # Default fallback\n","\n","                                print(f\"   â„¹ï¸ Filling missing non-critical feature {name} with {fill_value}\")\n","                                # Get dimensions from first available feature\n","                                if X_list:\n","                                    H, W = X_list[0].shape\n","                                else:\n","                                    # Fallback dimensions (should not happen in practice)\n","                                    H, W = 300, 621\n","                                X_list.append(np.full((H, W), fill_value, dtype=np.float32))\n","\n","                    if not missing_critical:\n","                        # All features handled (either available or filled), build X matrix\n","                        X = np.stack(X_list, axis=0).astype(np.float32)  # (C, H, W)\n","\n","                        # CONFIRMED: NO2: mask==1 is valid pixels, mask==0 is invalid\n","                        # Additional robustness: ensure target > 0 for physical validity\n","                        valid_mask = (data['no2_mask'] == 1) & (data['no2_target'] > 0)\n","\n","                        # --- å…³é”®ä¿®æ­£ï¼šæ­£ç¡®å¤„ç†2Dæ©è†œç´¢å¼• ---\n","                        C, H, W = X.shape\n","                        X_flat = X.reshape(C, -1)  # (C, H*W) - æ‰“å¹³ç©ºé—´ç»´åº¦\n","                        valid_flat = valid_mask.ravel()  # (H*W,) - æ‰“å¹³æ©è†œ\n","                        valid_pixels = X_flat[:, valid_flat]  # (C, N_valid) - æ­£ç¡®æå–æœ‰æ•ˆåƒç´ \n","\n","                        if valid_pixels.shape[1] == 0:\n","                            continue\n","\n","                        # Random sampling to avoid memory issues\n","                        n_samples = min(sample_per_file, valid_pixels.shape[1])\n","                        if n_samples < valid_pixels.shape[1]:\n","                            indices = np.random.choice(valid_pixels.shape[1], n_samples, replace=False)\n","                            sampled = valid_pixels[:, indices]\n","                        else:\n","                            sampled = valid_pixels\n","\n","                        # Filter NaN/inf\n","                        finite_mask = np.isfinite(sampled)\n","\n","                        # Accumulate statistics\n","                        for j, feat_name in enumerate(feature_order):\n","                            if feat_name in CONTINUOUS_FEATURES:\n","                                values = sampled[j]\n","                                finite_values = values[finite_mask[j]]\n","\n","                                if len(finite_values) > 0:\n","                                    # 1-99 percentile winsorization\n","                                    q1, q99 = np.percentile(finite_values, [1, 99])\n","                                    clipped = np.clip(finite_values, q1, q99)\n","\n","                                    stats[feat_name]['sum'] += np.sum(clipped)\n","                                    stats[feat_name]['sumsq'] += np.sum(clipped**2)\n","                                    stats[feat_name]['count'] += len(clipped)\n","                            elif feat_name in CATEGORICAL_FEATURES:\n","                                # Categorical features don't need standardization\n","                                pass\n","                            elif feat_name in NON_STANDARDIZED:\n","                                # Non-standardized features don't need standardization\n","                                pass\n","\n","                        processed_files += 1\n","\n","            except Exception as e:\n","                print(f\"âŒ Failed to load file {fname}: {e}\")\n","                continue\n","\n","    print(f\"\\nğŸ“Š Processing statistics:\")\n","    print(f\"Total files: {total_files}\")\n","    print(f\"Successfully processed: {processed_files}\")\n","    print(f\"Processing rate: {processed_files/total_files*100:.1f}%\")\n","\n","    if not stats:\n","        print(\"âŒ No valid samples found\")\n","        return None\n","\n","    # Compute final statistics\n","    scalers = {}\n","    print(f\"\\nğŸ”¢ Computing standardization parameters...\")\n","\n","    for feat_name in feature_order:\n","        if feat_name in CONTINUOUS_FEATURES:\n","            # è¿ç»­ç‰¹å¾ï¼šç»Ÿä¸€å¤„ç†ï¼Œå³ä½¿æ²¡æœ‰ç»Ÿè®¡åˆ°ä¹Ÿç»™é»˜è®¤å€¼\n","            if feat_name in stats:\n","                count = stats[feat_name]['count']\n","                if count > 0:\n","                    mean = stats[feat_name]['sum'] / count\n","                    variance = (stats[feat_name]['sumsq'] / count) - (mean**2)\n","                    std = np.sqrt(max(variance, 1e-8))  # Avoid division by zero\n","\n","                    scalers[feat_name] = {\n","                        'mean': float(mean),\n","                        'std': float(std),\n","                        'count': int(count)\n","                    }\n","                    print(f\"  {feat_name}: mean={mean:.4f}, std={std:.4f}, count={count}\")\n","                else:\n","                    # æœ‰ç»Ÿè®¡ä½†count=0ï¼Œç»™é»˜è®¤å€¼\n","                    scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                    print(f\"  {feat_name}: No valid samples, using default values\")\n","            else:\n","                # å®Œå…¨æ²¡æœ‰ç»Ÿè®¡åˆ°ï¼Œä¹Ÿç»™é»˜è®¤å€¼\n","                scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                print(f\"  {feat_name}: No statistics collected, using default values\")\n","        elif feat_name in CATEGORICAL_FEATURES:\n","            scalers[feat_name] = {'type': 'categorical'}\n","            print(f\"  {feat_name}: categorical (one-hot)\")\n","        elif feat_name in NON_STANDARDIZED:\n","            scalers[feat_name] = {'type': 'non_standardized'}\n","            print(f\"  {feat_name}: non_standardized\")\n","        else:\n","            # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºfeature_orderåº”è¯¥åŒ…å«æ‰€æœ‰ç‰¹å¾\n","            scalers[feat_name] = {'type': 'unexpected'}\n","            print(f\"  {feat_name}: unexpected type - check feature_order definition\")\n","\n","    return scalers, feature_order\n","\n","def apply_no2_scaler(X: np.ndarray, feature_names: list, scaler_file: str = None):\n","    \"\"\"\n","    Apply NO2 standardization to feature matrix\n","\n","    Args:\n","        X: Feature matrix (C, H, W) or (C, N)\n","        feature_names: List of feature names\n","        scaler_file: Path to scaler file (if None, auto-detect)\n","\n","    Returns:\n","        X_scaled: Standardized feature matrix\n","    \"\"\"\n","    if scaler_file is None:\n","        scaler_file = os.path.join(OUT_DIR, \"no2_scalers_2019_2021.npz\")\n","\n","    if not os.path.exists(scaler_file):\n","        raise FileNotFoundError(f\"NO2 scaler file not found: {scaler_file}\")\n","\n","    with np.load(scaler_file, allow_pickle=True) as data:\n","        scalers = data['scalers'].item()\n","\n","    X_scaled = X.copy().astype(np.float32)\n","\n","    for i, feat_name in enumerate(feature_names):\n","        if feat_name in scalers and 'mean' in scalers[feat_name]:\n","            # Apply z-score normalization for continuous features\n","            mean = scalers[feat_name]['mean']\n","            std = scalers[feat_name]['std']\n","            X_scaled[i] = (X[i] - mean) / std\n","        # Skip categorical and non-standardized features (they remain unchanged)\n","\n","    return X_scaled\n","\n","# Compute NO2 standardization parameters\n","print(\"=\" * 60)\n","print(\" NO2 Standardization Parameter Computation (FINAL VERIFIED)\")\n","print(\"=\" * 60)\n","\n","no2_scalers, no2_features = compute_no2_scalers()\n","\n","if no2_scalers is not None:\n","    # Save results\n","    output_file = os.path.join(OUT_DIR, \"no2_scalers_2019_2021.npz\")\n","    np.savez_compressed(\n","        output_file,\n","        scalers=no2_scalers,\n","        feature_order=no2_features,\n","        metadata={\n","            'data_type': 'no2',\n","            'years': [2019, 2020, 2021],\n","            'mask_logic': 'mask==1 is valid, mask==0 is invalid (CONFIRMED FROM SOURCE CODE)',\n","            'total_features': len(no2_features),\n","            'continuous_features': len([f for f in no2_features if f in CONTINUOUS_FEATURES])\n","        }\n","    )\n","\n","    print(f\"\\nâœ… NO2 standardization parameters saved to: {output_file}\")\n","    print(f\" Feature statistics:\")\n","    print(f\"  Total features: {len(no2_features)}\")\n","    print(f\"  Continuous features: {len([f for f in no2_features if f in CONTINUOUS_FEATURES])}\")\n","    print(f\"  Categorical features: {len([f for f in no2_features if f in CATEGORICAL_FEATURES])}\")\n","    print(f\"  Non-standardized features: {len([f for f in no2_features if f in NON_STANDARDIZED])}\")\n","    # Show parameters for first few continuous features\n","    print(f\"\\nğŸ” Standardization parameters for first 5 continuous features:\")\n","    continuous_count = 0\n","    for feat_name in no2_features:\n","        if feat_name in CONTINUOUS_FEATURES and feat_name in no2_scalers:\n","            if 'mean' in no2_scalers[feat_name]:\n","                print(f\"  {feat_name}: mean={no2_scalers[feat_name]['mean']:.4f}, std={no2_scalers[feat_name]['std']:.4f}\")\n","                continuous_count += 1\n","                if continuous_count >= 5:\n","                    break\n","else:\n","    print(\"âŒ Computation failed\")\n","\n","print(\"\\n Next step: Ready to train NO2 model!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Wbue9izZtux","executionInfo":{"status":"ok","timestamp":1758059594588,"user_tz":-120,"elapsed":680371,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"351a0c94-2d5c-4aa1-bc41-72bdd797812b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n"," NO2 Standardization Parameter Computation (FINAL VERIFIED)\n","============================================================\n","ğŸš€ Starting NO2 standardization parameter computation...\n","Data path: /content/drive/MyDrive/Feature_Stacks\n","Training years: [2019, 2020, 2021]\n","Samples per file: 1000\n","\n"," Processing 2019: 365 files\n","  0/365 (0.0%)\n","  50/365 (13.7%)\n","  100/365 (27.4%)\n","  150/365 (41.1%)\n","  200/365 (54.8%)\n","  250/365 (68.5%)\n","  300/365 (82.2%)\n","  350/365 (95.9%)\n","\n"," Processing 2020: 366 files\n","  0/366 (0.0%)\n","  50/366 (13.7%)\n","  100/366 (27.3%)\n","  150/366 (41.0%)\n","  200/366 (54.6%)\n","  250/366 (68.3%)\n","  300/366 (82.0%)\n","  350/366 (95.6%)\n","\n"," Processing 2021: 365 files\n","  0/365 (0.0%)\n","  50/365 (13.7%)\n","  100/365 (27.4%)\n","  150/365 (41.1%)\n","  200/365 (54.8%)\n","  250/365 (68.5%)\n","  300/365 (82.2%)\n","  350/365 (95.9%)\n","\n","ğŸ“Š Processing statistics:\n","Total files: 1096\n","Successfully processed: 1071\n","Processing rate: 97.7%\n","\n","ğŸ”¢ Computing standardization parameters...\n","  dem: mean=606.2273, std=771.7322, count=1052695\n","  slope: mean=11.3112, std=11.0288, count=994379\n","  pop: mean=135.9125, std=369.5149, count=992667\n","  lulc_class_0: categorical (one-hot)\n","  lulc_class_1: categorical (one-hot)\n","  lulc_class_2: categorical (one-hot)\n","  lulc_class_3: categorical (one-hot)\n","  lulc_class_4: categorical (one-hot)\n","  lulc_class_5: categorical (one-hot)\n","  lulc_class_6: categorical (one-hot)\n","  lulc_class_7: categorical (one-hot)\n","  lulc_class_8: categorical (one-hot)\n","  lulc_class_9: categorical (one-hot)\n","  sin_doy: non_standardized\n","  cos_doy: non_standardized\n","  weekday_weight: non_standardized\n","  u10: mean=-0.2344, std=1.5692, count=1052695\n","  v10: mean=0.1392, std=1.4423, count=1052695\n","  blh: mean=951.2973, std=558.4578, count=1052695\n","  tp: mean=0.0003, std=0.0009, count=1052695\n","  t2m: mean=15.8182, std=9.8316, count=1052695\n","  sp: No statistics collected, using default values\n","  str: mean=-29437.2015, std=32870.4722, count=1631\n","  ssr_clr: No statistics collected, using default values\n","  ws: mean=1.7724, std=1.2206, count=1052695\n","  wd_sin: non_standardized\n","  wd_cos: non_standardized\n","  no2_lag_1day: mean=0.0000, std=0.0001, count=1033884\n","  no2_neighbor: mean=0.0001, std=0.0001, count=1052695\n","\n","âœ… NO2 standardization parameters saved to: /content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\n"," Feature statistics:\n","  Total features: 29\n","  Continuous features: 14\n","  Categorical features: 10\n","  Non-standardized features: 5\n","\n","ğŸ” Standardization parameters for first 5 continuous features:\n","  dem: mean=606.2273, std=771.7322\n","  slope: mean=11.3112, std=11.0288\n","  pop: mean=135.9125, std=369.5149\n","  u10: mean=-0.2344, std=1.5692\n","  v10: mean=0.1392, std=1.4423\n","\n"," Next step: Ready to train NO2 model!\n"]}]},{"cell_type":"code","source":["# Quick Check NO2 Scaler Results\n","import os\n","import numpy as np\n","\n","def quick_check_no2_scaler_results():\n","    \"\"\"å¿«é€Ÿæ£€æŸ¥NO2 scalerç»“æœ\"\"\"\n","    print(\"ğŸ” Quick NO2 Scaler Results Check\")\n","    print(\"=\" * 40)\n","\n","    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n","    scaler_file = \"/content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(f\"âŒ Scaler file not found: {scaler_file}\")\n","        return False\n","\n","    print(f\"âœ… Scaler file exists\")\n","\n","    # 2. åŠ è½½å¹¶æ£€æŸ¥åŸºæœ¬å†…å®¹\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            scalers = data['scalers'].item()\n","            feature_order = data['feature_order']\n","            metadata = data['metadata'].item()\n","\n","        print(f\"âœ… File loaded successfully\")\n","        print(f\"ğŸ“Š Total features: {len(feature_order)}\")\n","        print(f\"ğŸ“Š Years: {metadata.get('years', 'unknown')}\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Failed to load: {e}\")\n","        return False\n","\n","    # 3. æ£€æŸ¥å…³é”®è¿ç»­ç‰¹å¾\n","    expected_continuous = [\n","        'dem', 'slope', 'pop', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","        'ws', 'no2_lag_1day', 'no2_neighbor'\n","    ]\n","\n","    print(f\"\\nğŸ¯ Key Continuous Features Analysis:\")\n","    continuous_ok = 0\n","    low_sample_features = []\n","    zero_value_features = []\n","\n","    for feat in expected_continuous:\n","        if feat in scalers and 'mean' in scalers[feat]:\n","            count = scalers[feat]['count']\n","            mean = scalers[feat]['mean']\n","            std = scalers[feat]['std']\n","\n","            if count > 0:\n","                print(f\"  âœ… {feat}: {count:,} samples, mean={mean:.4f}, std={std:.4f}\")\n","                continuous_ok += 1\n","\n","                # æ£€æŸ¥å¼‚å¸¸æƒ…å†µ\n","                if count < 100000:  # æ ·æœ¬æ•°å¤ªå°‘\n","                    low_sample_features.append((feat, count))\n","                if abs(mean) < 0.001 and std < 0.001:  # å€¼å‡ ä¹ä¸º0\n","                    zero_value_features.append((feat, mean, std))\n","            else:\n","                print(f\"  âš ï¸ {feat}: default values (no samples)\")\n","        else:\n","            print(f\"  âŒ {feat}: missing\")\n","\n","    # 4. æ£€æŸ¥å…¶ä»–ç‰¹å¾ç±»å‹\n","    categorical_count = sum(1 for feat in feature_order\n","                           if feat in scalers and scalers[feat].get('type') == 'categorical')\n","    non_std_count = sum(1 for feat in feature_order\n","                       if feat in scalers and scalers[feat].get('type') == 'non_standardized')\n","\n","    print(f\"\\nğŸ“Š Feature Type Summary:\")\n","    print(f\"  Continuous features: {continuous_ok}/14\")\n","    print(f\"  Categorical features: {categorical_count}\")\n","    print(f\"  Non-standardized features: {non_std_count}\")\n","\n","    # 5. å¼‚å¸¸æƒ…å†µæŠ¥å‘Š\n","    if low_sample_features:\n","        print(f\"\\nâš ï¸ Features with low sample counts:\")\n","        for feat, count in low_sample_features:\n","            print(f\"  {feat}: {count:,} samples\")\n","\n","    if zero_value_features:\n","        print(f\"\\nâš ï¸ Features with near-zero values:\")\n","        for feat, mean, std in zero_value_features:\n","            print(f\"  {feat}: mean={mean:.6f}, std={std:.6f}\")\n","\n","    # 6. æ€»ä½“è¯„ä¼°\n","    success_rate = (continuous_ok / 14) * 100\n","    print(f\"\\nğŸ¯ Success Rate: {success_rate:.1f}%\")\n","\n","    if success_rate >= 80:\n","        print(f\"ğŸ‰ NO2 Scaler looks good!\")\n","        if low_sample_features or zero_value_features:\n","            print(f\"ğŸ’¡ Note: Some features have unusual values, but this might be normal for the dataset\")\n","        return True\n","    else:\n","        print(f\"âš ï¸ NO2 Scaler needs attention\")\n","        return False\n","\n","if __name__ == \"__main__\":\n","    quick_check_no2_scaler_results()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JwdLGyehm12","executionInfo":{"status":"ok","timestamp":1758059724542,"user_tz":-120,"elapsed":255,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"f33bf732-a954-4771-9e98-334cb0308e21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” Quick NO2 Scaler Results Check\n","========================================\n","âœ… Scaler file exists\n","âœ… File loaded successfully\n","ğŸ“Š Total features: 29\n","ğŸ“Š Years: [2019, 2020, 2021]\n","\n","ğŸ¯ Key Continuous Features Analysis:\n","  âœ… dem: 1,052,695 samples, mean=606.2273, std=771.7322\n","  âœ… slope: 994,379 samples, mean=11.3112, std=11.0288\n","  âœ… pop: 992,667 samples, mean=135.9125, std=369.5149\n","  âœ… u10: 1,052,695 samples, mean=-0.2344, std=1.5692\n","  âœ… v10: 1,052,695 samples, mean=0.1392, std=1.4423\n","  âœ… blh: 1,052,695 samples, mean=951.2973, std=558.4578\n","  âœ… tp: 1,052,695 samples, mean=0.0003, std=0.0009\n","  âœ… t2m: 1,052,695 samples, mean=15.8182, std=9.8316\n","  âš ï¸ sp: default values (no samples)\n","  âœ… str: 1,631 samples, mean=-29437.2015, std=32870.4722\n","  âš ï¸ ssr_clr: default values (no samples)\n","  âœ… ws: 1,052,695 samples, mean=1.7724, std=1.2206\n","  âœ… no2_lag_1day: 1,033,884 samples, mean=0.0000, std=0.0001\n","  âœ… no2_neighbor: 1,052,695 samples, mean=0.0001, std=0.0001\n","\n","ğŸ“Š Feature Type Summary:\n","  Continuous features: 12/14\n","  Categorical features: 10\n","  Non-standardized features: 5\n","\n","âš ï¸ Features with low sample counts:\n","  str: 1,631 samples\n","\n","âš ï¸ Features with near-zero values:\n","  tp: mean=0.000289, std=0.000872\n","  no2_lag_1day: mean=0.000004, std=0.000100\n","  no2_neighbor: mean=0.000055, std=0.000100\n","\n","ğŸ¯ Success Rate: 85.7%\n","ğŸ‰ NO2 Scaler looks good!\n","ğŸ’¡ Note: Some features have unusual values, but this might be normal for the dataset\n"]}]},{"cell_type":"code","source":["# Validate NO2 Scaler Results\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","\n","def validate_no2_scaler_results():\n","    \"\"\"éªŒè¯NO2 scalerè¿è¡Œç»“æœ\"\"\"\n","    print(\"ğŸ” NO2 Scaler Results Validation\")\n","    print(\"=\" * 50)\n","\n","    # 1. æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨\n","    scaler_file = \"/content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(f\"âŒ Scaler file not found: {scaler_file}\")\n","        return False\n","\n","    print(f\"âœ… Scaler file found: {scaler_file}\")\n","\n","    # 2. åŠ è½½å¹¶æ£€æŸ¥scalerå†…å®¹\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            scalers = data['scalers'].item()\n","            feature_order = data['feature_order']\n","            metadata = data['metadata'].item()\n","\n","        print(f\"âœ… Successfully loaded scaler data\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Failed to load scaler file: {e}\")\n","        return False\n","\n","    # 3. æ£€æŸ¥å…ƒæ•°æ®\n","    print(f\"\\nğŸ“Š Metadata:\")\n","    print(f\"  Data type: {metadata.get('data_type', 'unknown')}\")\n","    print(f\"  Years: {metadata.get('years', 'unknown')}\")\n","    print(f\"  Mask logic: {metadata.get('mask_logic', 'unknown')}\")\n","    print(f\"  Total features: {metadata.get('total_features', 'unknown')}\")\n","    print(f\"  Continuous features: {metadata.get('continuous_features', 'unknown')}\")\n","\n","    # 4. æ£€æŸ¥ç‰¹å¾é¡ºåº\n","    print(f\"\\nğŸ“‹ Feature Order ({len(feature_order)} features):\")\n","    for i, feat in enumerate(feature_order):\n","        print(f\"  {i+1:2d}. {feat}\")\n","\n","    # 5. æ£€æŸ¥scalerå‚æ•°\n","    print(f\"\\nğŸ”¢ Scaler Parameters:\")\n","\n","    continuous_count = 0\n","    categorical_count = 0\n","    non_standardized_count = 0\n","    default_count = 0\n","\n","    for feat_name in feature_order:\n","        if feat_name in scalers:\n","            scaler_info = scalers[feat_name]\n","\n","            if 'mean' in scaler_info:\n","                # è¿ç»­ç‰¹å¾\n","                mean = scaler_info['mean']\n","                std = scaler_info['std']\n","                count = scaler_info['count']\n","                continuous_count += 1\n","\n","                if count > 0:\n","                    print(f\"  âœ… {feat_name}: mean={mean:.4f}, std={std:.4f}, count={count:,}\")\n","                else:\n","                    print(f\"  âš ï¸ {feat_name}: default values (mean=0, std=1, count=0)\")\n","                    default_count += 1\n","\n","            elif scaler_info.get('type') == 'categorical':\n","                categorical_count += 1\n","                print(f\"  ğŸ“Š {feat_name}: categorical (one-hot)\")\n","\n","            elif scaler_info.get('type') == 'non_standardized':\n","                non_standardized_count += 1\n","                print(f\"  ğŸš« {feat_name}: non_standardized\")\n","\n","            else:\n","                print(f\"  â“ {feat_name}: {scaler_info}\")\n","\n","    # 6. ç»Ÿè®¡æ€»ç»“\n","    print(f\"\\nğŸ“ˆ Summary Statistics:\")\n","    print(f\"  Total features: {len(feature_order)}\")\n","    print(f\"  Continuous features: {continuous_count}\")\n","    print(f\"  Categorical features: {categorical_count}\")\n","    print(f\"  Non-standardized features: {non_standardized_count}\")\n","    print(f\"  Features with default values: {default_count}\")\n","\n","    # 7. éªŒè¯å…³é”®è¿ç»­ç‰¹å¾\n","    expected_continuous = [\n","        'dem', 'slope', 'pop', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","        'ws', 'no2_lag_1day', 'no2_neighbor'\n","    ]\n","\n","    print(f\"\\nğŸ¯ Key Continuous Features Validation:\")\n","    missing_features = []\n","    for feat in expected_continuous:\n","        if feat in scalers and 'mean' in scalers[feat]:\n","            count = scalers[feat]['count']\n","            if count > 0:\n","                print(f\"  âœ… {feat}: {count:,} samples\")\n","            else:\n","                print(f\"  âš ï¸ {feat}: default values (no samples)\")\n","                missing_features.append(feat)\n","        else:\n","            print(f\"  âŒ {feat}: not found in scalers\")\n","            missing_features.append(feat)\n","\n","    # 8. æ£€æŸ¥æ•°æ®è´¨é‡\n","    print(f\"\\nğŸ” Data Quality Check:\")\n","\n","    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬\n","    total_samples = sum(scalers[feat]['count'] for feat in scalers\n","                       if 'count' in scalers[feat] and scalers[feat]['count'] > 0)\n","\n","    if total_samples > 0:\n","        print(f\"  âœ… Total valid samples: {total_samples:,}\")\n","\n","        # æ£€æŸ¥æ ‡å‡†å·®æ˜¯å¦åˆç†\n","        extreme_std_features = []\n","        for feat in expected_continuous:\n","            if feat in scalers and 'std' in scalers[feat]:\n","                std = scalers[feat]['std']\n","                if std < 0.001 or std > 1000:\n","                    extreme_std_features.append((feat, std))\n","\n","        if extreme_std_features:\n","            print(f\"  âš ï¸ Features with extreme std values:\")\n","            for feat, std in extreme_std_features:\n","                print(f\"    {feat}: std={std:.6f}\")\n","        else:\n","            print(f\"  âœ… All std values are reasonable\")\n","\n","    else:\n","        print(f\"  âŒ No valid samples found!\")\n","        return False\n","\n","    # 9. æµ‹è¯•scaleråº”ç”¨\n","    print(f\"\\nğŸ§ª Testing Scaler Application:\")\n","\n","    try:\n","        # åˆ›å»ºæµ‹è¯•æ•°æ®\n","        test_X = np.random.randn(len(feature_order), 10, 10).astype(np.float32)\n","\n","        # åº”ç”¨scaler\n","        from Generate_NO2_standardization_parameter import apply_no2_scaler\n","        X_scaled = apply_no2_scaler(test_X, feature_order, scaler_file)\n","\n","        print(f\"  âœ… Scaler application test passed\")\n","        print(f\"  Input shape: {test_X.shape}\")\n","        print(f\"  Output shape: {X_scaled.shape}\")\n","\n","        # æ£€æŸ¥æ ‡å‡†åŒ–æ•ˆæœ\n","        for i, feat_name in enumerate(feature_order):\n","            if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                mean_scaled = np.mean(X_scaled[i])\n","                std_scaled = np.std(X_scaled[i])\n","                print(f\"  {feat_name}: scaled_mean={mean_scaled:.4f}, scaled_std={std_scaled:.4f}\")\n","                break  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªè¿ç»­ç‰¹å¾\n","\n","    except Exception as e:\n","        print(f\"  âŒ Scaler application test failed: {e}\")\n","        return False\n","\n","    # 10. æœ€ç»ˆè¯„ä¼°\n","    print(f\"\\nğŸ¯ Final Assessment:\")\n","\n","    success_criteria = [\n","        (continuous_count == 14, f\"14 continuous features processed\"),\n","        (categorical_count == 10, f\"10 categorical features identified\"),\n","        (non_standardized_count == 5, f\"5 non-standardized features identified\"),\n","        (total_samples > 100000, f\"Sufficient samples ({total_samples:,})\"),\n","        (len(missing_features) == 0, f\"No missing key features\"),\n","        (len(extreme_std_features) == 0, f\"No extreme std values\")\n","    ]\n","\n","    passed = 0\n","    for criterion, description in success_criteria:\n","        if criterion:\n","            print(f\"  âœ… {description}\")\n","            passed += 1\n","        else:\n","            print(f\"  âŒ {description}\")\n","\n","    success_rate = passed / len(success_criteria) * 100\n","    print(f\"\\nğŸ“Š Overall Success Rate: {success_rate:.1f}% ({passed}/{len(success_criteria)})\")\n","\n","    if success_rate >= 80:\n","        print(f\"ğŸ‰ NO2 Scaler validation PASSED!\")\n","        return True\n","    else:\n","        print(f\"âš ï¸ NO2 Scaler validation needs attention\")\n","        return False\n","\n","def visualize_scaler_statistics():\n","    \"\"\"å¯è§†åŒ–scalerç»Ÿè®¡ä¿¡æ¯\"\"\"\n","    print(f\"\\nğŸ“Š Visualizing Scaler Statistics...\")\n","\n","    scaler_file = \"/content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(f\"âŒ Scaler file not found for visualization\")\n","        return\n","\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            scalers = data['scalers'].item()\n","            feature_order = data['feature_order']\n","\n","        # æå–è¿ç»­ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯\n","        continuous_features = []\n","        means = []\n","        stds = []\n","        counts = []\n","\n","        for feat_name in feature_order:\n","            if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                continuous_features.append(feat_name)\n","                means.append(scalers[feat_name]['mean'])\n","                stds.append(scalers[feat_name]['std'])\n","                counts.append(scalers[feat_name]['count'])\n","\n","        if not continuous_features:\n","            print(f\"âŒ No continuous features found for visualization\")\n","            return\n","\n","        # åˆ›å»ºå›¾è¡¨\n","        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n","\n","        # 1. å‡å€¼åˆ†å¸ƒ\n","        ax1.bar(range(len(continuous_features)), means)\n","        ax1.set_title('Feature Means')\n","        ax1.set_xlabel('Features')\n","        ax1.set_ylabel('Mean Value')\n","        ax1.set_xticks(range(len(continuous_features)))\n","        ax1.set_xticklabels(continuous_features, rotation=45, ha='right')\n","\n","        # 2. æ ‡å‡†å·®åˆ†å¸ƒ\n","        ax2.bar(range(len(continuous_features)), stds)\n","        ax2.set_title('Feature Standard Deviations')\n","        ax2.set_xlabel('Features')\n","        ax2.set_ylabel('Standard Deviation')\n","        ax2.set_xticks(range(len(continuous_features)))\n","        ax2.set_xticklabels(continuous_features, rotation=45, ha='right')\n","\n","        # 3. æ ·æœ¬æ•°é‡åˆ†å¸ƒ\n","        ax3.bar(range(len(continuous_features)), counts)\n","        ax3.set_title('Sample Counts')\n","        ax3.set_xlabel('Features')\n","        ax3.set_ylabel('Sample Count')\n","        ax3.set_xticks(range(len(continuous_features)))\n","        ax3.set_xticklabels(continuous_features, rotation=45, ha='right')\n","        ax3.set_yscale('log')\n","\n","        # 4. å‡å€¼vsæ ‡å‡†å·®æ•£ç‚¹å›¾\n","        ax4.scatter(means, stds, s=100, alpha=0.7)\n","        ax4.set_title('Mean vs Standard Deviation')\n","        ax4.set_xlabel('Mean')\n","        ax4.set_ylabel('Standard Deviation')\n","\n","        # æ·»åŠ ç‰¹å¾æ ‡ç­¾\n","        for i, feat in enumerate(continuous_features):\n","            ax4.annotate(feat, (means[i], stds[i]), xytext=(5, 5),\n","                        textcoords='offset points', fontsize=8)\n","\n","        plt.tight_layout()\n","        plt.savefig('/content/drive/MyDrive/Scalers/no2_scaler_statistics.png',\n","                   dpi=300, bbox_inches='tight')\n","        plt.show()\n","\n","        print(f\"âœ… Visualization saved to: /content/drive/MyDrive/Scalers/no2_scaler_statistics.png\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Visualization failed: {e}\")\n","\n","if __name__ == \"__main__\":\n","    # è¿è¡ŒéªŒè¯\n","    success = validate_no2_scaler_results()\n","\n","    if success:\n","        # å¦‚æœéªŒè¯æˆåŠŸï¼Œè¿è¡Œå¯è§†åŒ–\n","        visualize_scaler_statistics()\n","\n","    print(f\"\\nğŸ¯ Next Steps:\")\n","    if success:\n","        print(f\"  1. âœ… NO2 scaler is ready for training\")\n","        print(f\"  2. ğŸš€ Run SO2 scaler: python 'Generate SO2 standardization parameter.py'\")\n","        print(f\"  3. ğŸ¯ Proceed with model training\")\n","    else:\n","        print(f\"  1. ğŸ”§ Fix issues identified in validation\")\n","        print(f\"  2. ğŸ”„ Re-run NO2 scaler if needed\")\n","        print(f\"  3. âœ… Validate again before proceeding\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5a3EHhqEhspJ","executionInfo":{"status":"ok","timestamp":1758059748539,"user_tz":-120,"elapsed":82,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"4ab59a8c-cc23-4154-b5a2-2e57f1bd6b82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” NO2 Scaler Results Validation\n","==================================================\n","âœ… Scaler file found: /content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\n","âœ… Successfully loaded scaler data\n","\n","ğŸ“Š Metadata:\n","  Data type: no2\n","  Years: [2019, 2020, 2021]\n","  Mask logic: mask==1 is valid, mask==0 is invalid (CONFIRMED FROM SOURCE CODE)\n","  Total features: 29\n","  Continuous features: 14\n","\n","ğŸ“‹ Feature Order (29 features):\n","   1. dem\n","   2. slope\n","   3. pop\n","   4. lulc_class_0\n","   5. lulc_class_1\n","   6. lulc_class_2\n","   7. lulc_class_3\n","   8. lulc_class_4\n","   9. lulc_class_5\n","  10. lulc_class_6\n","  11. lulc_class_7\n","  12. lulc_class_8\n","  13. lulc_class_9\n","  14. sin_doy\n","  15. cos_doy\n","  16. weekday_weight\n","  17. u10\n","  18. v10\n","  19. blh\n","  20. tp\n","  21. t2m\n","  22. sp\n","  23. str\n","  24. ssr_clr\n","  25. ws\n","  26. wd_sin\n","  27. wd_cos\n","  28. no2_lag_1day\n","  29. no2_neighbor\n","\n","ğŸ”¢ Scaler Parameters:\n","  âœ… dem: mean=606.2273, std=771.7322, count=1,052,695\n","  âœ… slope: mean=11.3112, std=11.0288, count=994,379\n","  âœ… pop: mean=135.9125, std=369.5149, count=992,667\n","  ğŸ“Š lulc_class_0: categorical (one-hot)\n","  ğŸ“Š lulc_class_1: categorical (one-hot)\n","  ğŸ“Š lulc_class_2: categorical (one-hot)\n","  ğŸ“Š lulc_class_3: categorical (one-hot)\n","  ğŸ“Š lulc_class_4: categorical (one-hot)\n","  ğŸ“Š lulc_class_5: categorical (one-hot)\n","  ğŸ“Š lulc_class_6: categorical (one-hot)\n","  ğŸ“Š lulc_class_7: categorical (one-hot)\n","  ğŸ“Š lulc_class_8: categorical (one-hot)\n","  ğŸ“Š lulc_class_9: categorical (one-hot)\n","  ğŸš« sin_doy: non_standardized\n","  ğŸš« cos_doy: non_standardized\n","  ğŸš« weekday_weight: non_standardized\n","  âœ… u10: mean=-0.2344, std=1.5692, count=1,052,695\n","  âœ… v10: mean=0.1392, std=1.4423, count=1,052,695\n","  âœ… blh: mean=951.2973, std=558.4578, count=1,052,695\n","  âœ… tp: mean=0.0003, std=0.0009, count=1,052,695\n","  âœ… t2m: mean=15.8182, std=9.8316, count=1,052,695\n","  âš ï¸ sp: default values (mean=0, std=1, count=0)\n","  âœ… str: mean=-29437.2015, std=32870.4722, count=1,631\n","  âš ï¸ ssr_clr: default values (mean=0, std=1, count=0)\n","  âœ… ws: mean=1.7724, std=1.2206, count=1,052,695\n","  ğŸš« wd_sin: non_standardized\n","  ğŸš« wd_cos: non_standardized\n","  âœ… no2_lag_1day: mean=0.0000, std=0.0001, count=1,033,884\n","  âœ… no2_neighbor: mean=0.0001, std=0.0001, count=1,052,695\n","\n","ğŸ“ˆ Summary Statistics:\n","  Total features: 29\n","  Continuous features: 14\n","  Categorical features: 10\n","  Non-standardized features: 5\n","  Features with default values: 2\n","\n","ğŸ¯ Key Continuous Features Validation:\n","  âœ… dem: 1,052,695 samples\n","  âœ… slope: 994,379 samples\n","  âœ… pop: 992,667 samples\n","  âœ… u10: 1,052,695 samples\n","  âœ… v10: 1,052,695 samples\n","  âœ… blh: 1,052,695 samples\n","  âœ… tp: 1,052,695 samples\n","  âœ… t2m: 1,052,695 samples\n","  âš ï¸ sp: default values (no samples)\n","  âœ… str: 1,631 samples\n","  âš ï¸ ssr_clr: default values (no samples)\n","  âœ… ws: 1,052,695 samples\n","  âœ… no2_lag_1day: 1,033,884 samples\n","  âœ… no2_neighbor: 1,052,695 samples\n","\n","ğŸ” Data Quality Check:\n","  âœ… Total valid samples: 11,444,121\n","  âš ï¸ Features with extreme std values:\n","    tp: std=0.000872\n","    str: std=32870.472234\n","    no2_lag_1day: std=0.000100\n","    no2_neighbor: std=0.000100\n","\n","ğŸ§ª Testing Scaler Application:\n","  âŒ Scaler application test failed: No module named 'Generate_NO2_standardization_parameter'\n","\n","ğŸ¯ Next Steps:\n","  1. ğŸ”§ Fix issues identified in validation\n","  2. ğŸ”„ Re-run NO2 scaler if needed\n","  3. âœ… Validate again before proceeding\n"]}]},{"cell_type":"code","source":["# Diagnose NO2 Feature Issues\n","import os\n","import numpy as np\n","from collections import defaultdict\n","\n","def diagnose_no2_feature_issues():\n","    \"\"\"è¯Šæ–­NO2ç‰¹å¾é—®é¢˜\"\"\"\n","    print(\"ğŸ” NO2 Feature Issues Diagnosis\")\n","    print(\"=\" * 50)\n","\n","    BASE_NO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","\n","    # æ£€æŸ¥çš„ç‰¹å¾\n","    problem_features = ['sp', 'ssr_clr', 'str']\n","\n","    # ç»Ÿè®¡ä¿¡æ¯\n","    feature_stats = defaultdict(lambda: {\n","        'total_files': 0,\n","        'files_with_feature': 0,\n","        'files_with_nan': 0,\n","        'files_with_finite': 0,\n","        'total_pixels': 0,\n","        'finite_pixels': 0,\n","        'nan_pixels': 0,\n","        'zero_pixels': 0,\n","        'negative_pixels': 0,\n","        'positive_pixels': 0\n","    })\n","\n","    # æ£€æŸ¥å‡ ä¸ªæ ·æœ¬æ–‡ä»¶\n","    sample_files = []\n","    for year in [2019, 2020, 2021]:\n","        year_dir = os.path.join(BASE_NO2, f\"NO2_{year}\")\n","        if os.path.exists(year_dir):\n","            files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","            if files:\n","                sample_files.append(os.path.join(year_dir, files[0]))  # å–ç¬¬ä¸€ä¸ªæ–‡ä»¶\n","                if len(sample_files) >= 3:  # æœ€å¤šæ£€æŸ¥3ä¸ªæ–‡ä»¶\n","                    break\n","\n","    print(f\"ğŸ“ Checking {len(sample_files)} sample files:\")\n","    for file_path in sample_files:\n","        print(f\"  {os.path.basename(file_path)}\")\n","\n","    print(f\"\\nğŸ” Detailed Analysis:\")\n","\n","    for file_path in sample_files:\n","        print(f\"\\nğŸ“„ File: {os.path.basename(file_path)}\")\n","\n","        try:\n","            with np.load(file_path) as data:\n","                print(f\"  ğŸ“Š Available keys: {list(data.files)}\")\n","\n","                # æ£€æŸ¥ç›®æ ‡æ•°æ®å’Œæ©è†œ\n","                if 'no2_target' in data.files and 'no2_mask' in data.files:\n","                    target = data['no2_target']\n","                    mask = data['no2_mask']\n","\n","                    # è®¡ç®—æœ‰æ•ˆåƒç´ \n","                    valid_mask = (mask == 1) & (target > 0)\n","                    valid_pixels = valid_mask.sum()\n","                    total_pixels = mask.size\n","\n","                    print(f\"  ğŸ¯ Valid pixels: {valid_pixels:,}/{total_pixels:,} ({100*valid_pixels/total_pixels:.1f}%)\")\n","\n","                    # æ£€æŸ¥é—®é¢˜ç‰¹å¾\n","                    for feat_name in problem_features:\n","                        if feat_name in data.files:\n","                            feature_data = data[feat_name]\n","\n","                            # åœ¨æœ‰æ•ˆåƒç´ ä¸Šçš„ç»Ÿè®¡\n","                            valid_feature_data = feature_data[valid_mask]\n","\n","                            # ç»Ÿè®¡ä¿¡æ¯\n","                            total_valid = len(valid_feature_data)\n","                            finite_count = np.isfinite(valid_feature_data).sum()\n","                            nan_count = np.isnan(valid_feature_data).sum()\n","                            zero_count = (valid_feature_data == 0).sum()\n","                            negative_count = (valid_feature_data < 0).sum()\n","                            positive_count = (valid_feature_data > 0).sum()\n","\n","                            print(f\"    {feat_name}:\")\n","                            print(f\"      Total valid pixels: {total_valid:,}\")\n","                            print(f\"      Finite values: {finite_count:,} ({100*finite_count/total_valid:.1f}%)\")\n","                            print(f\"      NaN values: {nan_count:,} ({100*nan_count/total_valid:.1f}%)\")\n","                            print(f\"      Zero values: {zero_count:,} ({100*zero_count/total_valid:.1f}%)\")\n","                            print(f\"      Negative values: {negative_count:,} ({100*negative_count/total_valid:.1f}%)\")\n","                            print(f\"      Positive values: {positive_count:,} ({100*positive_count/total_valid:.1f}%)\")\n","\n","                            if finite_count > 0:\n","                                finite_data = valid_feature_data[np.isfinite(valid_feature_data)]\n","                                print(f\"      Range: [{finite_data.min():.6f}, {finite_data.max():.6f}]\")\n","                                print(f\"      Mean: {finite_data.mean():.6f}\")\n","                                print(f\"      Std: {finite_data.std():.6f}\")\n","\n","                            # æ›´æ–°ç»Ÿè®¡\n","                            feature_stats[feat_name]['total_files'] += 1\n","                            feature_stats[feat_name]['files_with_feature'] += 1\n","                            feature_stats[feat_name]['total_pixels'] += total_valid\n","                            feature_stats[feat_name]['finite_pixels'] += finite_count\n","                            feature_stats[feat_name]['nan_pixels'] += nan_count\n","\n","                            if finite_count > 0:\n","                                feature_stats[feat_name]['files_with_finite'] += 1\n","                            if nan_count > 0:\n","                                feature_stats[feat_name]['files_with_nan'] += 1\n","                        else:\n","                            print(f\"    {feat_name}: âŒ NOT FOUND in file\")\n","                            feature_stats[feat_name]['total_files'] += 1\n","\n","        except Exception as e:\n","            print(f\"  âŒ Error loading file: {e}\")\n","\n","    # æ€»ç»“ç»Ÿè®¡\n","    print(f\"\\nğŸ“Š Summary Statistics:\")\n","    for feat_name in problem_features:\n","        stats = feature_stats[feat_name]\n","        print(f\"\\n  {feat_name}:\")\n","        print(f\"    Files checked: {stats['total_files']}\")\n","        print(f\"    Files with feature: {stats['files_with_feature']}\")\n","        print(f\"    Files with finite values: {stats['files_with_finite']}\")\n","        print(f\"    Files with NaN values: {stats['files_with_nan']}\")\n","\n","        if stats['total_pixels'] > 0:\n","            finite_ratio = stats['finite_pixels'] / stats['total_pixels']\n","            nan_ratio = stats['nan_pixels'] / stats['total_pixels']\n","            print(f\"    Finite ratio: {finite_ratio:.1%}\")\n","            print(f\"    NaN ratio: {nan_ratio:.1%}\")\n","\n","            if finite_ratio < 0.1:\n","                print(f\"    âš ï¸ Very low finite ratio - consider removing from CONTINUOUS_FEATURES\")\n","            elif nan_ratio > 0.9:\n","                print(f\"    âš ï¸ Very high NaN ratio - consider removing from CONTINUOUS_FEATURES\")\n","            else:\n","                print(f\"    âœ… Data quality acceptable\")\n","\n","def check_feature_naming_consistency():\n","    \"\"\"æ£€æŸ¥ç‰¹å¾å‘½åä¸€è‡´æ€§\"\"\"\n","    print(f\"\\nğŸ” Feature Naming Consistency Check:\")\n","    print(\"=\" * 50)\n","\n","    BASE_NO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","\n","    # æ£€æŸ¥ä¸åŒå¹´ä»½çš„æ–‡ä»¶é”®å\n","    all_keys = set()\n","    year_keys = {}\n","\n","    for year in [2019, 2020, 2021]:\n","        year_dir = os.path.join(BASE_NO2, f\"NO2_{year}\")\n","        if os.path.exists(year_dir):\n","            files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","            if files:\n","                # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ–‡ä»¶\n","                file_path = os.path.join(year_dir, files[0])\n","                try:\n","                    with np.load(file_path) as data:\n","                        keys = set(data.files)\n","                        all_keys.update(keys)\n","                        year_keys[year] = keys\n","                        print(f\"  {year}: {len(keys)} keys\")\n","                except Exception as e:\n","                    print(f\"  {year}: Error - {e}\")\n","\n","    # æ£€æŸ¥å…³é”®ç‰¹å¾\n","    key_features = ['sp', 'ssr_clr', 'ssr_clear', 'str']\n","    print(f\"\\nğŸ¯ Key Feature Analysis:\")\n","\n","    for feat in key_features:\n","        found_in_years = []\n","        for year, keys in year_keys.items():\n","            if feat in keys:\n","                found_in_years.append(year)\n","\n","        if found_in_years:\n","            print(f\"  {feat}: Found in years {found_in_years}\")\n","        else:\n","            print(f\"  {feat}: âŒ NOT FOUND in any year\")\n","\n","    # æ£€æŸ¥å¯èƒ½çš„å‘½åå˜ä½“\n","    print(f\"\\nğŸ” Possible naming variants:\")\n","    for year, keys in year_keys.items():\n","        radiation_keys = [k for k in keys if 'ssr' in k.lower() or 'radiation' in k.lower()]\n","        pressure_keys = [k for k in keys if 'sp' in k.lower() or 'pressure' in k.lower()]\n","        str_keys = [k for k in keys if 'str' in k.lower()]\n","\n","        if radiation_keys:\n","            print(f\"  {year} radiation-related keys: {radiation_keys}\")\n","        if pressure_keys:\n","            print(f\"  {year} pressure-related keys: {pressure_keys}\")\n","        if str_keys:\n","            print(f\"  {year} str-related keys: {str_keys}\")\n","\n","if __name__ == \"__main__\":\n","    diagnose_no2_feature_issues()\n","    check_feature_naming_consistency()\n","\n","    print(f\"\\nğŸ’¡ Recommendations:\")\n","    print(\"=\" * 50)\n","    print(\"1. If sp/ssr_clr/str have >90% NaN values â†’ Remove from CONTINUOUS_FEATURES\")\n","    print(\"2. If naming inconsistency found â†’ Update feature names in scaler\")\n","    print(\"3. If data coverage is poor â†’ Consider alternative features or imputation\")\n","    print(\"4. For str with low samples â†’ Consider removing or using alternative radiation variable\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y33NQ6QfjDBm","executionInfo":{"status":"ok","timestamp":1758061412817,"user_tz":-120,"elapsed":175,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"974423a0-90a1-4d8c-af0a-220d14c25cea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” NO2 Feature Issues Diagnosis\n","==================================================\n","ğŸ“ Checking 3 sample files:\n","  NO2_stack_20190102.npz\n","  NO2_stack_20200101.npz\n","  NO2_stack_20210101.npz\n","\n","ğŸ” Detailed Analysis:\n","\n","ğŸ“„ File: NO2_stack_20190102.npz\n","  ğŸ“Š Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","  ğŸ¯ Valid pixels: 73,631/186,300 (39.5%)\n","    sp:\n","      Total valid pixels: 73,631\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 0 (0.0%)\n","      Positive values: 73,631 (100.0%)\n","    ssr_clr:\n","      Total valid pixels: 73,631\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 0 (0.0%)\n","      Positive values: 73,631 (100.0%)\n","    str:\n","      Total valid pixels: 73,631\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 73,631 (100.0%)\n","      Positive values: 0 (0.0%)\n","\n","ğŸ“„ File: NO2_stack_20200101.npz\n","  ğŸ“Š Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","  ğŸ¯ Valid pixels: 88,336/186,300 (47.4%)\n","    sp:\n","      Total valid pixels: 88,336\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 0 (0.0%)\n","      Positive values: 88,336 (100.0%)\n","    ssr_clr:\n","      Total valid pixels: 88,336\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 0 (0.0%)\n","      Positive values: 88,336 (100.0%)\n","    str:\n","      Total valid pixels: 88,336\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 88,336 (100.0%)\n","      Positive values: 0 (0.0%)\n","\n","ğŸ“„ File: NO2_stack_20210101.npz\n","  ğŸ“Š Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","  ğŸ¯ Valid pixels: 0/186,300 (0.0%)\n","    sp:\n","      Total valid pixels: 0\n","      Finite values: 0 (nan%)\n","      NaN values: 0 (nan%)\n","      Zero values: 0 (nan%)\n","      Negative values: 0 (nan%)\n","      Positive values: 0 (nan%)\n","    ssr_clr:\n","      Total valid pixels: 0\n","      Finite values: 0 (nan%)\n","      NaN values: 0 (nan%)\n","      Zero values: 0 (nan%)\n","      Negative values: 0 (nan%)\n","      Positive values: 0 (nan%)\n","    str:\n","      Total valid pixels: 0\n","      Finite values: 0 (nan%)\n","      NaN values: 0 (nan%)\n","      Zero values: 0 (nan%)\n","      Negative values: 0 (nan%)\n","      Positive values: 0 (nan%)\n","\n","ğŸ“Š Summary Statistics:\n","\n","  sp:\n","    Files checked: 3\n","    Files with feature: 3\n","    Files with finite values: 0\n","    Files with NaN values: 0\n","    Finite ratio: 0.0%\n","    NaN ratio: 0.0%\n","    âš ï¸ Very low finite ratio - consider removing from CONTINUOUS_FEATURES\n","\n","  ssr_clr:\n","    Files checked: 3\n","    Files with feature: 3\n","    Files with finite values: 0\n","    Files with NaN values: 0\n","    Finite ratio: 0.0%\n","    NaN ratio: 0.0%\n","    âš ï¸ Very low finite ratio - consider removing from CONTINUOUS_FEATURES\n","\n","  str:\n","    Files checked: 3\n","    Files with feature: 3\n","    Files with finite values: 0\n","    Files with NaN values: 0\n","    Finite ratio: 0.0%\n","    NaN ratio: 0.0%\n","    âš ï¸ Very low finite ratio - consider removing from CONTINUOUS_FEATURES\n","\n","ğŸ” Feature Naming Consistency Check:\n","==================================================\n","  2019: 37 keys\n","  2020: 37 keys\n","  2021: 37 keys\n","\n","ğŸ¯ Key Feature Analysis:\n","  sp: Found in years [2019, 2020, 2021]\n","  ssr_clr: Found in years [2019, 2020, 2021]\n","  ssr_clear: âŒ NOT FOUND in any year\n","  str: Found in years [2019, 2020, 2021]\n","\n","ğŸ” Possible naming variants:\n","  2019 radiation-related keys: ['ssr_clr']\n","  2019 pressure-related keys: ['sp']\n","  2019 str-related keys: ['str']\n","  2020 radiation-related keys: ['ssr_clr']\n","  2020 pressure-related keys: ['sp']\n","  2020 str-related keys: ['str']\n","  2021 radiation-related keys: ['ssr_clr']\n","  2021 pressure-related keys: ['sp']\n","  2021 str-related keys: ['str']\n","\n","ğŸ’¡ Recommendations:\n","==================================================\n","1. If sp/ssr_clr/str have >90% NaN values â†’ Remove from CONTINUOUS_FEATURES\n","2. If naming inconsistency found â†’ Update feature names in scaler\n","3. If data coverage is poor â†’ Consider alternative features or imputation\n","4. For str with low samples â†’ Consider removing or using alternative radiation variable\n"]}]},{"cell_type":"code","source":["# Quick Check Problem Features\n","import os\n","import numpy as np\n","\n","def quick_check_problem_features():\n","    \"\"\"å¿«é€Ÿæ£€æŸ¥é—®é¢˜ç‰¹å¾\"\"\"\n","    print(\"ğŸ” Quick Check Problem Features\")\n","    print(\"=\" * 40)\n","\n","    BASE_NO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","\n","    # æ£€æŸ¥ä¸€ä¸ªæ ·æœ¬æ–‡ä»¶\n","    sample_file = None\n","    for year in [2019, 2020, 2021]:\n","        year_dir = os.path.join(BASE_NO2, f\"NO2_{year}\")\n","        if os.path.exists(year_dir):\n","            files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","            if files:\n","                sample_file = os.path.join(year_dir, files[0])\n","                break\n","\n","    if not sample_file:\n","        print(\"âŒ No sample file found\")\n","        return\n","\n","    print(f\"ğŸ“„ Checking file: {os.path.basename(sample_file)}\")\n","\n","    try:\n","        with np.load(sample_file) as data:\n","            print(f\"ğŸ“Š Available keys: {list(data.files)}\")\n","\n","            # æ£€æŸ¥ç›®æ ‡æ•°æ®å’Œæ©è†œ\n","            if 'no2_target' in data.files and 'no2_mask' in data.files:\n","                target = data['no2_target']\n","                mask = data['no2_mask']\n","\n","                # è®¡ç®—æœ‰æ•ˆåƒç´ \n","                valid_mask = (mask == 1) & (target > 0)\n","                valid_pixels = valid_mask.sum()\n","                total_pixels = mask.size\n","\n","                print(f\"ğŸ¯ Valid pixels: {valid_pixels:,}/{total_pixels:,} ({100*valid_pixels/total_pixels:.1f}%)\")\n","\n","                # æ£€æŸ¥é—®é¢˜ç‰¹å¾\n","                problem_features = ['sp', 'ssr_clr', 'str']\n","\n","                for feat_name in problem_features:\n","                    print(f\"\\nğŸ” {feat_name}:\")\n","\n","                    if feat_name in data.files:\n","                        feature_data = data[feat_name]\n","                        valid_feature_data = feature_data[valid_mask]\n","\n","                        # åŸºæœ¬ç»Ÿè®¡\n","                        total_valid = len(valid_feature_data)\n","                        finite_count = np.isfinite(valid_feature_data).sum()\n","                        nan_count = np.isnan(valid_feature_data).sum()\n","\n","                        print(f\"  Total valid pixels: {total_valid:,}\")\n","                        print(f\"  Finite values: {finite_count:,} ({100*finite_count/total_valid:.1f}%)\")\n","                        print(f\"  NaN values: {nan_count:,} ({100*nan_count/total_valid:.1f}%)\")\n","\n","                        if finite_count > 0:\n","                            finite_data = valid_feature_data[np.isfinite(valid_feature_data)]\n","                            print(f\"  Range: [{finite_data.min():.6f}, {finite_data.max():.6f}]\")\n","                            print(f\"  Mean: {finite_data.mean():.6f}\")\n","                            print(f\"  Std: {finite_data.std():.6f}\")\n","\n","                            # åˆ¤æ–­æ•°æ®è´¨é‡\n","                            if finite_count / total_valid < 0.1:\n","                                print(f\"  âš ï¸ Very low finite ratio - should be removed from CONTINUOUS_FEATURES\")\n","                            elif nan_count / total_valid > 0.9:\n","                                print(f\"  âš ï¸ Very high NaN ratio - should be removed from CONTINUOUS_FEATURES\")\n","                            else:\n","                                print(f\"  âœ… Data quality acceptable\")\n","                        else:\n","                            print(f\"  âŒ No finite values - should be removed from CONTINUOUS_FEATURES\")\n","                    else:\n","                        print(f\"  âŒ Feature not found in file\")\n","\n","                        # æ£€æŸ¥å¯èƒ½çš„å‘½åå˜ä½“\n","                        possible_names = []\n","                        if feat_name == 'ssr_clr':\n","                            possible_names = ['ssr_clear', 'ssr', 'clear_sky_radiation']\n","                        elif feat_name == 'sp':\n","                            possible_names = ['pressure', 'surface_pressure', 'sp_pressure']\n","                        elif feat_name == 'str':\n","                            possible_names = ['surface_thermal_radiation', 'thermal_radiation', 'str_radiation']\n","\n","                        for possible_name in possible_names:\n","                            if possible_name in data.files:\n","                                print(f\"  ğŸ’¡ Found possible variant: {possible_name}\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error loading file: {e}\")\n","\n","if __name__ == \"__main__\":\n","    quick_check_problem_features()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WXVlJNi6mWfE","executionInfo":{"status":"ok","timestamp":1758061412924,"user_tz":-120,"elapsed":24,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"ae4e7fd5-dbf8-411d-a338-3440fd67a503"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” Quick Check Problem Features\n","========================================\n","ğŸ“„ Checking file: NO2_stack_20190102.npz\n","ğŸ“Š Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","ğŸ¯ Valid pixels: 73,631/186,300 (39.5%)\n","\n","ğŸ” sp:\n","  Total valid pixels: 73,631\n","  Finite values: 0 (0.0%)\n","  NaN values: 0 (0.0%)\n","  âŒ No finite values - should be removed from CONTINUOUS_FEATURES\n","\n","ğŸ” ssr_clr:\n","  Total valid pixels: 73,631\n","  Finite values: 0 (0.0%)\n","  NaN values: 0 (0.0%)\n","  âŒ No finite values - should be removed from CONTINUOUS_FEATURES\n","\n","ğŸ” str:\n","  Total valid pixels: 73,631\n","  Finite values: 0 (0.0%)\n","  NaN values: 0 (0.0%)\n","  âŒ No finite values - should be removed from CONTINUOUS_FEATURES\n"]}]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/GeoGapFiller/regenerate_no2_scaler_fixed.py\n","#!/usr/bin/env python3\n","\"\"\"\n","NO2 Scaleré‡æ–°ç”Ÿæˆè„šæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰\n","Regenerate NO2 Scaler (Fixed Version)\n","\n","ä½¿ç”¨ä¿®å¤åçš„ç‰¹å¾æ ˆé‡æ–°ç”ŸæˆNO2æ ‡å‡†åŒ–å‚æ•°\n","\"\"\"\n","\n","import os\n","import numpy as np\n","import sys\n","from datetime import datetime\n","import time\n","\n","# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„\n","sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n","\n","def regenerate_no2_scaler():\n","    \"\"\"é‡æ–°ç”ŸæˆNO2 scaler\"\"\"\n","    print(\"ğŸš€ Regenerating NO2 Scaler (Fixed Version)\")\n","    print(\"=\" * 60)\n","\n","    # é…ç½®è·¯å¾„\n","    base_path = \"/content/drive/MyDrive\"\n","    feature_stack_dir = os.path.join(base_path, \"Feature_Stacks\", \"NO2_Independent\")\n","    output_dir = os.path.join(base_path, \"Scalers\")\n","\n","    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # è¾“å‡ºæ–‡ä»¶è·¯å¾„\n","    scaler_file = os.path.join(output_dir, \"no2_scalers_2019_2021_fixed.npz\")\n","\n","    print(f\"ğŸ“ Feature stack directory: {feature_stack_dir}\")\n","    print(f\"ğŸ“ Output directory: {output_dir}\")\n","    print(f\"ğŸ“ Scaler file: {scaler_file}\")\n","\n","    # æ£€æŸ¥ç‰¹å¾æ ˆç›®å½•\n","    if not os.path.exists(feature_stack_dir):\n","        print(f\"âŒ Feature stack directory not found: {feature_stack_dir}\")\n","        return False\n","\n","    # è·å–ç‰¹å¾æ ˆæ–‡ä»¶åˆ—è¡¨\n","    feature_files = [f for f in os.listdir(feature_stack_dir) if f.startswith(\"NO2_stack_\") and f.endswith(\".npz\")]\n","    feature_files.sort()\n","\n","    print(f\"ğŸ“Š Found {len(feature_files)} feature stack files\")\n","\n","    if len(feature_files) == 0:\n","        print(\"âŒ No feature stack files found!\")\n","        return False\n","\n","    # å®šä¹‰ç‰¹å¾ç±»å‹\n","    CONTINUOUS_FEATURES = [\n","        'dem', 'slope', 'pop', 'u10', 'v10', 'blh', 'tp', 't2m',\n","        'sp', 'str', 'ssr_clr', 'ws', 'no2_lag_1day', 'no2_neighbor'\n","    ]\n","\n","    CATEGORICAL_FEATURES = [\n","        'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4',\n","        'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9'\n","    ]\n","\n","    NON_STANDARDIZED = [\n","        'wd_sin', 'wd_cos', 'sin_doy', 'cos_doy', 'weekday_weight'\n","    ]\n","\n","    # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯\n","    scalers = {}\n","    feature_order = []\n","    total_samples = 0\n","    processed_files = 0\n","    failed_files = 0\n","\n","    # ä¸ºæ¯ä¸ªè¿ç»­ç‰¹å¾åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯\n","    for feat in CONTINUOUS_FEATURES:\n","        scalers[feat] = {\n","            'sum': 0.0,\n","            'sum_sq': 0.0,\n","            'count': 0,\n","            'min': float('inf'),\n","            'max': float('-inf')\n","        }\n","\n","    print(f\"\\nğŸ” Processing feature stack files...\")\n","    start_time = time.time()\n","\n","    # å¤„ç†æ¯ä¸ªç‰¹å¾æ ˆæ–‡ä»¶\n","    for i, filename in enumerate(feature_files):\n","        try:\n","            file_path = os.path.join(feature_stack_dir, filename)\n","\n","            # åŠ è½½ç‰¹å¾æ ˆ\n","            with np.load(file_path, allow_pickle=True) as data:\n","                # æ£€æŸ¥å¿…è¦çš„é”®\n","                if 'no2_target' not in data or 'no2_mask' not in data:\n","                    print(f\"   âš ï¸ {filename}: Missing target or mask, skipping\")\n","                    failed_files += 1\n","                    continue\n","\n","                # è·å–ç›®æ ‡å˜é‡å’Œæ©è†œ\n","                y = data['no2_target']\n","                mask = data['no2_mask']\n","\n","                # åˆ›å»ºæœ‰æ•ˆæ©è†œï¼ˆmask==1ä¸”y>0ï¼‰\n","                valid_mask = (mask == 1) & (y > 0)\n","\n","                if not np.any(valid_mask):\n","                    print(f\"   âš ï¸ {filename}: No valid pixels, skipping\")\n","                    failed_files += 1\n","                    continue\n","\n","                # æ„å»ºç‰¹å¾çŸ©é˜µ\n","                X = []\n","                current_feature_order = []\n","\n","                # æ·»åŠ è¿ç»­ç‰¹å¾\n","                for feat in CONTINUOUS_FEATURES:\n","                    if feat in data:\n","                        X.append(data[feat])\n","                        current_feature_order.append(feat)\n","                    else:\n","                        print(f\"   âš ï¸ {filename}: Missing continuous feature {feat}\")\n","                        # ä½¿ç”¨NaNå¡«å……\n","                        X.append(np.full_like(y, np.nan))\n","                        current_feature_order.append(feat)\n","\n","                # æ·»åŠ åˆ†ç±»ç‰¹å¾ï¼ˆone-hotç¼–ç ï¼‰\n","                for feat in CATEGORICAL_FEATURES:\n","                    if feat in data:\n","                        X.append(data[feat])\n","                        current_feature_order.append(feat)\n","                    else:\n","                        print(f\"   âš ï¸ {filename}: Missing categorical feature {feat}\")\n","                        # ä½¿ç”¨0å¡«å……\n","                        X.append(np.zeros_like(y))\n","                        current_feature_order.append(feat)\n","\n","                # æ·»åŠ éæ ‡å‡†åŒ–ç‰¹å¾\n","                for feat in NON_STANDARDIZED:\n","                    if feat in data:\n","                        X.append(data[feat])\n","                        current_feature_order.append(feat)\n","                    else:\n","                        print(f\"   âš ï¸ {filename}: Missing non-standardized feature {feat}\")\n","                        # ä½¿ç”¨é»˜è®¤å€¼å¡«å……\n","                        if feat in ['wd_sin', 'wd_cos']:\n","                            X.append(np.zeros_like(y))\n","                        elif feat in ['sin_doy', 'cos_doy']:\n","                            X.append(np.zeros_like(y))\n","                        elif feat == 'weekday_weight':\n","                            X.append(np.ones_like(y))\n","                        current_feature_order.append(feat)\n","\n","                # è½¬æ¢ä¸ºnumpyæ•°ç»„\n","                X = np.array(X)\n","\n","                # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´\n","                if not feature_order:\n","                    feature_order = current_feature_order\n","                elif feature_order != current_feature_order:\n","                    print(f\"   âš ï¸ {filename}: Feature order mismatch, skipping\")\n","                    failed_files += 1\n","                    continue\n","\n","                # å±•å¹³æ•°æ®\n","                X_flat = X.reshape(X.shape[0], -1)\n","                valid_mask_flat = valid_mask.flatten()\n","\n","                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯\n","                for j, feat in enumerate(CONTINUOUS_FEATURES):\n","                    if j < X_flat.shape[0]:\n","                        feat_data = X_flat[j, valid_mask_flat]\n","                        finite_mask = np.isfinite(feat_data)\n","\n","                        if np.any(finite_mask):\n","                            finite_data = feat_data[finite_mask]\n","                            scalers[feat]['sum'] += np.sum(finite_data)\n","                            scalers[feat]['sum_sq'] += np.sum(finite_data**2)\n","                            scalers[feat]['count'] += len(finite_data)\n","                            scalers[feat]['min'] = min(scalers[feat]['min'], np.min(finite_data))\n","                            scalers[feat]['max'] = max(scalers[feat]['max'], np.max(finite_data))\n","\n","                processed_files += 1\n","                total_samples += np.sum(valid_mask)\n","\n","                if (i + 1) % 100 == 0:\n","                    elapsed = time.time() - start_time\n","                    rate = (i + 1) / elapsed if elapsed > 0 else 0\n","                    print(f\"   ğŸ“Š Processed {i + 1}/{len(feature_files)} files, rate: {rate:.1f} files/sec\")\n","\n","        except Exception as e:\n","            print(f\"   âŒ {filename}: Error - {e}\")\n","            failed_files += 1\n","\n","    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯\n","    print(f\"\\nğŸ”¢ Computing final statistics...\")\n","\n","    final_scalers = {}\n","    for feat in CONTINUOUS_FEATURES:\n","        if scalers[feat]['count'] > 0:\n","            mean = scalers[feat]['sum'] / scalers[feat]['count']\n","            variance = (scalers[feat]['sum_sq'] / scalers[feat]['count']) - (mean ** 2)\n","            std = np.sqrt(max(0, variance))\n","\n","            final_scalers[feat] = {\n","                'mean': mean,\n","                'std': std,\n","                'count': scalers[feat]['count'],\n","                'min': scalers[feat]['min'],\n","                'max': scalers[feat]['max']\n","            }\n","        else:\n","            # æ²¡æœ‰æ ·æœ¬çš„ç‰¹å¾ä½¿ç”¨é»˜è®¤å€¼\n","            final_scalers[feat] = {\n","                'mean': 0.0,\n","                'std': 1.0,\n","                'count': 0,\n","                'min': 0.0,\n","                'max': 0.0\n","            }\n","\n","    # ä¿å­˜scaler\n","    metadata = {\n","        'years': [2019, 2020, 2021],\n","        'total_files': len(feature_files),\n","        'processed_files': processed_files,\n","        'failed_files': failed_files,\n","        'total_samples': total_samples,\n","        'processing_time': time.time() - start_time,\n","        'mask_logic': 'mask==1 is valid, mask==0 is invalid',\n","        'target_logic': 'y > 0 for physical validity'\n","    }\n","\n","    np.savez_compressed(\n","        scaler_file,\n","        scalers=final_scalers,\n","        feature_order=feature_order,\n","        metadata=metadata\n","    )\n","\n","    # æ˜¾ç¤ºç»“æœ\n","    print(f\"\\nğŸ“Š NO2 Scaler Regeneration Summary:\")\n","    print(f\"   - Total files: {len(feature_files)}\")\n","    print(f\"   - Processed: {processed_files}\")\n","    print(f\"   - Failed: {failed_files}\")\n","    print(f\"   - Success rate: {processed_files/len(feature_files)*100:.1f}%\")\n","    print(f\"   - Total samples: {total_samples:,}\")\n","    print(f\"   - Processing time: {(time.time() - start_time)/60:.1f} minutes\")\n","    print(f\"   - Scaler file: {scaler_file}\")\n","\n","    # æ˜¾ç¤ºå…³é”®ç‰¹å¾ç»Ÿè®¡\n","    print(f\"\\nğŸ” Key Features Statistics:\")\n","    key_features = ['sp', 'str', 'ssr_clr', 'dem', 't2m', 'u10', 'v10']\n","    for feat in key_features:\n","        if feat in final_scalers:\n","            stats = final_scalers[feat]\n","            print(f\"   {feat}: mean={stats['mean']:.4f}, std={stats['std']:.4f}, count={stats['count']:,}\")\n","\n","    return True\n","\n","def main():\n","    \"\"\"ä¸»å‡½æ•°\"\"\"\n","    print(\"ğŸš€ NO2 Scaler Regeneration (Fixed Version)\")\n","    print(\"=\" * 60)\n","\n","    # ç¡®è®¤å¼€å§‹\n","    response = input(\"â“ Do you want to regenerate NO2 scaler? (y/n): \").lower().strip()\n","\n","    if response == 'y':\n","        success = regenerate_no2_scaler()\n","\n","        if success:\n","            print(\"\\nğŸ‰ NO2 Scaler regeneration completed successfully!\")\n","            print(\"ğŸ“‹ Next steps:\")\n","            print(\"   1. âœ… Scaler regenerated with fixed data\")\n","            print(\"   2. ğŸ”„ Ready for model training\")\n","            print(\"   3. ğŸ”„ Ready for performance evaluation\")\n","        else:\n","            print(\"\\nâŒ NO2 Scaler regeneration failed!\")\n","    else:\n","        print(\"\\nâ¸ï¸ NO2 Scaler regeneration cancelled\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDgWKM4z5wSw","executionInfo":{"status":"ok","timestamp":1758066098558,"user_tz":-120,"elapsed":50,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"541f6b02-177d-400b-fc63-4385c35d18a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/GeoGapFiller/regenerate_no2_scaler_fixed.py\n"]}]},{"cell_type":"code","source":["!python /content/drive/MyDrive/GeoGapFiller/regenerate_no2_scaler_fixed.py"],"metadata":{"id":"cdKsZ8fW5_pd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","NO2 Scaler for Dictionary Structure\n","ä¸“é—¨å¤„ç†NO2å­—å…¸å¼.npzæ–‡ä»¶ç»“æ„çš„scalerç”Ÿæˆ\n","\n","NO2æ–‡ä»¶ç»“æ„: {'no2_target': array, 'dem': array, 'u10': array, ...}\n","éœ€è¦å…ˆæ„å»ºç‰¹å¾çŸ©é˜µï¼Œå†è¿›è¡Œæ ‡å‡†åŒ–\n","\"\"\"\n","\n","import os\n","import numpy as np\n","import glob\n","from datetime import datetime\n","import time\n","from collections import defaultdict\n","\n","def build_feature_matrix_from_dict(data_dict, feature_order):\n","    \"\"\"\n","    ä»å­—å…¸ç»“æ„æ„å»ºç‰¹å¾çŸ©é˜µ\n","\n","    Args:\n","        data_dict: NO2å­—å…¸å¼æ•°æ® {'no2_target': array, 'dem': array, ...}\n","        feature_order: ç‰¹å¾é¡ºåºåˆ—è¡¨\n","\n","    Returns:\n","        X: ç‰¹å¾çŸ©é˜µ (C, H, W)\n","    \"\"\"\n","    X_list = []\n","\n","    for feat_name in feature_order:\n","        if feat_name in data_dict:\n","            X_list.append(data_dict[feat_name].astype(np.float32))\n","        else:\n","            # å¤„ç†ç¼ºå¤±ç‰¹å¾\n","            if feat_name.startswith('lulc_class_'):\n","                # LULCç‰¹å¾ç¼ºå¤±ç”¨0å¡«å……\n","                fill_value = 0.0\n","            elif feat_name in ['sin_doy', 'cos_doy', 'wd_sin', 'wd_cos']:\n","                # æ—¶é—´/é£å‘ç‰¹å¾ç¼ºå¤±ç”¨0å¡«å……\n","                fill_value = 0.0\n","            elif feat_name == 'weekday_weight':\n","                # å·¥ä½œæ—¥æƒé‡ç¼ºå¤±ç”¨1å¡«å……\n","                fill_value = 1.0\n","            else:\n","                # å…¶ä»–ç‰¹å¾ç¼ºå¤±ç”¨NaNå¡«å……\n","                fill_value = np.nan\n","\n","            # è·å–ç»´åº¦ - æ”¹è¿›ï¼šä¼˜å…ˆä»no2_targetæ¨æ–­\n","            if X_list:\n","                H, W = X_list[0].shape\n","            elif 'no2_target' in data_dict:\n","                H, W = data_dict['no2_target'].shape\n","            elif 'no2_mask' in data_dict:\n","                H, W = data_dict['no2_mask'].shape\n","            else:\n","                H, W = 300, 621  # æœ€åå…œåº•\n","\n","            X_list.append(np.full((H, W), fill_value, dtype=np.float32))\n","\n","    return np.stack(X_list, axis=0)  # (C, H, W)\n","\n","def compute_no2_scaler_dictionary_structure(years=[2019, 2020, 2021], sample_per_file=1000):\n","    \"\"\"\n","    è®¡ç®—NO2æ ‡å‡†åŒ–å‚æ•° - ä¸“é—¨å¤„ç†å­—å…¸ç»“æ„\n","\n","    Args:\n","        years: è®­ç»ƒå¹´ä»½\n","        sample_per_file: æ¯ä¸ªæ–‡ä»¶é‡‡æ ·çš„åƒç´ æ•°\n","    \"\"\"\n","    print(\"ğŸš€ Computing NO2 Scaler for Dictionary Structure\")\n","    print(\"=\" * 60)\n","    print(\"ğŸ“‹ NO2 file structure: Dictionary format\")\n","    print(\"ğŸ“‹ Process: Build feature matrix â†’ Compute statistics â†’ Generate scaler\")\n","\n","    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§\n","    np.random.seed(42)\n","\n","    # é…ç½®è·¯å¾„\n","    base_path = \"/content/drive/MyDrive\"\n","    feature_stack_base = os.path.join(base_path, \"Feature_Stacks\")\n","    output_dir = os.path.join(base_path, \"Scalers\")\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # å®šä¹‰ç‰¹å¾ç±»å‹å’Œé¡ºåº\n","    CONTINUOUS_FEATURES = [\n","        # æ°”è±¡ç‰¹å¾\n","        'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","        # é£åœºæ´¾ç”Ÿç‰¹å¾\n","        'ws',  # ç§»é™¤ wd_sin, wd_cos\n","        # NO2ç›¸å…³ç‰¹å¾\n","        'no2_lag_1day', 'no2_neighbor',\n","        # é™æ€ç‰¹å¾\n","        'dem', 'slope', 'pop'\n","    ]\n","\n","    CATEGORICAL_FEATURES = [\n","        'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4',\n","        'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9'\n","    ]\n","\n","    NON_STANDARDIZED = [\n","        'sin_doy', 'cos_doy', 'weekday_weight',\n","        'wd_sin', 'wd_cos'  # æ–°å¢ï¼šé£å‘ç‰¹å¾ä¸æ ‡å‡†åŒ–\n","    ]\n","\n","    # å®Œæ•´çš„ç‰¹å¾é¡ºåº\n","    feature_order = (\n","        CONTINUOUS_FEATURES +\n","        CATEGORICAL_FEATURES +\n","        NON_STANDARDIZED\n","    )\n","\n","    print(f\"ğŸ“Š Feature configuration:\")\n","    print(f\"   - Continuous features: {len(CONTINUOUS_FEATURES)}\")\n","    print(f\"   - Categorical features: {len(CATEGORICAL_FEATURES)}\")\n","    print(f\"   - Non-standardized features: {len(NON_STANDARDIZED)}\")\n","    print(f\"   - Total features: {len(feature_order)}\")\n","    print(f\"   - æ”¹è¿›: é£å‘ç‰¹å¾(wd_sin, wd_cos)ä¸æ ‡å‡†åŒ–ï¼Œä¿æŒ[-1,1]èŒƒå›´\")\n","    print(f\"   - æ”¹è¿›: æœ‰æ•ˆåƒç´ åˆ¤å®šå¯é…ç½®ï¼Œé»˜è®¤ä¸è¿‡æ»¤y<=0\")\n","\n","    # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯\n","    stats = defaultdict(lambda: {'sum': 0.0, 'sumsq': 0.0, 'count': 0})\n","\n","    total_files = 0\n","    processed_files = 0\n","    failed_files = 0\n","\n","    print(f\"\\nğŸ” Processing NO2 dictionary files...\")\n","    start_time = time.time()\n","\n","    # æŒ‰å¹´ä»½å¤„ç†\n","    for year in years:\n","        year_dir = os.path.join(feature_stack_base, f\"NO2_{year}\")\n","        if not os.path.exists(year_dir):\n","            print(f\"âš ï¸ Year directory not found: {year_dir}\")\n","            continue\n","\n","        files = [f for f in os.listdir(year_dir) if f.startswith(\"NO2_stack_\") and f.endswith(\".npz\")]\n","        files.sort()\n","        total_files += len(files)\n","\n","        print(f\"ğŸ“… Processing {year}: {len(files)} files\")\n","\n","        for i, filename in enumerate(files):\n","            try:\n","                file_path = os.path.join(year_dir, filename)\n","\n","                # åŠ è½½å­—å…¸å¼æ•°æ®\n","                with np.load(file_path, allow_pickle=True) as data:\n","                    # æ£€æŸ¥å¿…è¦é”®\n","                    if 'no2_target' not in data or 'no2_mask' not in data:\n","                        print(f\"   âš ï¸ {filename}: Missing target or mask\")\n","                        failed_files += 1\n","                        continue\n","\n","                    # è·å–ç›®æ ‡å˜é‡å’Œæ©è†œ\n","                    y = data['no2_target']\n","                    mask = data['no2_mask']\n","\n","                    # åˆ›å»ºæœ‰æ•ˆæ©è†œ - æ”¹è¿›ç‰ˆï¼šå¯é…ç½®æ˜¯å¦è¿‡æ»¤éæ­£å€¼\n","                    FILTER_NONPOSITIVE_TARGET = False  # å¯é…ç½®ï¼šæ˜¯å¦è¿‡æ»¤y<=0çš„åƒç´ \n","                    valid_mask = (mask == 1)\n","                    if FILTER_NONPOSITIVE_TARGET:\n","                        valid_mask &= (y > 0)\n","\n","                    if not np.any(valid_mask):\n","                        print(f\"   âš ï¸ {filename}: No valid pixels\")\n","                        failed_files += 1\n","                        continue\n","\n","                    # ä»å­—å…¸æ„å»ºç‰¹å¾çŸ©é˜µ\n","                    X = build_feature_matrix_from_dict(data, feature_order)\n","\n","                    # å½¢çŠ¶ä¸€è‡´æ€§æ£€æŸ¥\n","                    if X.shape[0] != len(feature_order):\n","                        print(f\"   âš ï¸ {filename}: Feature matrix shape mismatch: X.shape[0]={X.shape[0]}, feature_order length={len(feature_order)}\")\n","                        failed_files += 1\n","                        continue\n","\n","                    # æå–æœ‰æ•ˆåƒç´ \n","                    C, H, W = X.shape\n","                    X_flat = X.reshape(C, -1)  # (C, H*W)\n","                    valid_flat = valid_mask.ravel()  # (H*W,)\n","                    valid_pixels = X_flat[:, valid_flat]  # (C, N_valid)\n","\n","                    if valid_pixels.shape[1] == 0:\n","                        continue\n","\n","                    # éšæœºé‡‡æ ·\n","                    n_samples = min(sample_per_file, valid_pixels.shape[1])\n","                    if n_samples < valid_pixels.shape[1]:\n","                        indices = np.random.choice(valid_pixels.shape[1], n_samples, replace=False)\n","                        sampled = valid_pixels[:, indices]\n","                    else:\n","                        sampled = valid_pixels\n","\n","                    # è¿‡æ»¤NaN/inf\n","                    finite_mask = np.isfinite(sampled)\n","\n","                    # ç´¯ç§¯ç»Ÿè®¡ä¿¡æ¯ï¼ˆåªå¯¹è¿ç»­ç‰¹å¾ï¼‰\n","                    for j, feat_name in enumerate(feature_order):\n","                        if feat_name in CONTINUOUS_FEATURES:\n","                            values = sampled[j]\n","                            finite_values = values[finite_mask[j]]\n","\n","                            if len(finite_values) > 0:\n","                                # 1-99ç™¾åˆ†ä½winsorization\n","                                q1, q99 = np.percentile(finite_values, [1, 99])\n","                                clipped = np.clip(finite_values, q1, q99)\n","\n","                                stats[feat_name]['sum'] += np.sum(clipped)\n","                                stats[feat_name]['sumsq'] += np.sum(clipped**2)\n","                                stats[feat_name]['count'] += len(clipped)\n","\n","                    processed_files += 1\n","\n","                    if (i + 1) % 50 == 0:\n","                        elapsed = time.time() - start_time\n","                        rate = (i + 1) / elapsed if elapsed > 0 else 0\n","                        progress_pct = (i + 1) / len(files) * 100\n","                        print(f\"   ğŸ“Š {year}: {i + 1}/{len(files)} files ({progress_pct:.1f}%), rate: {rate:.1f} files/sec\")\n","\n","            except Exception as e:\n","                print(f\"   âŒ {filename}: Error - {e}\")\n","                failed_files += 1\n","\n","    print(f\"\\nğŸ“Š Processing statistics:\")\n","    print(f\"   - Total files: {total_files}\")\n","    print(f\"   - Processed: {processed_files}\")\n","    print(f\"   - Failed: {failed_files}\")\n","    print(f\"   - Success rate: {processed_files/total_files*100:.1f}%\")\n","\n","    # ç‰¹å¾ä¸€è‡´æ€§æ£€æŸ¥\n","    print(f\"\\nğŸ” Feature consistency check:\")\n","    all_defined_features = set(CONTINUOUS_FEATURES + CATEGORICAL_FEATURES + NON_STANDARDIZED)\n","    feature_order_set = set(feature_order)\n","\n","    missing_in_order = all_defined_features - feature_order_set\n","    extra_in_order = feature_order_set - all_defined_features\n","\n","    if missing_in_order:\n","        print(f\"   âš ï¸ Features defined but missing in feature_order: {missing_in_order}\")\n","    if extra_in_order:\n","        print(f\"   âš ï¸ Features in feature_order but not defined: {extra_in_order}\")\n","    if not missing_in_order and not extra_in_order:\n","        print(f\"   âœ… All feature names are consistent!\")\n","\n","    if not stats:\n","        print(\"âŒ No valid samples found\")\n","        return None, None\n","\n","    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯\n","    scalers = {}\n","    print(f\"\\nğŸ”¢ Computing standardization parameters...\")\n","\n","    for feat_name in feature_order:\n","        if feat_name in CONTINUOUS_FEATURES:\n","            if feat_name in stats and stats[feat_name]['count'] > 0:\n","                count = stats[feat_name]['count']\n","                mean = stats[feat_name]['sum'] / count\n","                variance = (stats[feat_name]['sumsq'] / count) - (mean**2)\n","                std = np.sqrt(max(variance, 1e-8))\n","\n","                scalers[feat_name] = {\n","                    'mean': float(mean),\n","                    'std': float(std),\n","                    'count': int(count)\n","                }\n","                print(f\"   {feat_name}: mean={mean:.4f}, std={std:.4f}, count={count}\")\n","            else:\n","                scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                print(f\"   {feat_name}: No valid samples, using defaults\")\n","        elif feat_name in CATEGORICAL_FEATURES:\n","            scalers[feat_name] = {'type': 'categorical'}\n","            print(f\"   {feat_name}: categorical (one-hot)\")\n","        elif feat_name in NON_STANDARDIZED:\n","            scalers[feat_name] = {'type': 'non_standardized'}\n","            print(f\"   {feat_name}: non_standardized\")\n","\n","    return scalers, feature_order\n","\n","def apply_no2_scaler_dictionary_structure(data_dict, feature_order, scalers):\n","    \"\"\"\n","    å¯¹å­—å…¸å¼NO2æ•°æ®åº”ç”¨æ ‡å‡†åŒ–\n","\n","    Args:\n","        data_dict: NO2å­—å…¸å¼æ•°æ®\n","        feature_order: ç‰¹å¾é¡ºåº\n","        scalers: æ ‡å‡†åŒ–å‚æ•°\n","\n","    Returns:\n","        X_scaled: æ ‡å‡†åŒ–åçš„ç‰¹å¾çŸ©é˜µ (C, H, W)\n","    \"\"\"\n","    # æ„å»ºç‰¹å¾çŸ©é˜µ\n","    X = build_feature_matrix_from_dict(data_dict, feature_order)\n","    X_scaled = X.copy().astype(np.float32)\n","\n","    # åº”ç”¨æ ‡å‡†åŒ–\n","    for i, feat_name in enumerate(feature_order):\n","        if feat_name in scalers and 'mean' in scalers[feat_name]:\n","            # å¯¹è¿ç»­ç‰¹å¾åº”ç”¨z-scoreæ ‡å‡†åŒ–\n","            mean = scalers[feat_name]['mean']\n","            std = scalers[feat_name]['std']\n","            X_scaled[i] = (X[i] - mean) / std\n","        # è·³è¿‡åˆ†ç±»å’Œéæ ‡å‡†åŒ–ç‰¹å¾\n","\n","    return X_scaled\n","\n","def save_no2_scaler_dictionary_structure(scalers, feature_order, years):\n","    \"\"\"ä¿å­˜NO2 scalerå‚æ•°\"\"\"\n","    output_dir = \"/content/drive/MyDrive/Scalers\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    output_file = os.path.join(output_dir, f\"no2_scalers_dict_{years[0]}_{years[-1]}.npz\")\n","\n","    metadata = {\n","        'data_type': 'no2_dictionary',\n","        'years': years,\n","        'file_structure': 'dictionary format: {no2_target: array, dem: array, ...}',\n","        'processing': 'build feature matrix â†’ standardize',\n","        'mask_logic': 'mask==1 is valid, mask==0 is invalid',\n","        'target_filtering': 'FILTER_NONPOSITIVE_TARGET=False (includes y=0 as valid)',\n","        'wind_features': 'wd_sin, wd_cos are non-standardized (preserve [-1,1] range)',\n","        'total_features': len(feature_order),\n","        'continuous_features': len([f for f in feature_order if 'mean' in scalers.get(f, {})]),\n","        'categorical_features': len([f for f in feature_order if scalers.get(f, {}).get('type') == 'categorical']),\n","        'non_standardized_features': len([f for f in feature_order if scalers.get(f, {}).get('type') == 'non_standardized'])\n","    }\n","\n","    np.savez_compressed(\n","        output_file,\n","        scalers=scalers,\n","        feature_order=feature_order,\n","        metadata=metadata\n","    )\n","\n","    print(f\"\\nâœ… NO2 Dictionary Scaler saved to: {output_file}\")\n","    return output_file\n","\n","def main():\n","    \"\"\"ä¸»å‡½æ•°\"\"\"\n","    print(\"ğŸš€ NO2 Scaler for Dictionary Structure\")\n","    print(\"=\" * 60)\n","    print(\"ğŸ“‹ This script handles NO2 dictionary-style .npz files\")\n","    print(\"ğŸ“‹ Process: Dictionary â†’ Feature Matrix â†’ Standardization\")\n","\n","    # è®¡ç®—scaler\n","    scalers, feature_order = compute_no2_scaler_dictionary_structure(years=[2019, 2020, 2021])\n","\n","    if scalers is not None:\n","        # ä¿å­˜ç»“æœ\n","        output_file = save_no2_scaler_dictionary_structure(scalers, feature_order, [2019, 2020, 2021])\n","\n","        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯\n","        print(f\"\\nğŸ“Š NO2 Dictionary Scaler Summary:\")\n","        print(f\"   - Total features: {len(feature_order)}\")\n","        print(f\"   - Continuous features: {len([f for f in feature_order if 'mean' in scalers.get(f, {})])}\")\n","        print(f\"   - Categorical features: {len([f for f in feature_order if scalers.get(f, {}).get('type') == 'categorical'])}\")\n","        print(f\"   - Non-standardized features: {len([f for f in feature_order if scalers.get(f, {}).get('type') == 'non_standardized'])}\")\n","\n","        # æ˜¾ç¤ºå…³é”®ç‰¹å¾å‚æ•°\n","        print(f\"\\nğŸ” Key Features Standardization Parameters:\")\n","        key_features = ['dem', 'slope', 'pop', 'u10', 'v10', 't2m', 'no2_lag_1day', 'no2_neighbor']\n","        for feat in key_features:\n","            if feat in scalers and 'mean' in scalers[feat]:\n","                print(f\"   {feat}: mean={scalers[feat]['mean']:.4f}, std={scalers[feat]['std']:.4f}\")\n","            elif feat in scalers:\n","                print(f\"   {feat}: {scalers[feat].get('type', 'unknown type')}\")\n","\n","        print(f\"\\nğŸ‰ NO2 Dictionary Scaler generation completed!\")\n","        print(f\"ğŸ“‹ Next steps:\")\n","        print(f\"   1. âœ… NO2 scaler ready for dictionary structure\")\n","        print(f\"   2. ğŸ”„ Use apply_no2_scaler_dictionary_structure() for standardization\")\n","        print(f\"   3. ğŸ”„ Ready for 3D CNN training\")\n","\n","    else:\n","        print(\"âŒ NO2 scaler computation failed\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29e0zqYSjYB3","executionInfo":{"status":"ok","timestamp":1758163243386,"user_tz":-120,"elapsed":981749,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"9abbb9cc-114a-4e50-dab3-207e2b8a60f4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ NO2 Scaler for Dictionary Structure\n","============================================================\n","ğŸ“‹ This script handles NO2 dictionary-style .npz files\n","ğŸ“‹ Process: Dictionary â†’ Feature Matrix â†’ Standardization\n","ğŸš€ Computing NO2 Scaler for Dictionary Structure\n","============================================================\n","ğŸ“‹ NO2 file structure: Dictionary format\n","ğŸ“‹ Process: Build feature matrix â†’ Compute statistics â†’ Generate scaler\n","ğŸ“Š Feature configuration:\n","   - Continuous features: 14\n","   - Categorical features: 10\n","   - Non-standardized features: 5\n","   - Total features: 29\n","   - æ”¹è¿›: é£å‘ç‰¹å¾(wd_sin, wd_cos)ä¸æ ‡å‡†åŒ–ï¼Œä¿æŒ[-1,1]èŒƒå›´\n","   - æ”¹è¿›: æœ‰æ•ˆåƒç´ åˆ¤å®šå¯é…ç½®ï¼Œé»˜è®¤ä¸è¿‡æ»¤y<=0\n","\n","ğŸ” Processing NO2 dictionary files...\n","ğŸ“… Processing 2019: 365 files\n","   âš ï¸ NO2_stack_20190201.npz: No valid pixels\n","   ğŸ“Š 2019: 50/365 files (13.7%), rate: 8.9 files/sec\n","   âš ï¸ NO2_stack_20190403.npz: No valid pixels\n","   ğŸ“Š 2019: 100/365 files (27.4%), rate: 9.5 files/sec\n","   âš ï¸ NO2_stack_20190415.npz: No valid pixels\n","   ğŸ“Š 2019: 150/365 files (41.1%), rate: 9.8 files/sec\n","   âš ï¸ NO2_stack_20190715.npz: No valid pixels\n","   ğŸ“Š 2019: 200/365 files (54.8%), rate: 9.6 files/sec\n","   ğŸ“Š 2019: 250/365 files (68.5%), rate: 9.7 files/sec\n","   ğŸ“Š 2019: 300/365 files (82.2%), rate: 9.6 files/sec\n","   âš ï¸ NO2_stack_20191112.npz: No valid pixels\n","   ğŸ“Š 2019: 350/365 files (95.9%), rate: 9.8 files/sec\n","   âš ï¸ NO2_stack_20191220.npz: No valid pixels\n","   âš ï¸ NO2_stack_20191231.npz: No valid pixels\n","ğŸ“… Processing 2020: 366 files\n","   ğŸ“Š 2020: 50/366 files (13.7%), rate: 0.5 files/sec\n","   ğŸ“Š 2020: 100/366 files (27.3%), rate: 0.6 files/sec\n","   ğŸ“Š 2020: 150/366 files (41.0%), rate: 0.7 files/sec\n","   âš ï¸ NO2_stack_20200612.npz: No valid pixels\n","   âš ï¸ NO2_stack_20200704.npz: No valid pixels\n","   âš ï¸ NO2_stack_20200715.npz: No valid pixels\n","   ğŸ“Š 2020: 200/366 files (54.6%), rate: 0.7 files/sec\n","   âš ï¸ NO2_stack_20200722.npz: No valid pixels\n","   âš ï¸ NO2_stack_20200726.npz: No valid pixels\n","   âš ï¸ NO2_stack_20200801.npz: No valid pixels\n","   ğŸ“Š 2020: 250/366 files (68.3%), rate: 0.7 files/sec\n","   âš ï¸ NO2_stack_20200914.npz: No valid pixels\n","   âš ï¸ NO2_stack_20201002.npz: No valid pixels\n","   ğŸ“Š 2020: 300/366 files (82.0%), rate: 0.7 files/sec\n","   âš ï¸ NO2_stack_20201204.npz: No valid pixels\n","   ğŸ“Š 2020: 350/366 files (95.6%), rate: 0.7 files/sec\n","   âš ï¸ NO2_stack_20201231.npz: No valid pixels\n","ğŸ“… Processing 2021: 365 files\n","   âš ï¸ NO2_stack_20210101.npz: No valid pixels\n","   âš ï¸ NO2_stack_20210105.npz: No valid pixels\n","   ğŸ“Š 2021: 50/365 files (13.7%), rate: 0.1 files/sec\n","   ğŸ“Š 2021: 100/365 files (27.4%), rate: 0.2 files/sec\n","   ğŸ“Š 2021: 150/365 files (41.1%), rate: 0.2 files/sec\n","   ğŸ“Š 2021: 200/365 files (54.8%), rate: 0.3 files/sec\n","   ğŸ“Š 2021: 250/365 files (68.5%), rate: 0.3 files/sec\n","   âš ï¸ NO2_stack_20210926.npz: No valid pixels\n","   âš ï¸ NO2_stack_20210929.npz: No valid pixels\n","   ğŸ“Š 2021: 300/365 files (82.2%), rate: 0.3 files/sec\n","   âš ï¸ NO2_stack_20211208.npz: No valid pixels\n","   âš ï¸ NO2_stack_20211211.npz: No valid pixels\n","   ğŸ“Š 2021: 350/365 files (95.9%), rate: 0.4 files/sec\n","   âš ï¸ NO2_stack_20211225.npz: No valid pixels\n","   âš ï¸ NO2_stack_20211231.npz: No valid pixels\n","\n","ğŸ“Š Processing statistics:\n","   - Total files: 1096\n","   - Processed: 1071\n","   - Failed: 25\n","   - Success rate: 97.7%\n","\n","ğŸ” Feature consistency check:\n","   âœ… All feature names are consistent!\n","\n","ğŸ”¢ Computing standardization parameters...\n","   u10: mean=-0.2277, std=1.5682, count=1053253\n","   v10: mean=0.1378, std=1.4416, count=1053253\n","   blh: mean=950.0508, std=559.0005, count=1053253\n","   tp: mean=0.0003, std=0.0009, count=1053253\n","   t2m: mean=15.7258, std=9.9254, count=1053253\n","   sp: mean=93981.8056, std=7744.4218, count=1053253\n","   str: mean=-1345259.2311, std=429521.3468, count=1053253\n","   ssr_clr: mean=6696537.3109, std=2633092.0839, count=1053253\n","   ws: mean=1.7714, std=1.2186, count=1053253\n","   no2_lag_1day: mean=0.0000, std=0.0001, count=1033986\n","   no2_neighbor: mean=0.0001, std=0.0001, count=1053253\n","   dem: mean=618.7810, std=781.1017, count=1053253\n","   slope: mean=11.4541, std=11.0903, count=993467\n","   pop: mean=135.3734, std=370.0605, count=991758\n","   lulc_class_0: categorical (one-hot)\n","   lulc_class_1: categorical (one-hot)\n","   lulc_class_2: categorical (one-hot)\n","   lulc_class_3: categorical (one-hot)\n","   lulc_class_4: categorical (one-hot)\n","   lulc_class_5: categorical (one-hot)\n","   lulc_class_6: categorical (one-hot)\n","   lulc_class_7: categorical (one-hot)\n","   lulc_class_8: categorical (one-hot)\n","   lulc_class_9: categorical (one-hot)\n","   sin_doy: non_standardized\n","   cos_doy: non_standardized\n","   weekday_weight: non_standardized\n","   wd_sin: non_standardized\n","   wd_cos: non_standardized\n","\n","âœ… NO2 Dictionary Scaler saved to: /content/drive/MyDrive/Scalers/no2_scalers_dict_2019_2021.npz\n","\n","ğŸ“Š NO2 Dictionary Scaler Summary:\n","   - Total features: 29\n","   - Continuous features: 14\n","   - Categorical features: 10\n","   - Non-standardized features: 5\n","\n","ğŸ” Key Features Standardization Parameters:\n","   dem: mean=618.7810, std=781.1017\n","   slope: mean=11.4541, std=11.0903\n","   pop: mean=135.3734, std=370.0605\n","   u10: mean=-0.2277, std=1.5682\n","   v10: mean=0.1378, std=1.4416\n","   t2m: mean=15.7258, std=9.9254\n","   no2_lag_1day: mean=0.0000, std=0.0001\n","   no2_neighbor: mean=0.0001, std=0.0001\n","\n","ğŸ‰ NO2 Dictionary Scaler generation completed!\n","ğŸ“‹ Next steps:\n","   1. âœ… NO2 scaler ready for dictionary structure\n","   2. ğŸ”„ Use apply_no2_scaler_dictionary_structure() for standardization\n","   3. ğŸ”„ Ready for 3D CNN training\n"]}]},{"cell_type":"markdown","source":["# 3. Generate SO2 standardization parameters"],"metadata":{"id":"9-fCu_qJhMfq"}},{"cell_type":"code","source":["# Generate SO2 standardization parameters\n","import os, numpy as np\n","from collections import defaultdict\n","\n","# Create output directory\n","os.makedirs(\"/content/drive/MyDrive/Scalers\", exist_ok=True)\n","\n","BASE_SO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","OUT_DIR = \"/content/drive/MyDrive/Scalers\"\n","\n","# Continuous features that need standardization (SO2 version)\n","CONTINUOUS_FEATURES = [\n","    'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clear',\n","    'so2_lag1', 'so2_neighbor', 'so2_climate_prior',  # SO2 related features (removed so2_neighbor_5x5)\n","    'dem', 'slope', 'population',\n","    'ws'  # Wind speed\n","]\n","\n","# Features that should not be standardized\n","NON_STANDARDIZED = ['sin_doy', 'cos_doy', 'weekday_weight', 'wd_sin', 'wd_cos']\n","\n","# Categorical features (one-hot encoded) - SO2 uses lulc_class_10,20,...,100\n","CATEGORICAL_FEATURES = [\n","    'lulc_class_10', 'lulc_class_20', 'lulc_class_30', 'lulc_class_40', 'lulc_class_50',\n","    'lulc_class_60', 'lulc_class_70', 'lulc_class_80', 'lulc_class_90', 'lulc_class_100'\n","]\n","\n","def compute_so2_scalers(years=[2019, 2020, 2021], sample_per_file=1000):\n","    \"\"\"Compute standardization parameters for SO2\"\"\"\n","    print(\"ğŸš€ Starting SO2 standardization parameter computation...\")\n","    print(f\"Data path: {BASE_SO2}\")\n","    print(f\"Training years: {years}\")\n","    print(f\"Samples per file: {sample_per_file}\")\n","\n","    # Set random seed for reproducibility\n","    rng = np.random.default_rng(42)\n","\n","    # Accumulate statistics\n","    stats = defaultdict(lambda: {'sum': 0.0, 'sumsq': 0.0, 'count': 0})\n","    feature_order = None\n","\n","    total_files = 0\n","    processed_files = 0\n","\n","    for year in years:\n","        year_dir = os.path.join(BASE_SO2, f\"SO2_{year}\")\n","        if not os.path.exists(year_dir):\n","            print(f\"âš ï¸ Year directory not found: {year_dir}\")\n","            continue\n","\n","        files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","        total_files += len(files)\n","        print(f\"\\n Processing {year}: {len(files)} files\")\n","\n","        for i, fname in enumerate(files):\n","            if i % 50 == 0:\n","                print(f\"  {i}/{len(files)} ({i/len(files)*100:.1f}%)\")\n","\n","            try:\n","                with np.load(os.path.join(year_dir, fname)) as data:\n","                    X = data['X']  # (C, H, W)\n","\n","                    # CONFIRMED: SO2: mask==1 is valid pixels, mask==0 is invalid\n","                    # Additional robustness: ensure target > 0 for physical validity\n","                    valid_mask = (data['mask'] == 1) & (data['y'] > 0)\n","\n","                    if feature_order is None:\n","                        # Get feature names\n","                        if 'feature_order' in data.files:\n","                            feature_order = data['feature_order'].tolist()\n","                        elif 'feature_names' in data.files:\n","                            feature_order = data['feature_names'].tolist()\n","                        else:\n","                            print(f\"âš ï¸ No feature names found: {fname}\")\n","                            continue\n","\n","                    # --- SO2 é²æ£’æ€§æ£€æŸ¥ï¼šéªŒè¯ç‰¹å¾çŸ©é˜µä¸ç‰¹å¾åä¸€è‡´æ€§ ---\n","                    if X.shape[0] != len(feature_order):\n","                        print(f\"âš ï¸ Feature matrix shape mismatch in {fname}: X.shape[0]={X.shape[0]}, feature_order length={len(feature_order)}\")\n","                        # å¯ä»¥é€‰æ‹©è·³è¿‡æˆ–è°ƒæ•´ï¼Œè¿™é‡Œé€‰æ‹©è·³è¿‡\n","                        continue\n","\n","                    # --- å…³é”®ä¿®æ­£ï¼šæ­£ç¡®å¤„ç†2Dæ©è†œç´¢å¼• ---\n","                    C, H, W = X.shape\n","                    X_flat = X.reshape(C, -1)  # (C, H*W) - æ‰“å¹³ç©ºé—´ç»´åº¦\n","                    valid_flat = valid_mask.ravel()  # (H*W,) - æ‰“å¹³æ©è†œ\n","                    valid_pixels = X_flat[:, valid_flat]  # (C, N_valid) - æ­£ç¡®æå–æœ‰æ•ˆåƒç´ \n","\n","                    if valid_pixels.shape[1] == 0:\n","                        continue\n","\n","                    # Random sampling to avoid memory issues (using fixed seed for reproducibility)\n","                    n_samples = min(sample_per_file, valid_pixels.shape[1])\n","                    if n_samples < valid_pixels.shape[1]:\n","                        indices = rng.choice(valid_pixels.shape[1], n_samples, replace=False)\n","                        sampled = valid_pixels[:, indices]\n","                    else:\n","                        sampled = valid_pixels\n","\n","                    # Filter NaN/inf\n","                    finite_mask = np.isfinite(sampled)\n","\n","                    # Accumulate statistics\n","                    for j, feat_name in enumerate(feature_order):\n","                        if feat_name in CONTINUOUS_FEATURES:\n","                            values = sampled[j]\n","                            finite_values = values[finite_mask[j]]\n","\n","                            if len(finite_values) > 0:\n","                                # 1-99 percentile winsorization\n","                                q1, q99 = np.percentile(finite_values, [1, 99])\n","                                clipped = np.clip(finite_values, q1, q99)\n","\n","                                stats[feat_name]['sum'] += np.sum(clipped)\n","                                stats[feat_name]['sumsq'] += np.sum(clipped**2)\n","                                stats[feat_name]['count'] += len(clipped)\n","                        elif feat_name in CATEGORICAL_FEATURES:\n","                            # Categorical features don't need standardization\n","                            pass\n","                        elif feat_name in NON_STANDARDIZED:\n","                            # Non-standardized features don't need standardization\n","                            pass\n","\n","                    processed_files += 1\n","\n","            except Exception as e:\n","                print(f\"âŒ Failed to load file {fname}: {e}\")\n","                continue\n","\n","    print(f\"\\nğŸ“Š Processing statistics:\")\n","    print(f\"Total files: {total_files}\")\n","    print(f\"Successfully processed: {processed_files}\")\n","    print(f\"Processing rate: {processed_files/total_files*100:.1f}%\")\n","\n","    if not stats:\n","        print(\"âŒ No valid samples found\")\n","        return None\n","\n","    # Compute final statistics\n","    scalers = {}\n","    print(f\"\\nğŸ”¢ Computing standardization parameters...\")\n","\n","    # --- åå­—ä¸€è‡´æ€§è‡ªæ£€ ---\n","    print(f\"\\nğŸ” Feature name consistency check:\")\n","    all_defined_features = set(CONTINUOUS_FEATURES + CATEGORICAL_FEATURES + NON_STANDARDIZED)\n","    feature_order_set = set(feature_order)\n","\n","    missing_in_order = all_defined_features - feature_order_set\n","    extra_in_order = feature_order_set - all_defined_features\n","\n","    if missing_in_order:\n","        print(f\"  âš ï¸ Features defined but missing in feature_order: {missing_in_order}\")\n","    if extra_in_order:\n","        print(f\"  âš ï¸ Features in feature_order but not defined: {extra_in_order}\")\n","    if not missing_in_order and not extra_in_order:\n","        print(f\"  âœ… All feature names are consistent!\")\n","\n","    # æ£€æŸ¥å…³é”®å­—æ®µå\n","    if 'ssr_clear' in feature_order:\n","        print(f\"  âœ… SO2 uses correct field name: ssr_clear\")\n","    else:\n","        print(f\"  âŒ Missing ssr_clear in feature_order\")\n","\n","    for feat_name in feature_order:\n","        if feat_name in CONTINUOUS_FEATURES:\n","            # è¿ç»­ç‰¹å¾ï¼šç»Ÿä¸€å¤„ç†ï¼Œå³ä½¿æ²¡æœ‰ç»Ÿè®¡åˆ°ä¹Ÿç»™é»˜è®¤å€¼\n","            if feat_name in stats:\n","                count = stats[feat_name]['count']\n","                if count > 0:\n","                    mean = stats[feat_name]['sum'] / count\n","                    variance = (stats[feat_name]['sumsq'] / count) - (mean**2)\n","                    std = np.sqrt(max(variance, 1e-8))  # Avoid division by zero\n","\n","                    scalers[feat_name] = {\n","                        'mean': float(mean),\n","                        'std': float(std),\n","                        'count': int(count)\n","                    }\n","                    print(f\"  {feat_name}: mean={mean:.4f}, std={std:.4f}, count={count}\")\n","                else:\n","                    # æœ‰ç»Ÿè®¡ä½†count=0ï¼Œç»™é»˜è®¤å€¼\n","                    scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                    print(f\"  {feat_name}: No valid samples, using default values\")\n","            else:\n","                # å®Œå…¨æ²¡æœ‰ç»Ÿè®¡åˆ°ï¼Œä¹Ÿç»™é»˜è®¤å€¼\n","                scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                print(f\"  {feat_name}: No statistics collected, using default values\")\n","        elif feat_name in CATEGORICAL_FEATURES:\n","            scalers[feat_name] = {'type': 'categorical'}\n","            print(f\"  {feat_name}: categorical (one-hot)\")\n","        elif feat_name in NON_STANDARDIZED:\n","            scalers[feat_name] = {'type': 'non_standardized'}\n","            print(f\"  {feat_name}: non_standardized\")\n","        else:\n","            # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºfeature_orderåº”è¯¥åŒ…å«æ‰€æœ‰ç‰¹å¾\n","            scalers[feat_name] = {'type': 'unexpected'}\n","            print(f\"  {feat_name}: unexpected type - check feature_order definition\")\n","\n","    return scalers, feature_order\n","\n","def apply_so2_scaler(X: np.ndarray, feature_names: list, scaler_file: str = None):\n","    \"\"\"\n","    Apply SO2 standardization to feature matrix\n","\n","    Args:\n","        X: Feature matrix (C, H, W) or (C, N)\n","        feature_names: List of feature names\n","        scaler_file: Path to scaler file (if None, auto-detect)\n","\n","    Returns:\n","        X_scaled: Standardized feature matrix\n","    \"\"\"\n","    if scaler_file is None:\n","        scaler_file = os.path.join(OUT_DIR, \"so2_scalers_2019_2021.npz\")\n","\n","    if not os.path.exists(scaler_file):\n","        raise FileNotFoundError(f\"SO2 scaler file not found: {scaler_file}\")\n","\n","    with np.load(scaler_file, allow_pickle=True) as data:\n","        scalers = data['scalers'].item()\n","\n","    X_scaled = X.copy().astype(np.float32)\n","\n","    for i, feat_name in enumerate(feature_names):\n","        if feat_name in scalers and 'mean' in scalers[feat_name]:\n","            # Apply z-score normalization for continuous features\n","            mean = scalers[feat_name]['mean']\n","            std = scalers[feat_name]['std']\n","            X_scaled[i] = (X[i] - mean) / std\n","        # Skip categorical and non-standardized features (they remain unchanged)\n","\n","    return X_scaled\n","\n","# Compute SO2 standardization parameters\n","print(\"=\" * 60)\n","print(\"ğŸš€ SO2 Standardization Parameter Computation\")\n","print(\"=\" * 60)\n","\n","so2_scalers, so2_features = compute_so2_scalers()\n","\n","if so2_scalers is not None:\n","    # Save results\n","    output_file = os.path.join(OUT_DIR, \"so2_scalers_2019_2021.npz\")\n","    np.savez_compressed(\n","        output_file,\n","        scalers=so2_scalers,\n","        feature_order=so2_features,\n","        metadata={\n","            'data_type': 'so2',\n","            'years': [2019, 2020, 2021],\n","            'mask_logic': 'mask==1 is valid, mask==0 is invalid (CONFIRMED FROM SOURCE CODE)',\n","            'total_features': len(so2_features),\n","            'continuous_features': len([f for f in so2_features if f in CONTINUOUS_FEATURES])\n","        }\n","    )\n","\n","    print(f\"\\nâœ… SO2 standardization parameters saved to: {output_file}\")\n","    print(f\" Feature statistics:\")\n","    print(f\"  Total features: {len(so2_features)}\")\n","    print(f\"  Continuous features: {len([f for f in so2_features if f in CONTINUOUS_FEATURES])}\")\n","    print(f\"  Categorical features: {len([f for f in so2_features if f in CATEGORICAL_FEATURES])}\")\n","    print(f\"  Non-standardized features: {len([f for f in so2_features if f in NON_STANDARDIZED])}\")\n","\n","    # Show parameters for first few continuous features\n","    print(f\"\\nğŸ” Standardization parameters for first 5 continuous features:\")\n","    continuous_count = 0\n","    for feat_name in so2_features:\n","        if feat_name in CONTINUOUS_FEATURES and feat_name in so2_scalers:\n","            if 'mean' in so2_scalers[feat_name]:\n","                print(f\"  {feat_name}: mean={so2_scalers[feat_name]['mean']:.4f}, std={so2_scalers[feat_name]['std']:.4f}\")\n","                continuous_count += 1\n","                if continuous_count >= 5:\n","                    break\n","else:\n","    print(\"âŒ Computation failed\")\n","\n","print(\"\\n Next step: Ready to train SO2 model!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dX5Ck19yfRnW","executionInfo":{"status":"ok","timestamp":1758061412625,"user_tz":-120,"elapsed":1555087,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"814fe962-1817-4bcb-b5b4-b680f77c00cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","ğŸš€ SO2 Standardization Parameter Computation\n","============================================================\n","ğŸš€ Starting SO2 standardization parameter computation...\n","Data path: /content/drive/MyDrive/Feature_Stacks\n","Training years: [2019, 2020, 2021]\n","Samples per file: 1000\n","\n"," Processing 2019: 365 files\n","  0/365 (0.0%)\n","  50/365 (13.7%)\n","  100/365 (27.4%)\n","  150/365 (41.1%)\n","  200/365 (54.8%)\n","  250/365 (68.5%)\n","  300/365 (82.2%)\n","  350/365 (95.9%)\n","\n"," Processing 2020: 366 files\n","  0/366 (0.0%)\n","  50/366 (13.7%)\n","  100/366 (27.3%)\n","  150/366 (41.0%)\n","  200/366 (54.6%)\n","  250/366 (68.3%)\n","  300/366 (82.0%)\n","  350/366 (95.6%)\n","\n"," Processing 2021: 365 files\n","  0/365 (0.0%)\n","  50/365 (13.7%)\n","  100/365 (27.4%)\n","  150/365 (41.1%)\n","  200/365 (54.8%)\n","  250/365 (68.5%)\n","  300/365 (82.2%)\n","  350/365 (95.9%)\n","\n","ğŸ“Š Processing statistics:\n","Total files: 1096\n","Successfully processed: 779\n","Processing rate: 71.1%\n","\n","ğŸ”¢ Computing standardization parameters...\n","\n","ğŸ” Feature name consistency check:\n","  âœ… All feature names are consistent!\n","  âœ… SO2 uses correct field name: ssr_clear\n","  dem: mean=400.9618, std=533.4883, count=761779\n","  slope: mean=9.2690, std=9.5274, count=761779\n","  population: mean=145.4318, std=384.2961, count=761779\n","  lulc_class_10: categorical (one-hot)\n","  lulc_class_20: categorical (one-hot)\n","  lulc_class_30: categorical (one-hot)\n","  lulc_class_40: categorical (one-hot)\n","  lulc_class_50: categorical (one-hot)\n","  lulc_class_60: categorical (one-hot)\n","  lulc_class_70: categorical (one-hot)\n","  lulc_class_80: categorical (one-hot)\n","  lulc_class_90: categorical (one-hot)\n","  lulc_class_100: categorical (one-hot)\n","  u10: mean=-0.4001, std=1.7508, count=761779\n","  v10: mean=0.2341, std=1.5989, count=761779\n","  ws: mean=2.0222, std=1.3279, count=761779\n","  wd_sin: non_standardized\n","  wd_cos: non_standardized\n","  blh: mean=1191.6626, std=466.9831, count=761779\n","  tp: mean=0.0003, std=0.0010, count=761779\n","  t2m: mean=20.3405, std=6.8925, count=761779\n","  sp: mean=96270.7153, std=5620.0365, count=761779\n","  str: mean=-1467077.4248, std=368888.7481, count=761779\n","  ssr_clear: mean=8178523.0930, std=1625752.2839, count=761779\n","  so2_lag1: mean=0.0000, std=0.0001, count=761779\n","  so2_neighbor: mean=0.0006, std=0.0009, count=761779\n","  so2_climate_prior: mean=0.0006, std=0.0004, count=761779\n","  sin_doy: non_standardized\n","  cos_doy: non_standardized\n","  weekday_weight: non_standardized\n","\n","âœ… SO2 standardization parameters saved to: /content/drive/MyDrive/Scalers/so2_scalers_2019_2021.npz\n"," Feature statistics:\n","  Total features: 30\n","  Continuous features: 15\n","  Categorical features: 10\n","  Non-standardized features: 5\n","\n","ğŸ” Standardization parameters for first 5 continuous features:\n","  dem: mean=400.9618, std=533.4883\n","  slope: mean=9.2690, std=9.5274\n","  population: mean=145.4318, std=384.2961\n","  u10: mean=-0.4001, std=1.7508\n","  v10: mean=0.2341, std=1.5989\n","\n"," Next step: Ready to train SO2 model!\n"]}]},{"cell_type":"code","source":["# Quick Check SO2 Scaler Results\n","import os\n","import numpy as np\n","\n","def quick_check_so2_scaler_results():\n","    \"\"\"å¿«é€Ÿæ£€æŸ¥SO2 scalerç»“æœ\"\"\"\n","    print(\"ğŸ” Quick SO2 Scaler Results Check\")\n","    print(\"=\" * 50)\n","\n","    # æ£€æŸ¥scaleræ–‡ä»¶\n","    scaler_file = \"/content/drive/MyDrive/Scalers/so2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(\"âŒ SO2 scaler file not found\")\n","        return\n","\n","    print(\"âœ… Scaler file exists\")\n","\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            print(\"âœ… File loaded successfully\")\n","\n","            # æ£€æŸ¥åŸºæœ¬ç»“æ„\n","            print(f\"ğŸ“Š Available keys: {list(data.files)}\")\n","\n","            # æ£€æŸ¥å…ƒæ•°æ®\n","            if 'metadata' in data.files:\n","                metadata = data['metadata'].item()\n","                print(f\"ğŸ“… Years: {metadata.get('years', 'Unknown')}\")\n","                print(f\"ğŸ“ Total files: {metadata.get('total_files', 'Unknown')}\")\n","                print(f\"ğŸ¯ Success rate: {metadata.get('success_rate', 'Unknown')}\")\n","                print(f\"ğŸ“Š Total samples: {metadata.get('total_samples', 'Unknown')}\")\n","\n","            # æ£€æŸ¥scalerå‚æ•°\n","            if 'scalers' in data.files:\n","                scalers = data['scalers'].item()\n","                print(f\"ğŸ“Š Total features: {len(scalers)}\")\n","\n","                # åˆ†æè¿ç»­ç‰¹å¾\n","                continuous_features = []\n","                categorical_features = []\n","                non_standardized_features = []\n","\n","                for feat_name, feat_info in scalers.items():\n","                    if isinstance(feat_info, dict):\n","                        if 'mean' in feat_info and 'std' in feat_info:\n","                            continuous_features.append(feat_name)\n","                        elif 'categorical' in feat_info:\n","                            categorical_features.append(feat_name)\n","                        else:\n","                            non_standardized_features.append(feat_name)\n","\n","                print(f\"ğŸ“Š Feature type summary:\")\n","                print(f\"  Continuous features: {len(continuous_features)}\")\n","                print(f\"  Categorical features: {len(categorical_features)}\")\n","                print(f\"  Non-standardized features: {len(non_standardized_features)}\")\n","\n","                # æ£€æŸ¥å…³é”®è¿ç»­ç‰¹å¾\n","                print(f\"\\nğŸ¯ Key Continuous Features Analysis:\")\n","                key_features = ['dem', 'slope', 'population', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clear', 'ws', 'so2_lag1', 'so2_neighbor', 'so2_climate_prior']\n","\n","                for feat_name in key_features:\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","                        count = scalers[feat_name].get('count', 'Unknown')\n","                        print(f\"  âœ… {feat_name}: {count:,} samples, mean={mean:.4f}, std={std:.4f}\")\n","                    else:\n","                        print(f\"  âŒ {feat_name}: Not found or no statistics\")\n","\n","                # æ£€æŸ¥é—®é¢˜ç‰¹å¾\n","                print(f\"\\nğŸ” Problem Feature Check:\")\n","                problem_features = ['sp', 'str', 'ssr_clear']\n","\n","                for feat_name in problem_features:\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","                        count = scalers[feat_name].get('count', 'Unknown')\n","\n","                        if abs(mean) < 1e-6 and abs(std) < 1e-6:\n","                            print(f\"  âš ï¸ {feat_name}: Near-zero values (mean={mean:.6f}, std={std:.6f})\")\n","                        elif count == 0:\n","                            print(f\"  âš ï¸ {feat_name}: No samples\")\n","                        else:\n","                            print(f\"  âœ… {feat_name}: Normal values (mean={mean:.4f}, std={std:.4f})\")\n","                    else:\n","                        print(f\"  âŒ {feat_name}: Not found\")\n","\n","                # è®¡ç®—æˆåŠŸç‡\n","                total_continuous = len(continuous_features)\n","                valid_continuous = sum(1 for f in key_features if f in scalers and 'mean' in scalers[f])\n","                success_rate = valid_continuous / len(key_features) * 100\n","\n","                print(f\"\\nğŸ¯ Success Rate: {success_rate:.1f}%\")\n","\n","                if success_rate >= 90:\n","                    print(\"ğŸ‰ SO2 Scaler looks excellent!\")\n","                elif success_rate >= 80:\n","                    print(\"âœ… SO2 Scaler looks good!\")\n","                elif success_rate >= 70:\n","                    print(\"âš ï¸ SO2 Scaler has some issues\")\n","                else:\n","                    print(\"âŒ SO2 Scaler has significant issues\")\n","\n","                # æ£€æŸ¥æ•°æ®è´¨é‡\n","                print(f\"\\nğŸ“Š Data Quality Assessment:\")\n","\n","                # æ£€æŸ¥æ ·æœ¬æ•°é‡\n","                sample_counts = [scalers[f].get('count', 0) for f in continuous_features if f in scalers and 'count' in scalers[f]]\n","                if sample_counts:\n","                    min_samples = min(sample_counts)\n","                    max_samples = max(sample_counts)\n","                    avg_samples = sum(sample_counts) / len(sample_counts)\n","\n","                    print(f\"  Sample count range: {min_samples:,} - {max_samples:,}\")\n","                    print(f\"  Average samples: {avg_samples:,.0f}\")\n","\n","                    if min_samples < 1000:\n","                        print(f\"  âš ï¸ Some features have very low sample counts\")\n","                    elif max_samples - min_samples > max_samples * 0.1:\n","                        print(f\"  âš ï¸ Sample count variation is high\")\n","                    else:\n","                        print(f\"  âœ… Sample counts are consistent\")\n","\n","                # æ£€æŸ¥æ•°å€¼èŒƒå›´\n","                print(f\"\\nğŸ” Value Range Analysis:\")\n","                for feat_name in ['sp', 'str', 'ssr_clear']:\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","\n","                        if feat_name == 'sp':\n","                            if 90000 < mean < 110000:\n","                                print(f\"  âœ… {feat_name}: Pressure values look normal\")\n","                            else:\n","                                print(f\"  âš ï¸ {feat_name}: Pressure values may be unusual\")\n","                        elif feat_name == 'str':\n","                            if mean < 0:\n","                                print(f\"  âœ… {feat_name}: Thermal radiation values look normal (negative)\")\n","                            else:\n","                                print(f\"  âš ï¸ {feat_name}: Thermal radiation values may be unusual\")\n","                        elif feat_name == 'ssr_clear':\n","                            if 0 < mean < 10000000:\n","                                print(f\"  âœ… {feat_name}: Solar radiation values look normal\")\n","                            else:\n","                                print(f\"  âš ï¸ {feat_name}: Solar radiation values may be unusual\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error loading scaler file: {e}\")\n","\n","if __name__ == \"__main__\":\n","    quick_check_so2_scaler_results()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NRhXOil3px5d","executionInfo":{"status":"ok","timestamp":1758061864726,"user_tz":-120,"elapsed":74,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"81463410-fc4d-4c79-df6f-092dc3c09786"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” Quick SO2 Scaler Results Check\n","==================================================\n","âœ… Scaler file exists\n","âœ… File loaded successfully\n","ğŸ“Š Available keys: ['scalers', 'feature_order', 'metadata']\n","ğŸ“… Years: [2019, 2020, 2021]\n","ğŸ“ Total files: Unknown\n","ğŸ¯ Success rate: Unknown\n","ğŸ“Š Total samples: Unknown\n","ğŸ“Š Total features: 30\n","ğŸ“Š Feature type summary:\n","  Continuous features: 15\n","  Categorical features: 0\n","  Non-standardized features: 15\n","\n","ğŸ¯ Key Continuous Features Analysis:\n","  âœ… dem: 761,779 samples, mean=400.9618, std=533.4883\n","  âœ… slope: 761,779 samples, mean=9.2690, std=9.5274\n","  âœ… population: 761,779 samples, mean=145.4318, std=384.2961\n","  âœ… u10: 761,779 samples, mean=-0.4001, std=1.7508\n","  âœ… v10: 761,779 samples, mean=0.2341, std=1.5989\n","  âœ… blh: 761,779 samples, mean=1191.6626, std=466.9831\n","  âœ… tp: 761,779 samples, mean=0.0003, std=0.0010\n","  âœ… t2m: 761,779 samples, mean=20.3405, std=6.8925\n","  âœ… sp: 761,779 samples, mean=96270.7153, std=5620.0365\n","  âœ… str: 761,779 samples, mean=-1467077.4248, std=368888.7481\n","  âœ… ssr_clear: 761,779 samples, mean=8178523.0930, std=1625752.2839\n","  âœ… ws: 761,779 samples, mean=2.0222, std=1.3279\n","  âœ… so2_lag1: 761,779 samples, mean=0.0000, std=0.0001\n","  âœ… so2_neighbor: 761,779 samples, mean=0.0006, std=0.0009\n","  âœ… so2_climate_prior: 761,779 samples, mean=0.0006, std=0.0004\n","\n","ğŸ” Problem Feature Check:\n","  âœ… sp: Normal values (mean=96270.7153, std=5620.0365)\n","  âœ… str: Normal values (mean=-1467077.4248, std=368888.7481)\n","  âœ… ssr_clear: Normal values (mean=8178523.0930, std=1625752.2839)\n","\n","ğŸ¯ Success Rate: 100.0%\n","ğŸ‰ SO2 Scaler looks excellent!\n","\n","ğŸ“Š Data Quality Assessment:\n","  Sample count range: 761,779 - 761,779\n","  Average samples: 761,779\n","  âœ… Sample counts are consistent\n","\n","ğŸ” Value Range Analysis:\n","  âœ… sp: Pressure values look normal\n","  âœ… str: Thermal radiation values look normal (negative)\n","  âœ… ssr_clear: Solar radiation values look normal\n"]}]},{"cell_type":"code","source":["# Validate SO2 Scaler Results\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def validate_so2_scaler_results():\n","    \"\"\"éªŒè¯SO2 scalerç»“æœ\"\"\"\n","    print(\"ğŸ” SO2 Scaler Results Validation\")\n","    print(\"=\" * 50)\n","\n","    # æ£€æŸ¥scaleræ–‡ä»¶\n","    scaler_file = \"/content/drive/MyDrive/Scalers/so2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(\"âŒ SO2 scaler file not found\")\n","        return\n","\n","    print(\"âœ… Scaler file exists\")\n","\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            print(\"âœ… File loaded successfully\")\n","\n","            # æ£€æŸ¥åŸºæœ¬ç»“æ„\n","            print(f\"ğŸ“Š Available keys: {list(data.files)}\")\n","\n","            # æ£€æŸ¥å…ƒæ•°æ®\n","            if 'metadata' in data.files:\n","                metadata = data['metadata'].item()\n","                print(f\"\\nğŸ“‹ Metadata:\")\n","                print(f\"  Years: {metadata.get('years', 'Unknown')}\")\n","                print(f\"  Total files: {metadata.get('total_files', 'Unknown')}\")\n","                print(f\"  Success rate: {metadata.get('success_rate', 'Unknown')}\")\n","                print(f\"  Total samples: {metadata.get('total_samples', 'Unknown')}\")\n","                print(f\"  Processing time: {metadata.get('processing_time', 'Unknown')}\")\n","                print(f\"  Mask logic: {metadata.get('mask_logic', 'Unknown')}\")\n","\n","            # æ£€æŸ¥scalerå‚æ•°\n","            if 'scalers' in data.files:\n","                scalers = data['scalers'].item()\n","                print(f\"\\nğŸ“Š Scaler Parameters:\")\n","                print(f\"  Total features: {len(scalers)}\")\n","\n","                # åˆ†æç‰¹å¾ç±»å‹\n","                continuous_features = []\n","                categorical_features = []\n","                non_standardized_features = []\n","\n","                for feat_name, feat_info in scalers.items():\n","                    if isinstance(feat_info, dict):\n","                        if 'mean' in feat_info and 'std' in feat_info:\n","                            continuous_features.append(feat_name)\n","                        elif 'categorical' in feat_info:\n","                            categorical_features.append(feat_name)\n","                        else:\n","                            non_standardized_features.append(feat_name)\n","\n","                print(f\"  Continuous features: {len(continuous_features)}\")\n","                print(f\"  Categorical features: {len(categorical_features)}\")\n","                print(f\"  Non-standardized features: {len(non_standardized_features)}\")\n","\n","                # è¯¦ç»†åˆ†æè¿ç»­ç‰¹å¾\n","                print(f\"\\nğŸ” Continuous Features Analysis:\")\n","                for feat_name in continuous_features:\n","                    if feat_name in scalers:\n","                        feat_info = scalers[feat_name]\n","                        mean = feat_info.get('mean', 0)\n","                        std = feat_info.get('std', 0)\n","                        count = feat_info.get('count', 0)\n","\n","                        print(f\"  {feat_name}:\")\n","                        print(f\"    Mean: {mean:.6f}\")\n","                        print(f\"    Std: {std:.6f}\")\n","                        print(f\"    Count: {count:,}\")\n","\n","                        # æ£€æŸ¥æ•°æ®è´¨é‡\n","                        if count == 0:\n","                            print(f\"    âš ï¸ No samples\")\n","                        elif abs(mean) < 1e-6 and abs(std) < 1e-6:\n","                            print(f\"    âš ï¸ Near-zero values\")\n","                        elif std == 0:\n","                            print(f\"    âš ï¸ Zero standard deviation\")\n","                        else:\n","                            print(f\"    âœ… Normal statistics\")\n","\n","                # æ£€æŸ¥å…³é”®ç‰¹å¾\n","                print(f\"\\nğŸ¯ Key Features Check:\")\n","                key_features = {\n","                    'dem': 'Elevation (m)',\n","                    'slope': 'Slope (degrees)',\n","                    'population': 'Population density',\n","                    'u10': 'U-wind component (m/s)',\n","                    'v10': 'V-wind component (m/s)',\n","                    'blh': 'Boundary layer height (m)',\n","                    'tp': 'Total precipitation (m)',\n","                    't2m': 'Temperature (K)',\n","                    'sp': 'Surface pressure (Pa)',\n","                    'str': 'Surface thermal radiation (J/mÂ²)',\n","                    'ssr_clear': 'Clear-sky solar radiation (J/mÂ²)',\n","                    'ws': 'Wind speed (m/s)',\n","                    'so2_lag1': 'SO2 lag-1 day',\n","                    'so2_neighbor': 'SO2 neighbor mean',\n","                    'so2_climate_prior': 'SO2 climate prior'\n","                }\n","\n","                for feat_name, description in key_features.items():\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","                        count = scalers[feat_name].get('count', 0)\n","\n","                        print(f\"  âœ… {feat_name} ({description}):\")\n","                        print(f\"    Mean: {mean:.4f}\")\n","                        print(f\"    Std: {std:.4f}\")\n","                        print(f\"    Samples: {count:,}\")\n","\n","                        # ç‰©ç†åˆç†æ€§æ£€æŸ¥\n","                        if feat_name == 'sp' and (mean < 80000 or mean > 110000):\n","                            print(f\"    âš ï¸ Pressure values may be unusual\")\n","                        elif feat_name == 't2m' and (mean < 250 or mean > 320):\n","                            print(f\"    âš ï¸ Temperature values may be unusual\")\n","                        elif feat_name == 'str' and mean > 0:\n","                            print(f\"    âš ï¸ Thermal radiation should be negative\")\n","                        elif feat_name == 'ssr_clear' and (mean < 0 or mean > 20000000):\n","                            print(f\"    âš ï¸ Solar radiation values may be unusual\")\n","                    else:\n","                        print(f\"  âŒ {feat_name}: Not found\")\n","\n","                # è®¡ç®—è´¨é‡è¯„åˆ†\n","                print(f\"\\nğŸ“Š Quality Assessment:\")\n","\n","                # æ£€æŸ¥æ ·æœ¬æ•°é‡ä¸€è‡´æ€§\n","                sample_counts = [scalers[f].get('count', 0) for f in continuous_features if f in scalers and 'count' in scalers[f]]\n","                if sample_counts:\n","                    min_samples = min(sample_counts)\n","                    max_samples = max(sample_counts)\n","                    avg_samples = sum(sample_counts) / len(sample_counts)\n","\n","                    print(f\"  Sample count analysis:\")\n","                    print(f\"    Min: {min_samples:,}\")\n","                    print(f\"    Max: {max_samples:,}\")\n","                    print(f\"    Average: {avg_samples:,.0f}\")\n","                    print(f\"    Variation: {((max_samples - min_samples) / avg_samples * 100):.1f}%\")\n","\n","                    if (max_samples - min_samples) / avg_samples < 0.05:\n","                        print(f\"    âœ… Sample counts are very consistent\")\n","                    elif (max_samples - min_samples) / avg_samples < 0.1:\n","                        print(f\"    âœ… Sample counts are consistent\")\n","                    else:\n","                        print(f\"    âš ï¸ Sample counts vary significantly\")\n","\n","                # æ£€æŸ¥æ•°å€¼èŒƒå›´\n","                print(f\"\\nğŸ” Value Range Analysis:\")\n","                for feat_name in ['sp', 'str', 'ssr_clear']:\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","\n","                        if feat_name == 'sp':\n","                            if 90000 < mean < 110000:\n","                                print(f\"  âœ… {feat_name}: Pressure values are in normal range\")\n","                            else:\n","                                print(f\"  âš ï¸ {feat_name}: Pressure values may be unusual\")\n","                        elif feat_name == 'str':\n","                            if mean < 0:\n","                                print(f\"  âœ… {feat_name}: Thermal radiation values are normal (negative)\")\n","                            else:\n","                                print(f\"  âš ï¸ {feat_name}: Thermal radiation values may be unusual\")\n","                        elif feat_name == 'ssr_clear':\n","                            if 0 < mean < 20000000:\n","                                print(f\"  âœ… {feat_name}: Solar radiation values are in normal range\")\n","                            else:\n","                                print(f\"  âš ï¸ {feat_name}: Solar radiation values may be unusual\")\n","\n","                # æ€»ä½“è´¨é‡è¯„åˆ†\n","                quality_score = calculate_quality_score(scalers, continuous_features)\n","                print(f\"\\nğŸ¯ Overall Quality Score: {quality_score:.1f}/100\")\n","\n","                if quality_score >= 90:\n","                    print(\"ğŸ‰ Excellent quality! Ready for training.\")\n","                elif quality_score >= 80:\n","                    print(\"âœ… Good quality! Ready for training.\")\n","                elif quality_score >= 70:\n","                    print(\"âš ï¸ Acceptable quality. Consider reviewing some features.\")\n","                else:\n","                    print(\"âŒ Poor quality. Significant issues need to be addressed.\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error loading scaler file: {e}\")\n","\n","def calculate_quality_score(scalers, continuous_features):\n","    \"\"\"è®¡ç®—è´¨é‡è¯„åˆ†\"\"\"\n","    score = 100.0\n","\n","    # æ£€æŸ¥è¿ç»­ç‰¹å¾\n","    for feat_name in continuous_features:\n","        if feat_name in scalers:\n","            feat_info = scalers[feat_name]\n","            count = feat_info.get('count', 0)\n","            mean = feat_info.get('mean', 0)\n","            std = feat_info.get('std', 0)\n","\n","            # æ ·æœ¬æ•°é‡æ£€æŸ¥\n","            if count == 0:\n","                score -= 10\n","            elif count < 1000:\n","                score -= 5\n","\n","            # æ•°å€¼åˆç†æ€§æ£€æŸ¥\n","            if abs(mean) < 1e-6 and abs(std) < 1e-6:\n","                score -= 15\n","            elif std == 0:\n","                score -= 10\n","\n","            # ç‰©ç†åˆç†æ€§æ£€æŸ¥\n","            if feat_name == 'sp' and (mean < 80000 or mean > 110000):\n","                score -= 5\n","            elif feat_name == 't2m' and (mean < 250 or mean > 320):\n","                score -= 5\n","            elif feat_name == 'str' and mean > 0:\n","                score -= 5\n","            elif feat_name == 'ssr_clear' and (mean < 0 or mean > 20000000):\n","                score -= 5\n","\n","    return max(0, score)\n","\n","if __name__ == \"__main__\":\n","    validate_so2_scaler_results()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAtUl5mgp2ZD","executionInfo":{"status":"ok","timestamp":1758061883016,"user_tz":-120,"elapsed":56,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"45336405-2ba5-4086-bd5b-81be9ecc0523"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” SO2 Scaler Results Validation\n","==================================================\n","âœ… Scaler file exists\n","âœ… File loaded successfully\n","ğŸ“Š Available keys: ['scalers', 'feature_order', 'metadata']\n","\n","ğŸ“‹ Metadata:\n","  Years: [2019, 2020, 2021]\n","  Total files: Unknown\n","  Success rate: Unknown\n","  Total samples: Unknown\n","  Processing time: Unknown\n","  Mask logic: mask==1 is valid, mask==0 is invalid (CONFIRMED FROM SOURCE CODE)\n","\n","ğŸ“Š Scaler Parameters:\n","  Total features: 30\n","  Continuous features: 15\n","  Categorical features: 0\n","  Non-standardized features: 15\n","\n","ğŸ” Continuous Features Analysis:\n","  dem:\n","    Mean: 400.961806\n","    Std: 533.488280\n","    Count: 761,779\n","    âœ… Normal statistics\n","  slope:\n","    Mean: 9.268992\n","    Std: 9.527353\n","    Count: 761,779\n","    âœ… Normal statistics\n","  population:\n","    Mean: 145.431770\n","    Std: 384.296097\n","    Count: 761,779\n","    âœ… Normal statistics\n","  u10:\n","    Mean: -0.400131\n","    Std: 1.750753\n","    Count: 761,779\n","    âœ… Normal statistics\n","  v10:\n","    Mean: 0.234117\n","    Std: 1.598915\n","    Count: 761,779\n","    âœ… Normal statistics\n","  ws:\n","    Mean: 2.022176\n","    Std: 1.327896\n","    Count: 761,779\n","    âœ… Normal statistics\n","  blh:\n","    Mean: 1191.662584\n","    Std: 466.983097\n","    Count: 761,779\n","    âœ… Normal statistics\n","  tp:\n","    Mean: 0.000326\n","    Std: 0.000961\n","    Count: 761,779\n","    âœ… Normal statistics\n","  t2m:\n","    Mean: 20.340453\n","    Std: 6.892523\n","    Count: 761,779\n","    âœ… Normal statistics\n","  sp:\n","    Mean: 96270.715305\n","    Std: 5620.036544\n","    Count: 761,779\n","    âœ… Normal statistics\n","  str:\n","    Mean: -1467077.424804\n","    Std: 368888.748057\n","    Count: 761,779\n","    âœ… Normal statistics\n","  ssr_clear:\n","    Mean: 8178523.092988\n","    Std: 1625752.283885\n","    Count: 761,779\n","    âœ… Normal statistics\n","  so2_lag1:\n","    Mean: 0.000001\n","    Std: 0.000100\n","    Count: 761,779\n","    âœ… Normal statistics\n","  so2_neighbor:\n","    Mean: 0.000626\n","    Std: 0.000903\n","    Count: 761,779\n","    âœ… Normal statistics\n","  so2_climate_prior:\n","    Mean: 0.000623\n","    Std: 0.000367\n","    Count: 761,779\n","    âœ… Normal statistics\n","\n","ğŸ¯ Key Features Check:\n","  âœ… dem (Elevation (m)):\n","    Mean: 400.9618\n","    Std: 533.4883\n","    Samples: 761,779\n","  âœ… slope (Slope (degrees)):\n","    Mean: 9.2690\n","    Std: 9.5274\n","    Samples: 761,779\n","  âœ… population (Population density):\n","    Mean: 145.4318\n","    Std: 384.2961\n","    Samples: 761,779\n","  âœ… u10 (U-wind component (m/s)):\n","    Mean: -0.4001\n","    Std: 1.7508\n","    Samples: 761,779\n","  âœ… v10 (V-wind component (m/s)):\n","    Mean: 0.2341\n","    Std: 1.5989\n","    Samples: 761,779\n","  âœ… blh (Boundary layer height (m)):\n","    Mean: 1191.6626\n","    Std: 466.9831\n","    Samples: 761,779\n","  âœ… tp (Total precipitation (m)):\n","    Mean: 0.0003\n","    Std: 0.0010\n","    Samples: 761,779\n","  âœ… t2m (Temperature (K)):\n","    Mean: 20.3405\n","    Std: 6.8925\n","    Samples: 761,779\n","    âš ï¸ Temperature values may be unusual\n","  âœ… sp (Surface pressure (Pa)):\n","    Mean: 96270.7153\n","    Std: 5620.0365\n","    Samples: 761,779\n","  âœ… str (Surface thermal radiation (J/mÂ²)):\n","    Mean: -1467077.4248\n","    Std: 368888.7481\n","    Samples: 761,779\n","  âœ… ssr_clear (Clear-sky solar radiation (J/mÂ²)):\n","    Mean: 8178523.0930\n","    Std: 1625752.2839\n","    Samples: 761,779\n","  âœ… ws (Wind speed (m/s)):\n","    Mean: 2.0222\n","    Std: 1.3279\n","    Samples: 761,779\n","  âœ… so2_lag1 (SO2 lag-1 day):\n","    Mean: 0.0000\n","    Std: 0.0001\n","    Samples: 761,779\n","  âœ… so2_neighbor (SO2 neighbor mean):\n","    Mean: 0.0006\n","    Std: 0.0009\n","    Samples: 761,779\n","  âœ… so2_climate_prior (SO2 climate prior):\n","    Mean: 0.0006\n","    Std: 0.0004\n","    Samples: 761,779\n","\n","ğŸ“Š Quality Assessment:\n","  Sample count analysis:\n","    Min: 761,779\n","    Max: 761,779\n","    Average: 761,779\n","    Variation: 0.0%\n","    âœ… Sample counts are very consistent\n","\n","ğŸ” Value Range Analysis:\n","  âœ… sp: Pressure values are in normal range\n","  âœ… str: Thermal radiation values are normal (negative)\n","  âœ… ssr_clear: Solar radiation values are in normal range\n","\n","ğŸ¯ Overall Quality Score: 95.0/100\n","ğŸ‰ Excellent quality! Ready for training.\n"]}]},{"cell_type":"code","source":["# æ£€æŸ¥å®é™…æ–‡ä»¶çŠ¶æ€\n","import os\n","\n","def check_manifest_status():\n","    \"\"\"æ£€æŸ¥SO2å’ŒNO2çš„manifestæ–‡ä»¶çŠ¶æ€\"\"\"\n","    print(\"ğŸ” æ£€æŸ¥manifestæ–‡ä»¶çŠ¶æ€...\")\n","\n","    # æ£€æŸ¥SO2\n","    so2_manifest_paths = [\n","        \"/content/drive/MyDrive/Feature_Stacks/so2_feature_stacks/so2_feature_stacks_manifest.csv\",\n","        \"/content/drive/MyDrive/so2_train_20250911_080237.txt\",\n","        \"/content/drive/MyDrive/so2_val_20250911_080237.txt\",\n","        \"/content/drive/MyDrive/so2_test_20250911_080237.txt\"\n","    ]\n","\n","    print(\"\\nğŸ“Š SO2æ–‡ä»¶çŠ¶æ€:\")\n","    for path in so2_manifest_paths:\n","        exists = os.path.exists(path)\n","        print(f\"  {path}: {'âœ…' if exists else 'âŒ'}\")\n","\n","    # æ£€æŸ¥NO2\n","    no2_manifest_paths = [\n","        \"/content/drive/MyDrive/Feature_Stacks/no2_feature_stacks/no2_feature_stacks_manifest.csv\",\n","        \"/content/drive/MyDrive/no2_train_*.txt\",\n","        \"/content/drive/MyDrive/no2_val_*.txt\",\n","        \"/content/drive/MyDrive/no2_test_*.txt\"\n","    ]\n","\n","    print(\"\\n NO2æ–‡ä»¶çŠ¶æ€:\")\n","    for path in no2_manifest_paths:\n","        if \"*\" in path:\n","            # æ£€æŸ¥é€šé…ç¬¦æ–‡ä»¶\n","            import glob\n","            files = glob.glob(path)\n","            print(f\"  {path}: {'âœ…' if files else 'âŒ'} ({len(files)} files)\")\n","        else:\n","            exists = os.path.exists(path)\n","            print(f\"  {path}: {'âœ…' if exists else 'âŒ'}\")\n","\n","# è¿è¡Œæ£€æŸ¥\n","check_manifest_status()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IzWSQxZDwxAJ","executionInfo":{"status":"ok","timestamp":1757664118634,"user_tz":-120,"elapsed":387,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"fc16b767-1b80-438d-ce23-4afbf45b80f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” æ£€æŸ¥manifestæ–‡ä»¶çŠ¶æ€...\n","\n","ğŸ“Š SO2æ–‡ä»¶çŠ¶æ€:\n","  /content/drive/MyDrive/Feature_Stacks/so2_feature_stacks/so2_feature_stacks_manifest.csv: âŒ\n","  /content/drive/MyDrive/so2_train_20250911_080237.txt: âœ…\n","  /content/drive/MyDrive/so2_val_20250911_080237.txt: âœ…\n","  /content/drive/MyDrive/so2_test_20250911_080237.txt: âœ…\n","\n"," NO2æ–‡ä»¶çŠ¶æ€:\n","  /content/drive/MyDrive/Feature_Stacks/no2_feature_stacks/no2_feature_stacks_manifest.csv: âŒ\n","  /content/drive/MyDrive/no2_train_*.txt: âŒ (0 files)\n","  /content/drive/MyDrive/no2_val_*.txt: âŒ (0 files)\n","  /content/drive/MyDrive/no2_test_*.txt: âŒ (0 files)\n"]}]},{"cell_type":"code","source":["# ä¿®æ­£ç‰ˆï¼šç”ŸæˆSO2å’ŒNO2çš„manifestæ–‡ä»¶ï¼ˆç¬¦åˆæœ€ä½³å®è·µï¼‰\n","import os\n","import pandas as pd\n","import numpy as np\n","import re\n","from datetime import datetime\n","import glob\n","\n","BASE_SO2 = \"/content/drive/MyDrive/Variables/so2_feature_stacks\"\n","BASE_NO2 = \"/content/drive/MyDrive/Variables/no2_feature_stacks\"\n","OUT_DIR = \"/content/drive/MyDrive\"\n","\n","def create_manifest_improved(data_type, base_path, output_dir):\n","    \"\"\"åˆ›å»ºmanifestæ–‡ä»¶ï¼ˆæ”¹è¿›ç‰ˆï¼‰\"\"\"\n","    print(f\"\\nğŸ“‹ åˆ›å»º{data_type} manifestæ–‡ä»¶ï¼ˆæ”¹è¿›ç‰ˆï¼‰...\")\n","\n","    manifest_data = []\n","    coverage_issues = []\n","\n","    for year in [2019, 2020, 2021, 2022, 2023]:\n","        year_dir = os.path.join(base_path, str(year))\n","        if not os.path.exists(year_dir):\n","            print(f\"âš ï¸ å¹´ä»½ç›®å½•ä¸å­˜åœ¨: {year_dir}\")\n","            continue\n","\n","        files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","        files.sort()\n","\n","        print(f\"  {year}: {len(files)} ä¸ªæ–‡ä»¶\")\n","\n","        for file in files:\n","            try:\n","                # æ”¹è¿›1: æ›´robustçš„æ—¥æœŸè§£æ\n","                m = re.search(r'(\\d{8})', file)\n","                if not m:\n","                    print(f\"âš ï¸ æ— æ³•è§£ææ—¥æœŸ: {file}\")\n","                    continue\n","\n","                date8 = m.group(1)\n","                date_formatted = f\"{date8[:4]}-{date8[4:6]}-{date8[6:8]}\"\n","\n","                file_path = os.path.join(year_dir, file)\n","\n","                # æ”¹è¿›3: è¦†ç›–ç‡ä¸€è‡´æ€§æ ¡éªŒ\n","                try:\n","                    with np.load(file_path, allow_pickle=True) as d:\n","                        # åŸºäºmaské‡æ–°è®¡ç®—è¦†ç›–ç‡\n","                        if 'mask' in d.files:\n","                            cov_chk = float((d['mask']==0).sum())/d['mask'].size\n","                        else:\n","                            cov_chk = np.nan\n","\n","                        # ä½¿ç”¨æ–‡ä»¶ä¸­çš„coverageï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è®¡ç®—å€¼\n","                        coverage = float(d['coverage']) if 'coverage' in d.files else cov_chk\n","\n","                        # ä¸€è‡´æ€§æ£€æŸ¥\n","                        if 'coverage' in d.files and not np.isnan(cov_chk):\n","                            diff = abs(coverage - cov_chk)\n","                            if diff > 0.01:  # å·®å¼‚è¶…è¿‡1%\n","                                coverage_issues.append({\n","                                    'file': file,\n","                                    'stored_coverage': coverage,\n","                                    'calculated_coverage': cov_chk,\n","                                    'difference': diff\n","                                })\n","\n","                        trainable = bool(d['trainable']) if 'trainable' in d.files else False\n","                        season = d['season'].item() if 'season' in d.files else 'unknown'\n","\n","                except Exception as e:\n","                    print(f\"âš ï¸ åŠ è½½æ–‡ä»¶å¤±è´¥ {file}: {e}\")\n","                    continue\n","\n","                manifest_data.append({\n","                    'date': date_formatted,\n","                    'path': file_path,\n","                    'coverage': coverage,\n","                    'trainable': trainable,\n","                    'season': season,\n","                    'year': int(date8[:4]),\n","                    'month': int(date8[4:6]),\n","                    'day': int(date8[6:8])\n","                })\n","\n","            except Exception as e:\n","                print(f\"âš ï¸ å¤„ç†æ–‡ä»¶å¤±è´¥ {file}: {e}\")\n","                continue\n","\n","    # åˆ›å»ºDataFrameå¹¶ä¿å­˜\n","    manifest_df = pd.DataFrame(manifest_data)\n","    manifest_file = os.path.join(output_dir, f'{data_type.lower()}_manifest.csv')\n","    manifest_df.to_csv(manifest_file, index=False)\n","\n","    print(f\"âœ… {data_type} manifestä¿å­˜: {manifest_file}\")\n","    print(f\"   æ€»æ–‡ä»¶æ•°: {len(manifest_df)}\")\n","    print(f\"   æ—¥æœŸèŒƒå›´: {manifest_df['date'].min()} åˆ° {manifest_df['date'].max()}\")\n","    print(f\"   å¯è®­ç»ƒæ–‡ä»¶: {manifest_df['trainable'].sum()}\")\n","\n","    # æŠ¥å‘Šè¦†ç›–ç‡é—®é¢˜\n","    if coverage_issues:\n","        print(f\"âš ï¸ å‘ç° {len(coverage_issues)} ä¸ªè¦†ç›–ç‡ä¸ä¸€è‡´æ–‡ä»¶\")\n","        for issue in coverage_issues[:5]:  # æ˜¾ç¤ºå‰5ä¸ª\n","            print(f\"   {issue['file']}: å­˜å‚¨={issue['stored_coverage']:.3f}, è®¡ç®—={issue['calculated_coverage']:.3f}, å·®å¼‚={issue['difference']:.3f}\")\n","\n","    return manifest_df\n","\n","def create_train_val_test_splits_improved(manifest_df, data_type, output_dir):\n","    \"\"\"åˆ›å»ºtrain/val/teståˆ’åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰\"\"\"\n","    print(f\"\\n åˆ›å»º{data_type} train/val/teståˆ’åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰...\")\n","\n","    # è¿‡æ»¤å¯è®­ç»ƒæ–‡ä»¶\n","    trainable_df = manifest_df[manifest_df['trainable'] == True].copy()\n","    print(f\"   å¯è®­ç»ƒæ–‡ä»¶: {len(trainable_df)}\")\n","\n","    if len(trainable_df) == 0:\n","        print(f\"âŒ æ²¡æœ‰å¯è®­ç»ƒæ–‡ä»¶\")\n","        return\n","\n","    # æŒ‰å¹´ä»½åˆ’åˆ†\n","    train_files = trainable_df[trainable_df['year'].isin([2019, 2020, 2021])]\n","    val_files = trainable_df[trainable_df['year'] == 2022]\n","    test_files = trainable_df[trainable_df['year'] == 2023]\n","\n","    print(f\"   è®­ç»ƒé›†: {len(train_files)} æ–‡ä»¶\")\n","    print(f\"   éªŒè¯é›†: {len(val_files)} æ–‡ä»¶\")\n","    print(f\"   æµ‹è¯•é›†: {len(test_files)} æ–‡ä»¶\")\n","\n","    # ä¿å­˜åˆ’åˆ†æ–‡ä»¶\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","    for split_name, split_df in [('train', train_files), ('val', val_files), ('test', test_files)]:\n","        if len(split_df) > 0:\n","            # æ”¹è¿›2: åŒæ—¶ä¿å­˜TXTå’ŒCSVæ–‡ä»¶\n","            # TXTæ–‡ä»¶ï¼ˆä»…æ—¥æœŸï¼Œå…¼å®¹ç°æœ‰ä»£ç ï¼‰\n","            split_txt = os.path.join(output_dir, f'{data_type.lower()}_{split_name}_{timestamp}.txt')\n","            with open(split_txt, 'w') as f:\n","                for _, row in split_df.iterrows():\n","                    f.write(f\"{row['date']}\\n\")\n","\n","            # CSVæ–‡ä»¶ï¼ˆåŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼‰\n","            split_csv = os.path.join(output_dir, f'{data_type.lower()}_{split_name}_{timestamp}.csv')\n","            split_df[['date', 'path', 'coverage', 'season']].to_csv(split_csv, index=False)\n","\n","            print(f\"   âœ… {split_name}: {split_txt} + {split_csv}\")\n","\n","    return train_files, val_files, test_files\n","\n","def main():\n","    \"\"\"ä¸»å‡½æ•°\"\"\"\n","    print(\" ç”ŸæˆSO2å’ŒNO2çš„manifestæ–‡ä»¶ä»¥åŠtrain/val/teståˆ’åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰...\")\n","\n","    # åˆ›å»ºSO2 manifestå’Œåˆ’åˆ†\n","    if os.path.exists(BASE_SO2):\n","        so2_manifest = create_manifest_improved(\"SO2\", BASE_SO2, OUT_DIR)\n","        create_train_val_test_splits_improved(so2_manifest, \"SO2\", OUT_DIR)\n","    else:\n","        print(f\"âŒ SO2æ•°æ®ç›®å½•ä¸å­˜åœ¨: {BASE_SO2}\")\n","\n","    # åˆ›å»ºNO2 manifestå’Œåˆ’åˆ†\n","    if os.path.exists(BASE_NO2):\n","        no2_manifest = create_manifest_improved(\"NO2\", BASE_NO2, OUT_DIR)\n","        create_train_val_test_splits_improved(no2_manifest, \"NO2\", OUT_DIR)\n","    else:\n","        print(f\"âŒ NO2æ•°æ®ç›®å½•ä¸å­˜åœ¨: {BASE_NO2}\")\n","\n","    print(\"\\nâœ… æ‰€æœ‰manifestå’Œåˆ’åˆ†æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼\")\n","\n","# è¿è¡Œä¸»å‡½æ•°\n","main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5e1LGRVO8yDJ","executionInfo":{"status":"ok","timestamp":1757667587244,"user_tz":-120,"elapsed":3258810,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"fcc0ef08-2d33-4f62-a96c-b3f0a9b98565"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" ç”ŸæˆSO2å’ŒNO2çš„manifestæ–‡ä»¶ä»¥åŠtrain/val/teståˆ’åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰...\n","\n","ğŸ“‹ åˆ›å»ºSO2 manifestæ–‡ä»¶ï¼ˆæ”¹è¿›ç‰ˆï¼‰...\n","  2019: 365 ä¸ªæ–‡ä»¶\n","  2020: 366 ä¸ªæ–‡ä»¶\n","  2021: 365 ä¸ªæ–‡ä»¶\n","  2022: 365 ä¸ªæ–‡ä»¶\n","  2023: 365 ä¸ªæ–‡ä»¶\n","âœ… SO2 manifestä¿å­˜: /content/drive/MyDrive/so2_manifest.csv\n","   æ€»æ–‡ä»¶æ•°: 1826\n","   æ—¥æœŸèŒƒå›´: 2019-01-01 åˆ° 2023-12-31\n","   å¯è®­ç»ƒæ–‡ä»¶: 852\n","âš ï¸ å‘ç° 1024 ä¸ªè¦†ç›–ç‡ä¸ä¸€è‡´æ–‡ä»¶\n","   so2_features_20190206.npz: å­˜å‚¨=0.012, è®¡ç®—=0.023, å·®å¼‚=0.011\n","   so2_features_20190207.npz: å­˜å‚¨=0.015, è®¡ç®—=0.046, å·®å¼‚=0.031\n","   so2_features_20190211.npz: å­˜å‚¨=0.050, è®¡ç®—=0.097, å·®å¼‚=0.047\n","   so2_features_20190212.npz: å­˜å‚¨=0.076, è®¡ç®—=0.239, å·®å¼‚=0.164\n","   so2_features_20190213.npz: å­˜å‚¨=0.048, è®¡ç®—=0.169, å·®å¼‚=0.121\n","\n"," åˆ›å»ºSO2 train/val/teståˆ’åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰...\n","   å¯è®­ç»ƒæ–‡ä»¶: 852\n","   è®­ç»ƒé›†: 507 æ–‡ä»¶\n","   éªŒè¯é›†: 175 æ–‡ä»¶\n","   æµ‹è¯•é›†: 170 æ–‡ä»¶\n","   âœ… train: /content/drive/MyDrive/so2_train_20250912_083216.txt + /content/drive/MyDrive/so2_train_20250912_083216.csv\n","   âœ… val: /content/drive/MyDrive/so2_val_20250912_083216.txt + /content/drive/MyDrive/so2_val_20250912_083216.csv\n","   âœ… test: /content/drive/MyDrive/so2_test_20250912_083216.txt + /content/drive/MyDrive/so2_test_20250912_083216.csv\n","\n","ğŸ“‹ åˆ›å»ºNO2 manifestæ–‡ä»¶ï¼ˆæ”¹è¿›ç‰ˆï¼‰...\n","  2019: 365 ä¸ªæ–‡ä»¶\n","  2020: 366 ä¸ªæ–‡ä»¶\n","  2021: 365 ä¸ªæ–‡ä»¶\n","  2022: 365 ä¸ªæ–‡ä»¶\n","  2023: 365 ä¸ªæ–‡ä»¶\n","âœ… NO2 manifestä¿å­˜: /content/drive/MyDrive/no2_manifest.csv\n","   æ€»æ–‡ä»¶æ•°: 1826\n","   æ—¥æœŸèŒƒå›´: 2019-01-01 åˆ° 2023-12-31\n","   å¯è®­ç»ƒæ–‡ä»¶: 570\n","\n"," åˆ›å»ºNO2 train/val/teståˆ’åˆ†ï¼ˆæ”¹è¿›ç‰ˆï¼‰...\n","   å¯è®­ç»ƒæ–‡ä»¶: 570\n","   è®­ç»ƒé›†: 302 æ–‡ä»¶\n","   éªŒè¯é›†: 146 æ–‡ä»¶\n","   æµ‹è¯•é›†: 122 æ–‡ä»¶\n","   âœ… train: /content/drive/MyDrive/no2_train_20250912_085947.txt + /content/drive/MyDrive/no2_train_20250912_085947.csv\n","   âœ… val: /content/drive/MyDrive/no2_val_20250912_085947.txt + /content/drive/MyDrive/no2_val_20250912_085947.csv\n","   âœ… test: /content/drive/MyDrive/no2_test_20250912_085947.txt + /content/drive/MyDrive/no2_test_20250912_085947.csv\n","\n","âœ… æ‰€æœ‰manifestå’Œåˆ’åˆ†æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼\n"]}]},{"cell_type":"code","source":["# Unified manifest + split builder (with per-dataset thresholds and QC guards)\n","import os, re, numpy as np, pandas as pd, random\n","from datetime import datetime\n","\n","# ----------------------------- Config -----------------------------\n","BASE_NO2 = \"/content/drive/MyDrive/Variables/no2_feature_stacks\"\n","BASE_SO2 = \"/content/drive/MyDrive/Variables/so2_feature_stacks\"\n","OUT_DIR   = \"/content/drive/MyDrive/Variables\"\n","YEARS     = [2019, 2020, 2021, 2022, 2023]\n","\n","# Year-based splits\n","TRAIN_YRS, VAL_YRS, TEST_YRS = [2019, 2020, 2021], [2022], [2023]\n","\n","# Per-dataset default trainable thresholds (overridable by file field 'trainable_threshold')\n","DEFAULT_THR = {\"NO2\": 0.40, \"SO2\": 0.10}\n","\n","# QC guards only when coverage_source == 'mask'\n","MIN_YPOS_FRAC = 0.01     # require at least 1% positive y pixels globally (very loose)\n","MAX_COV_DIFF  = 0.05     # |mean(mask==0) - mean(y>0 & finite)| should be < 0.05\n","\n","# Repro\n","random.seed(42); np.random.seed(42)\n","\n","# ----------------------------- Helpers -----------------------------\n","def parse_date_from_filename(fname: str) -> str | None:\n","    m = re.search(r'(\\d{8})', fname)\n","    if not m:\n","        return None\n","    d = m.group(1)\n","    return f\"{d[:4]}-{d[4:6]}-{d[6:8]}\"\n","\n","def compute_coverage_unified(fp: str, data_type: str):\n","    \"\"\"\n","    Returns:\n","      coverage: float\n","      trainable: bool (after applying threshold and QC if needed)\n","      threshold_used: float\n","      season: str\n","      coverage_source: str in {'domain+data','data_coverage','mask','none','error'}\n","      y_positive_frac: float (only meaningful when source=='mask', else np.nan)\n","      qc_pass: bool\n","    \"\"\"\n","    try:\n","        with np.load(fp, allow_pickle=True) as d:\n","            # 1) coverage compute with unified precedence\n","            if {'data_mask','domain_mask'}.issubset(d.files):\n","                source = 'domain+data'\n","                dom = (d['domain_mask'] == 1)\n","                dat = (d['data_mask'] == 1)\n","                denom = max(dom.sum(), 1)\n","                cov = float((dat & dom).sum() / denom)\n","            elif 'data_coverage' in d.files:\n","                source = 'data_coverage'\n","                cov = float(d['data_coverage'])\n","            elif 'mask' in d.files:\n","                source = 'mask'\n","                cov = float((d['mask']==0).sum() / d['mask'].size)\n","            else:\n","                source = 'none'\n","                cov = 0.0\n","\n","            # 2) threshold\n","            thr = float(d['trainable_threshold']) if 'trainable_threshold' in d.files else DEFAULT_THR[data_type]\n","            season = d['season'].item() if 'season' in d.files else 'unknown'\n","\n","            # 3) QC guard if we fell back to 'mask' (no AOI info)\n","            qc_pass = True\n","            y_pos_frac = np.nan\n","            if source == 'mask':\n","                try:\n","                    y = d['y']\n","                    mask = d['mask']\n","                    pos = np.isfinite(y) & (y > 0)\n","                    y_pos_frac = float(pos.sum() / y.size)\n","                    cov_mask = float((mask==0).sum() / mask.size)\n","                    qc_pass = (y_pos_frac >= MIN_YPOS_FRAC) and (abs(cov_mask - y_pos_frac) < MAX_COV_DIFF)\n","                except Exception:\n","                    qc_pass = False\n","\n","            trainable = bool((cov >= thr) and qc_pass)\n","            return cov, trainable, thr, season, source, y_pos_frac, qc_pass\n","\n","    except Exception:\n","        return 0.0, False, DEFAULT_THR[data_type], 'unknown', 'error', np.nan, False\n","\n","def quick_summary(df: pd.DataFrame, name: str):\n","    if df.empty:\n","        print(f\"\\n[{name}] files=0\")\n","        return\n","    print(f\"\\n[{name}] files={len(df)}  cov_mean={df['coverage'].mean():.3f}  cov_med={df['coverage'].median():.3f}\")\n","    print(\" season counts:\\n\", df['season'].value_counts())\n","    print(\" coverage_source breakdown:\\n\", df['coverage_source'].value_counts())\n","\n","def build_manifest(data_type: str, base_dir: str) -> pd.DataFrame:\n","    print(f\"\\nğŸ“‹ Building manifest for {data_type} ...\")\n","    rows = []\n","    for y in YEARS:\n","        ydir = os.path.join(base_dir, str(y))\n","        if not os.path.isdir(ydir):\n","            continue\n","        files = sorted([f for f in os.listdir(ydir) if f.endswith(\".npz\")])\n","        print(f\"  {y}: {len(files)} files\")\n","        for f in files:\n","            date = parse_date_from_filename(f)\n","            if not date:\n","                continue\n","            fp = os.path.join(ydir, f)\n","            cov, trn, thr, season, source, ypf, qc = compute_coverage_unified(fp, data_type)\n","            rows.append({\n","                'date': date,\n","                'path': fp,\n","                'coverage': cov,\n","                'trainable': trn,\n","                'threshold': thr,\n","                'season': season,\n","                'coverage_source': source,\n","                'y_positive_frac': ypf,\n","                'qc_pass': qc,\n","                'year': int(date[:4]),\n","                'month': int(date[5:7]),\n","                'day': int(date[8:10]),\n","            })\n","    df = pd.DataFrame(rows).sort_values('date').reset_index(drop=True)\n","    out_csv = os.path.join(OUT_DIR, f\"{data_type.lower()}_manifest.csv\")\n","    df.to_csv(out_csv, index=False)\n","    print(f\"âœ… {data_type} manifest saved: {out_csv}\")\n","    print(f\"   total={len(df)}, trainable={int(df['trainable'].sum())}, date_range={df['date'].min()}..{df['date'].max()}\")\n","    quick_summary(df, f\"{data_type} manifest\")\n","    return df\n","\n","def save_splits(df: pd.DataFrame, data_type: str):\n","    splits = {\n","        'train': df[(df['trainable'] == True) & (df['year'].isin(TRAIN_YRS))],\n","        'val':   df[(df['trainable'] == True) & (df['year'].isin(VAL_YRS))],\n","        'test':  df[(df['trainable'] == True) & (df['year'].isin(TEST_YRS))],\n","    }\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    for name, sdf in splits.items():\n","        quick_summary(sdf, f\"{data_type} {name}\")\n","        if sdf.empty:\n","            continue\n","        txt = os.path.join(OUT_DIR, f\"{data_type.lower()}_{name}_{ts}.txt\")\n","        csv = os.path.join(OUT_DIR, f\"{data_type.lower()}_{name}_{ts}.csv\")\n","        with open(txt, \"w\") as f:\n","            for d in sdf['date'].tolist():\n","                f.write(f\"{d}\\n\")\n","        sdf[['date','path','coverage','season','coverage_source','threshold','qc_pass']].to_csv(csv, index=False)\n","        print(f\"   âœ… {name}: {len(sdf)} -> {txt} + {csv}\")\n","\n","# ----------------------------- Run -----------------------------\n","print(\"ğŸš€ Building unified manifests and splits with QC guards...\")\n","\n","so2_df = build_manifest(\"SO2\", BASE_SO2)\n","save_splits(so2_df, \"SO2\")\n","\n","no2_df = build_manifest(\"NO2\", BASE_NO2)\n","save_splits(no2_df, \"NO2\")\n","\n","print(\"\\nğŸ‰ Done.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjy86FfNKYUk","executionInfo":{"status":"ok","timestamp":1757671059341,"user_tz":-120,"elapsed":3260135,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"49929340-29d8-445b-d2e4-2cefecc4ea52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Building unified manifests and splits with QC guards...\n","\n","ğŸ“‹ Building manifest for SO2 ...\n","  2019: 365 files\n","  2020: 366 files\n","  2021: 365 files\n","  2022: 365 files\n","  2023: 365 files\n","âœ… SO2 manifest saved: /content/drive/MyDrive/Variables/so2_manifest.csv\n","   total=1826, trainable=852, date_range=2019-01-01..2023-12-31\n","\n","[SO2 manifest] files=1826  cov_mean=0.107  cov_med=0.086\n"," season counts:\n"," season\n","spring    460\n","summer    460\n","autumn    455\n","winter    451\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","domain+data    1826\n","Name: count, dtype: int64\n","\n","[SO2 train] files=507  cov_mean=0.199  cov_med=0.188\n"," season counts:\n"," season\n","summer    224\n","spring    168\n","autumn     93\n","winter     22\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","domain+data    507\n","Name: count, dtype: int64\n","   âœ… train: 507 -> /content/drive/MyDrive/Variables/so2_train_20250912_093025.txt + /content/drive/MyDrive/Variables/so2_train_20250912_093025.csv\n","\n","[SO2 val] files=175  cov_mean=0.219  cov_med=0.219\n"," season counts:\n"," season\n","summer    81\n","spring    49\n","autumn    38\n","winter     7\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","domain+data    175\n","Name: count, dtype: int64\n","   âœ… val: 175 -> /content/drive/MyDrive/Variables/so2_val_20250912_093025.txt + /content/drive/MyDrive/Variables/so2_val_20250912_093025.csv\n","\n","[SO2 test] files=170  cov_mean=0.206  cov_med=0.180\n"," season counts:\n"," season\n","summer    71\n","spring    56\n","autumn    38\n","winter     5\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","domain+data    170\n","Name: count, dtype: int64\n","   âœ… test: 170 -> /content/drive/MyDrive/Variables/so2_test_20250912_093025.txt + /content/drive/MyDrive/Variables/so2_test_20250912_093025.csv\n","\n","ğŸ“‹ Building manifest for NO2 ...\n","  2019: 365 files\n","  2020: 366 files\n","  2021: 365 files\n","  2022: 365 files\n","  2023: 365 files\n","âœ… NO2 manifest saved: /content/drive/MyDrive/Variables/no2_manifest.csv\n","   total=1826, trainable=570, date_range=2019-01-01..2023-12-31\n","\n","[NO2 manifest] files=1826  cov_mean=0.281  cov_med=0.310\n"," season counts:\n"," season\n","spring    460\n","summer    460\n","autumn    455\n","winter    451\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","mask    1826\n","Name: count, dtype: int64\n","\n","[NO2 train] files=302  cov_mean=0.456  cov_med=0.453\n"," season counts:\n"," season\n","summer    94\n","spring    78\n","autumn    68\n","winter    62\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","mask    302\n","Name: count, dtype: int64\n","   âœ… train: 302 -> /content/drive/MyDrive/Variables/no2_train_20250912_095739.txt + /content/drive/MyDrive/Variables/no2_train_20250912_095739.csv\n","\n","[NO2 val] files=146  cov_mean=0.456  cov_med=0.454\n"," season counts:\n"," season\n","summer    53\n","spring    37\n","autumn    33\n","winter    23\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","mask    146\n","Name: count, dtype: int64\n","   âœ… val: 146 -> /content/drive/MyDrive/Variables/no2_val_20250912_095739.txt + /content/drive/MyDrive/Variables/no2_val_20250912_095739.csv\n","\n","[NO2 test] files=122  cov_mean=0.462  cov_med=0.468\n"," season counts:\n"," season\n","autumn    40\n","summer    32\n","winter    29\n","spring    21\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","mask    122\n","Name: count, dtype: int64\n","   âœ… test: 122 -> /content/drive/MyDrive/Variables/no2_test_20250912_095739.txt + /content/drive/MyDrive/Variables/no2_test_20250912_095739.csv\n","\n","ğŸ‰ Done.\n"]}]},{"cell_type":"code","source":["import os, json, numpy as np, pandas as pd\n","\n","OUT = \"/content/drive/MyDrive/Variables\"\n","\n","def freeze_scaler(path, name):\n","    with np.load(path, allow_pickle=True) as d:\n","        scalers = d['scalers'].item()\n","        feats = [k for k in d['feature_order'].tolist()]\n","        meta = d['metadata'].item() if 'metadata' in d.files else {}\n","    info = {\n","        \"file\": path, \"num_features\": len(feats), \"features\": feats,\n","        \"scaler_keys\": sorted([k for k,v in scalers.items() if 'mean' in v]),\n","        \"metadata\": meta\n","    }\n","    with open(os.path.join(OUT, f\"{name}_scaler_frozen.json\"), \"w\") as f:\n","        json.dump(info, f, indent=2)\n","    print(f\"âœ… freeze: {name} -> {os.path.join(OUT, f'{name}_scaler_frozen.json')}\")\n","\n","def split_summary(csv_path, tag):\n","    df = pd.read_csv(csv_path)\n","    summ = {\n","        \"files\": len(df),\n","        \"cov_mean\": float(df['coverage'].mean()),\n","        \"cov_median\": float(df['coverage'].median()),\n","        \"season_counts\": df['season'].value_counts().to_dict(),\n","        \"source_counts\": (df['coverage_source'].value_counts().to_dict()\n","                          if 'coverage_source' in df.columns else {})\n","    }\n","    out = csv_path.replace(\".csv\", \"_summary.json\")\n","    with open(out, \"w\") as f: json.dump(summ, f, indent=2)\n","    print(f\"âœ… summary: {tag} -> {out}\")\n","\n","# 1) å†»ç»“ä¸¤ä»½scaler\n","freeze_scaler(\"/content/drive/MyDrive/no2_scalers_2019_2021.npz\", \"no2\")\n","freeze_scaler(\"/content/drive/MyDrive/so2_scalers_2019_2021.npz\", \"so2\")\n","\n","# 2) ä¸ºæ¯ä¸ªsplitä¿å­˜æ‘˜è¦ï¼ˆNO2/SO2 å„è‡ª train/val/testï¼‰\n","for p in [\n","    \"so2_train\", \"so2_val\", \"so2_test\",\n","    \"no2_train\", \"no2_val\", \"no2_test\"\n","]:\n","    # è‹¥æ–‡ä»¶åå¸¦æ—¶é—´æˆ³ï¼Œå¯ç”¨æœ€æ–°ä¸€ä¸ª\n","    from glob import glob\n","    cand = sorted(glob(os.path.join(OUT, f\"{p}_*.csv\")))\n","    if cand:\n","        split_summary(cand[-1], p)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vagez9-YcGZy","executionInfo":{"status":"ok","timestamp":1757672411807,"user_tz":-120,"elapsed":776,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"ba3f6a15-65e6-4efd-bbd0-1159e0b89ef2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… freeze: no2 -> /content/drive/MyDrive/Variables/no2_scaler_frozen.json\n","âœ… freeze: so2 -> /content/drive/MyDrive/Variables/so2_scaler_frozen.json\n","âœ… summary: so2_train -> /content/drive/MyDrive/Variables/so2_train_20250912_093025_summary.json\n","âœ… summary: so2_val -> /content/drive/MyDrive/Variables/so2_val_20250912_093025_summary.json\n","âœ… summary: so2_test -> /content/drive/MyDrive/Variables/so2_test_20250912_093025_summary.json\n","âœ… summary: no2_train -> /content/drive/MyDrive/Variables/no2_train_20250912_095739_summary.json\n","âœ… summary: no2_val -> /content/drive/MyDrive/Variables/no2_val_20250912_095739_summary.json\n","âœ… summary: no2_test -> /content/drive/MyDrive/Variables/no2_test_20250912_095739_summary.json\n"]}]}]}