{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNPb8O98uvdyGtaple+ORR0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6nQ97sYJYT7","executionInfo":{"status":"ok","timestamp":1758162212537,"user_tz":-120,"elapsed":8679,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"5c324674-5685-40d1-ca4a-2b24e726e565"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# --- 1. ÂÆâË£Ö‰æùËµñ ---\n","!pip install xarray netCDF4 matplotlib geopandas rasterio rioxarray --quiet\n","\n","# --- 2. ÊåÇËΩΩGoogle Drive ---\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# --- 3. ÂØºÂÖ•Â∫ì ---\n","import os\n","import numpy as np\n","import pandas as pd\n","import xarray as xr\n","import geopandas as gpd\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","source":["# 1. Verification: NO2 and SO2 File Structure Check"],"metadata":{"id":"OVIKlerodYIA"}},{"cell_type":"code","source":["# Complete Verification: NO2 and SO2 File Structure Check\n","import os, numpy as np, glob\n","\n","def check_file_structures():\n","    \"\"\"Complete verification of NO2 and SO2 file structures\"\"\"\n","    print(\"üîç Complete File Structure Verification\")\n","    print(\"=\"*60)\n","\n","    base_path = \"/content/drive/MyDrive/Feature_Stacks\"\n","\n","    # Check NO2 structure\n","    print(\"\\nüìÅ Checking NO2 File Structure:\")\n","    print(\"-\" * 40)\n","\n","    no2_dir = os.path.join(base_path, \"NO2_2019\")\n","    if not os.path.exists(no2_dir):\n","        print(f\"‚ùå NO2 directory not found: {no2_dir}\")\n","        return\n","\n","    no2_files = sorted(glob.glob(os.path.join(no2_dir, \"NO2_stack_*.npz\")))\n","    if len(no2_files) == 0:\n","        print(f\"‚ùå No NO2 files found in: {no2_dir}\")\n","        return\n","\n","    print(f\"‚úÖ Found {len(no2_files)} NO2 files\")\n","\n","    # Check first NO2 file\n","    no2_file = no2_files[0]\n","    print(f\"üìÑ Analyzing: {os.path.basename(no2_file)}\")\n","\n","    try:\n","        with np.load(no2_file, allow_pickle=True) as data:\n","            print(f\" NO2 file keys: {list(data.files)}\")\n","\n","            # Check for X matrix\n","            if 'X' in data.files:\n","                X = data['X']\n","                print(f\"‚úÖ NO2 has X matrix: shape={X.shape}\")\n","            else:\n","                print(f\"‚ùå NO2 missing X matrix\")\n","                print(f\"   Available keys: {[k for k in data.files if k not in ['coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']]}\")\n","\n","            # Check for target and mask\n","            if 'no2_target' in data.files:\n","                target = data['no2_target']\n","                print(f\"‚úÖ NO2 has target: shape={target.shape}\")\n","            else:\n","                print(f\"‚ùå NO2 missing target\")\n","\n","            if 'no2_mask' in data.files:\n","                mask = data['no2_mask']\n","                print(f\"‚úÖ NO2 has mask: shape={mask.shape}\")\n","            else:\n","                print(f\"‚ùå NO2 missing mask\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error loading NO2 file: {e}\")\n","\n","    # Check SO2 structure\n","    print(\"\\nüìÅ Checking SO2 File Structure:\")\n","    print(\"-\" * 40)\n","\n","    so2_dir = os.path.join(base_path, \"SO2_2019\")\n","    if not os.path.exists(so2_dir):\n","        print(f\"‚ùå SO2 directory not found: {so2_dir}\")\n","        return\n","\n","    so2_files = sorted(glob.glob(os.path.join(so2_dir, \"SO2_stack_*.npz\")))\n","    if len(so2_files) == 0:\n","        print(f\"‚ùå No SO2 files found in: {so2_dir}\")\n","        return\n","\n","    print(f\"‚úÖ Found {len(so2_files)} SO2 files\")\n","\n","    # Check first SO2 file\n","    so2_file = so2_files[0]\n","    print(f\"üìÑ Analyzing: {os.path.basename(so2_file)}\")\n","\n","    try:\n","        with np.load(so2_file, allow_pickle=True) as data:\n","            print(f\" SO2 file keys: {list(data.files)}\")\n","\n","            # Check for X matrix\n","            if 'X' in data.files:\n","                X = data['X']\n","                print(f\"‚úÖ SO2 has X matrix: shape={X.shape}\")\n","            else:\n","                print(f\"‚ùå SO2 missing X matrix\")\n","\n","            # Check for target and mask\n","            if 'y' in data.files:\n","                target = data['y']\n","                print(f\"‚úÖ SO2 has target: shape={target.shape}\")\n","            else:\n","                print(f\"‚ùå SO2 missing target\")\n","\n","            if 'mask' in data.files:\n","                mask = data['mask']\n","                print(f\"‚úÖ SO2 has mask: shape={mask.shape}\")\n","            else:\n","                print(f\"‚ùå SO2 missing mask\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error loading SO2 file: {e}\")\n","\n","    # Summary\n","    print(\"\\nüìã Summary:\")\n","    print(\"-\" * 40)\n","    print(\"File structure comparison:\")\n","    print(\"  NO2: Dictionary structure (no X matrix)\")\n","    print(\"  SO2: Matrix structure (has X matrix)\")\n","    print(\"\\nRecommendation:\")\n","    print(\"  Need to modify NO2 scaler to handle dictionary structure\")\n","\n","def check_no2_feature_availability():\n","    \"\"\"Check if NO2 has all required features for scaler\"\"\"\n","    print(\"\\nüîç Checking NO2 Feature Availability:\")\n","    print(\"-\" * 40)\n","\n","    base_path = \"/content/drive/MyDrive/Feature_Stacks\"\n","    no2_dir = os.path.join(base_path, \"NO2_2019\")\n","    no2_files = sorted(glob.glob(os.path.join(no2_dir, \"NO2_stack_*.npz\")))\n","\n","    if len(no2_files) == 0:\n","        print(\"‚ùå No NO2 files found\")\n","        return\n","\n","    # Required features for scaler\n","    required_features = [\n","        'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","        'no2_lag_1day', 'no2_neighbor',\n","        'dem', 'slope', 'pop'\n","    ]\n","\n","    try:\n","        with np.load(no2_files[0], allow_pickle=True) as data:\n","            available_keys = list(data.files)\n","            print(f\"üìä Available features: {len(available_keys)}\")\n","\n","            missing_features = []\n","            available_features = []\n","\n","            for feature in required_features:\n","                if feature in available_keys:\n","                    available_features.append(feature)\n","                else:\n","                    missing_features.append(feature)\n","\n","            print(f\"‚úÖ Available required features: {len(available_features)}\")\n","            print(f\"   {available_features}\")\n","\n","            if missing_features:\n","                print(f\"‚ùå Missing required features: {len(missing_features)}\")\n","                print(f\"   {missing_features}\")\n","            else:\n","                print(\"‚úÖ All required features available for scaler\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error checking features: {e}\")\n","\n","def suggest_no2_scaler_fix():\n","    \"\"\"Suggest how to fix NO2 scaler for dictionary structure\"\"\"\n","    print(\"\\nüõ†Ô∏è Suggested NO2 Scaler Fix:\")\n","    print(\"-\" * 40)\n","\n","    print(\"Current NO2 scaler expects:\")\n","    print(\"  - data['X'] (matrix structure)\")\n","    print(\"  - data['mask'] (unified mask)\")\n","\n","    print(\"\\nActual NO2 structure:\")\n","    print(\"  - Dictionary of individual features\")\n","    print(\"  - data['no2_mask'] (specific mask)\")\n","\n","    print(\"\\nRequired changes:\")\n","    print(\"1. Build X matrix from dictionary features\")\n","    print(\"2. Use data['no2_mask'] instead of data['mask']\")\n","    print(\"3. Handle feature ordering manually\")\n","\n","    print(\"\\nCode changes needed:\")\n","    print(\"\"\"\n","    # Instead of:\n","    X = data['X']\n","    valid_mask = data['mask'] == 1\n","\n","    # Use:\n","    feature_order = ['dem', 'slope', 'pop', 'u10', 'v10', ...]\n","    X = np.stack([data[name] for name in feature_order], axis=0)\n","    valid_mask = data['no2_mask'] == 1\n","    \"\"\")\n","\n","# Run all checks\n","print(\" Starting Complete File Structure Verification\")\n","print(\"=\"*80)\n","\n","check_file_structures()\n","check_no2_feature_availability()\n","suggest_no2_scaler_fix()\n","\n","print(\"\\n Next Steps:\")\n","print(\"1. If NO2 missing X matrix ‚Üí Modify NO2 scaler code\")\n","print(\"2. If NO2 has X matrix ‚Üí Run scaler directly\")\n","print(\"3. Ensure mask semantics are correct (1=valid, 0=invalid)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXj1uPXDVVHp","executionInfo":{"status":"ok","timestamp":1758057610887,"user_tz":-120,"elapsed":196,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"6b5a10ab-157b-4484-fbad-3f3ad2d9d625"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Starting Complete File Structure Verification\n","================================================================================\n","üîç Complete File Structure Verification\n","============================================================\n","\n","üìÅ Checking NO2 File Structure:\n","----------------------------------------\n","‚úÖ Found 365 NO2 files\n","üìÑ Analyzing: NO2_stack_20190101.npz\n"," NO2 file keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","‚ùå NO2 missing X matrix\n","   Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor']\n","‚úÖ NO2 has target: shape=(300, 621)\n","‚úÖ NO2 has mask: shape=(300, 621)\n","\n","üìÅ Checking SO2 File Structure:\n","----------------------------------------\n","‚úÖ Found 365 SO2 files\n","üìÑ Analyzing: SO2_stack_20190101.npz\n"," SO2 file keys: ['X', 'y', 'mask', 'feature_names', 'cont_idx', 'onehot_idx', 'noscale_idx', 'coverage', 'trainable', 'pollutant', 'season', 'date', 'doy', 'weekday', 'year_len', 'grid_height', 'grid_width', 'lag1_fill_ratio', 'neighbor_fill_ratio', 'file_version']\n","‚úÖ SO2 has X matrix: shape=(30, 300, 621)\n","‚úÖ SO2 has target: shape=(300, 621)\n","‚úÖ SO2 has mask: shape=(300, 621)\n","\n","üìã Summary:\n","----------------------------------------\n","File structure comparison:\n","  NO2: Dictionary structure (no X matrix)\n","  SO2: Matrix structure (has X matrix)\n","\n","Recommendation:\n","  Need to modify NO2 scaler to handle dictionary structure\n","\n","üîç Checking NO2 Feature Availability:\n","----------------------------------------\n","üìä Available features: 37\n","‚úÖ Available required features: 13\n","   ['u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'no2_lag_1day', 'no2_neighbor', 'dem', 'slope', 'pop']\n","‚úÖ All required features available for scaler\n","\n","üõ†Ô∏è Suggested NO2 Scaler Fix:\n","----------------------------------------\n","Current NO2 scaler expects:\n","  - data['X'] (matrix structure)\n","  - data['mask'] (unified mask)\n","\n","Actual NO2 structure:\n","  - Dictionary of individual features\n","  - data['no2_mask'] (specific mask)\n","\n","Required changes:\n","1. Build X matrix from dictionary features\n","2. Use data['no2_mask'] instead of data['mask']\n","3. Handle feature ordering manually\n","\n","Code changes needed:\n","\n","    # Instead of:\n","    X = data['X']\n","    valid_mask = data['mask'] == 1\n","    \n","    # Use:\n","    feature_order = ['dem', 'slope', 'pop', 'u10', 'v10', ...]\n","    X = np.stack([data[name] for name in feature_order], axis=0)\n","    valid_mask = data['no2_mask'] == 1\n","    \n","\n"," Next Steps:\n","1. If NO2 missing X matrix ‚Üí Modify NO2 scaler code\n","2. If NO2 has X matrix ‚Üí Run scaler directly\n","3. Ensure mask semantics are correct (1=valid, 0=invalid)\n"]}]},{"cell_type":"markdown","source":["# 2. Generate NO2 standardization parameters"],"metadata":{"id":"pSN8d8lLddkI"}},{"cell_type":"code","source":["# Generate NO2 standardization parameters (FINAL VERIFIED VERSION)\n","import os, numpy as np\n","from collections import defaultdict\n","\n","# Create output directory\n","os.makedirs(\"/content/drive/MyDrive/Scalers\", exist_ok=True)\n","\n","BASE_NO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","OUT_DIR = \"/content/drive/MyDrive/Scalers\"\n","\n","# Continuous features that need standardization (NO2 version)\n","CONTINUOUS_FEATURES = [\n","    'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","    'no2_lag_1day', 'no2_neighbor',  # NO2 related features\n","    'dem', 'slope', 'pop',\n","    'ws'  # Wind speed (if not already standardized)\n","]\n","\n","# Features that should not be standardized\n","NON_STANDARDIZED = ['sin_doy', 'cos_doy', 'weekday_weight', 'wd_sin', 'wd_cos']\n","\n","# Categorical features (one-hot encoded)\n","CATEGORICAL_FEATURES = [\n","    'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4',\n","    'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9'\n","]\n","\n","def compute_no2_scalers(years=[2019, 2020, 2021], sample_per_file=1000):\n","    \"\"\"Compute standardization parameters for NO2\"\"\"\n","    print(\"üöÄ Starting NO2 standardization parameter computation...\")\n","    print(f\"Data path: {BASE_NO2}\")\n","    print(f\"Training years: {years}\")\n","    print(f\"Samples per file: {sample_per_file}\")\n","\n","    # Accumulate statistics\n","    stats = defaultdict(lambda: {'sum': 0.0, 'sumsq': 0.0, 'count': 0})\n","    feature_order = None\n","\n","    total_files = 0\n","    processed_files = 0\n","\n","    for year in years:\n","        year_dir = os.path.join(BASE_NO2, f\"NO2_{year}\")\n","        if not os.path.exists(year_dir):\n","            print(f\"‚ö†Ô∏è Year directory not found: {year_dir}\")\n","            continue\n","\n","        files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","        total_files += len(files)\n","        print(f\"\\n Processing {year}: {len(files)} files\")\n","\n","        for i, fname in enumerate(files):\n","            if i % 50 == 0:\n","                print(f\"  {i}/{len(files)} ({i/len(files)*100:.1f}%)\")\n","\n","            try:\n","                with np.load(os.path.join(year_dir, fname)) as data:\n","                    # NO2 uses dictionary structure, need to build X matrix\n","                    if feature_order is None:\n","                        # Define feature order for NO2 (based on verification results)\n","                        feature_order = [\n","                            'dem', 'slope', 'pop',\n","                            'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4',\n","                            'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9',\n","                            'sin_doy', 'cos_doy', 'weekday_weight',\n","                            'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","                            'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor'\n","                        ]\n","\n","                    # Build X matrix from dictionary features with robustness checks\n","                    # Strategy: Fill missing non-critical features with appropriate defaults\n","                    X_list = []\n","                    missing_critical = False\n","\n","                    for name in feature_order:\n","                        if name in data.files:\n","                            X_list.append(data[name].astype(np.float32))\n","                        else:\n","                            print(f\"   ‚ö†Ô∏è Missing feature: {name} in {fname}\")\n","                            if name in CONTINUOUS_FEATURES:\n","                                # Critical feature missing - skip this file\n","                                print(f\"   ‚ùå Critical continuous feature missing, skipping file\")\n","                                missing_critical = True\n","                                break\n","                            else:\n","                                # Non-critical feature missing - fill with appropriate default\n","                                if name.startswith('lulc_class_'):\n","                                    fill_value = 0.0  # One-hot encoded, 0 is appropriate for missing class\n","                                elif name in ['sin_doy', 'cos_doy']:\n","                                    fill_value = 0.0  # Default to 0 for missing temporal features\n","                                elif name == 'weekday_weight':\n","                                    fill_value = 1.0  # Default to weekday weight\n","                                elif name in ['wd_sin', 'wd_cos']:\n","                                    fill_value = 0.0  # Default wind direction components\n","                                else:\n","                                    fill_value = 0.0  # Default fallback\n","\n","                                print(f\"   ‚ÑπÔ∏è Filling missing non-critical feature {name} with {fill_value}\")\n","                                # Get dimensions from first available feature\n","                                if X_list:\n","                                    H, W = X_list[0].shape\n","                                else:\n","                                    # Fallback dimensions (should not happen in practice)\n","                                    H, W = 300, 621\n","                                X_list.append(np.full((H, W), fill_value, dtype=np.float32))\n","\n","                    if not missing_critical:\n","                        # All features handled (either available or filled), build X matrix\n","                        X = np.stack(X_list, axis=0).astype(np.float32)  # (C, H, W)\n","\n","                        # CONFIRMED: NO2: mask==1 is valid pixels, mask==0 is invalid\n","                        # Additional robustness: ensure target > 0 for physical validity\n","                        valid_mask = (data['no2_mask'] == 1) & (data['no2_target'] > 0)\n","\n","                        # --- ÂÖ≥ÈîÆ‰øÆÊ≠£ÔºöÊ≠£Á°ÆÂ§ÑÁêÜ2DÊé©ËÜúÁ¥¢Âºï ---\n","                        C, H, W = X.shape\n","                        X_flat = X.reshape(C, -1)  # (C, H*W) - ÊâìÂπ≥Á©∫Èó¥Áª¥Â∫¶\n","                        valid_flat = valid_mask.ravel()  # (H*W,) - ÊâìÂπ≥Êé©ËÜú\n","                        valid_pixels = X_flat[:, valid_flat]  # (C, N_valid) - Ê≠£Á°ÆÊèêÂèñÊúâÊïàÂÉèÁ¥†\n","\n","                        if valid_pixels.shape[1] == 0:\n","                            continue\n","\n","                        # Random sampling to avoid memory issues\n","                        n_samples = min(sample_per_file, valid_pixels.shape[1])\n","                        if n_samples < valid_pixels.shape[1]:\n","                            indices = np.random.choice(valid_pixels.shape[1], n_samples, replace=False)\n","                            sampled = valid_pixels[:, indices]\n","                        else:\n","                            sampled = valid_pixels\n","\n","                        # Filter NaN/inf\n","                        finite_mask = np.isfinite(sampled)\n","\n","                        # Accumulate statistics\n","                        for j, feat_name in enumerate(feature_order):\n","                            if feat_name in CONTINUOUS_FEATURES:\n","                                values = sampled[j]\n","                                finite_values = values[finite_mask[j]]\n","\n","                                if len(finite_values) > 0:\n","                                    # 1-99 percentile winsorization\n","                                    q1, q99 = np.percentile(finite_values, [1, 99])\n","                                    clipped = np.clip(finite_values, q1, q99)\n","\n","                                    stats[feat_name]['sum'] += np.sum(clipped)\n","                                    stats[feat_name]['sumsq'] += np.sum(clipped**2)\n","                                    stats[feat_name]['count'] += len(clipped)\n","                            elif feat_name in CATEGORICAL_FEATURES:\n","                                # Categorical features don't need standardization\n","                                pass\n","                            elif feat_name in NON_STANDARDIZED:\n","                                # Non-standardized features don't need standardization\n","                                pass\n","\n","                        processed_files += 1\n","\n","            except Exception as e:\n","                print(f\"‚ùå Failed to load file {fname}: {e}\")\n","                continue\n","\n","    print(f\"\\nüìä Processing statistics:\")\n","    print(f\"Total files: {total_files}\")\n","    print(f\"Successfully processed: {processed_files}\")\n","    print(f\"Processing rate: {processed_files/total_files*100:.1f}%\")\n","\n","    if not stats:\n","        print(\"‚ùå No valid samples found\")\n","        return None\n","\n","    # Compute final statistics\n","    scalers = {}\n","    print(f\"\\nüî¢ Computing standardization parameters...\")\n","\n","    for feat_name in feature_order:\n","        if feat_name in CONTINUOUS_FEATURES:\n","            # ËøûÁª≠ÁâπÂæÅÔºöÁªü‰∏ÄÂ§ÑÁêÜÔºåÂç≥‰ΩøÊ≤°ÊúâÁªüËÆ°Âà∞‰πüÁªôÈªòËÆ§ÂÄº\n","            if feat_name in stats:\n","                count = stats[feat_name]['count']\n","                if count > 0:\n","                    mean = stats[feat_name]['sum'] / count\n","                    variance = (stats[feat_name]['sumsq'] / count) - (mean**2)\n","                    std = np.sqrt(max(variance, 1e-8))  # Avoid division by zero\n","\n","                    scalers[feat_name] = {\n","                        'mean': float(mean),\n","                        'std': float(std),\n","                        'count': int(count)\n","                    }\n","                    print(f\"  {feat_name}: mean={mean:.4f}, std={std:.4f}, count={count}\")\n","                else:\n","                    # ÊúâÁªüËÆ°‰ΩÜcount=0ÔºåÁªôÈªòËÆ§ÂÄº\n","                    scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                    print(f\"  {feat_name}: No valid samples, using default values\")\n","            else:\n","                # ÂÆåÂÖ®Ê≤°ÊúâÁªüËÆ°Âà∞Ôºå‰πüÁªôÈªòËÆ§ÂÄº\n","                scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                print(f\"  {feat_name}: No statistics collected, using default values\")\n","        elif feat_name in CATEGORICAL_FEATURES:\n","            scalers[feat_name] = {'type': 'categorical'}\n","            print(f\"  {feat_name}: categorical (one-hot)\")\n","        elif feat_name in NON_STANDARDIZED:\n","            scalers[feat_name] = {'type': 'non_standardized'}\n","            print(f\"  {feat_name}: non_standardized\")\n","        else:\n","            # ËøôÁßçÊÉÖÂÜµÁêÜËÆ∫‰∏ä‰∏çÂ∫îËØ•ÂèëÁîüÔºåÂõ†‰∏∫feature_orderÂ∫îËØ•ÂåÖÂê´ÊâÄÊúâÁâπÂæÅ\n","            scalers[feat_name] = {'type': 'unexpected'}\n","            print(f\"  {feat_name}: unexpected type - check feature_order definition\")\n","\n","    return scalers, feature_order\n","\n","def apply_no2_scaler(X: np.ndarray, feature_names: list, scaler_file: str = None):\n","    \"\"\"\n","    Apply NO2 standardization to feature matrix\n","\n","    Args:\n","        X: Feature matrix (C, H, W) or (C, N)\n","        feature_names: List of feature names\n","        scaler_file: Path to scaler file (if None, auto-detect)\n","\n","    Returns:\n","        X_scaled: Standardized feature matrix\n","    \"\"\"\n","    if scaler_file is None:\n","        scaler_file = os.path.join(OUT_DIR, \"no2_scalers_2019_2021.npz\")\n","\n","    if not os.path.exists(scaler_file):\n","        raise FileNotFoundError(f\"NO2 scaler file not found: {scaler_file}\")\n","\n","    with np.load(scaler_file, allow_pickle=True) as data:\n","        scalers = data['scalers'].item()\n","\n","    X_scaled = X.copy().astype(np.float32)\n","\n","    for i, feat_name in enumerate(feature_names):\n","        if feat_name in scalers and 'mean' in scalers[feat_name]:\n","            # Apply z-score normalization for continuous features\n","            mean = scalers[feat_name]['mean']\n","            std = scalers[feat_name]['std']\n","            X_scaled[i] = (X[i] - mean) / std\n","        # Skip categorical and non-standardized features (they remain unchanged)\n","\n","    return X_scaled\n","\n","# Compute NO2 standardization parameters\n","print(\"=\" * 60)\n","print(\" NO2 Standardization Parameter Computation (FINAL VERIFIED)\")\n","print(\"=\" * 60)\n","\n","no2_scalers, no2_features = compute_no2_scalers()\n","\n","if no2_scalers is not None:\n","    # Save results\n","    output_file = os.path.join(OUT_DIR, \"no2_scalers_2019_2021.npz\")\n","    np.savez_compressed(\n","        output_file,\n","        scalers=no2_scalers,\n","        feature_order=no2_features,\n","        metadata={\n","            'data_type': 'no2',\n","            'years': [2019, 2020, 2021],\n","            'mask_logic': 'mask==1 is valid, mask==0 is invalid (CONFIRMED FROM SOURCE CODE)',\n","            'total_features': len(no2_features),\n","            'continuous_features': len([f for f in no2_features if f in CONTINUOUS_FEATURES])\n","        }\n","    )\n","\n","    print(f\"\\n‚úÖ NO2 standardization parameters saved to: {output_file}\")\n","    print(f\" Feature statistics:\")\n","    print(f\"  Total features: {len(no2_features)}\")\n","    print(f\"  Continuous features: {len([f for f in no2_features if f in CONTINUOUS_FEATURES])}\")\n","    print(f\"  Categorical features: {len([f for f in no2_features if f in CATEGORICAL_FEATURES])}\")\n","    print(f\"  Non-standardized features: {len([f for f in no2_features if f in NON_STANDARDIZED])}\")\n","    # Show parameters for first few continuous features\n","    print(f\"\\nüîç Standardization parameters for first 5 continuous features:\")\n","    continuous_count = 0\n","    for feat_name in no2_features:\n","        if feat_name in CONTINUOUS_FEATURES and feat_name in no2_scalers:\n","            if 'mean' in no2_scalers[feat_name]:\n","                print(f\"  {feat_name}: mean={no2_scalers[feat_name]['mean']:.4f}, std={no2_scalers[feat_name]['std']:.4f}\")\n","                continuous_count += 1\n","                if continuous_count >= 5:\n","                    break\n","else:\n","    print(\"‚ùå Computation failed\")\n","\n","print(\"\\n Next step: Ready to train NO2 model!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Wbue9izZtux","executionInfo":{"status":"ok","timestamp":1758059594588,"user_tz":-120,"elapsed":680371,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"351a0c94-2d5c-4aa1-bc41-72bdd797812b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n"," NO2 Standardization Parameter Computation (FINAL VERIFIED)\n","============================================================\n","üöÄ Starting NO2 standardization parameter computation...\n","Data path: /content/drive/MyDrive/Feature_Stacks\n","Training years: [2019, 2020, 2021]\n","Samples per file: 1000\n","\n"," Processing 2019: 365 files\n","  0/365 (0.0%)\n","  50/365 (13.7%)\n","  100/365 (27.4%)\n","  150/365 (41.1%)\n","  200/365 (54.8%)\n","  250/365 (68.5%)\n","  300/365 (82.2%)\n","  350/365 (95.9%)\n","\n"," Processing 2020: 366 files\n","  0/366 (0.0%)\n","  50/366 (13.7%)\n","  100/366 (27.3%)\n","  150/366 (41.0%)\n","  200/366 (54.6%)\n","  250/366 (68.3%)\n","  300/366 (82.0%)\n","  350/366 (95.6%)\n","\n"," Processing 2021: 365 files\n","  0/365 (0.0%)\n","  50/365 (13.7%)\n","  100/365 (27.4%)\n","  150/365 (41.1%)\n","  200/365 (54.8%)\n","  250/365 (68.5%)\n","  300/365 (82.2%)\n","  350/365 (95.9%)\n","\n","üìä Processing statistics:\n","Total files: 1096\n","Successfully processed: 1071\n","Processing rate: 97.7%\n","\n","üî¢ Computing standardization parameters...\n","  dem: mean=606.2273, std=771.7322, count=1052695\n","  slope: mean=11.3112, std=11.0288, count=994379\n","  pop: mean=135.9125, std=369.5149, count=992667\n","  lulc_class_0: categorical (one-hot)\n","  lulc_class_1: categorical (one-hot)\n","  lulc_class_2: categorical (one-hot)\n","  lulc_class_3: categorical (one-hot)\n","  lulc_class_4: categorical (one-hot)\n","  lulc_class_5: categorical (one-hot)\n","  lulc_class_6: categorical (one-hot)\n","  lulc_class_7: categorical (one-hot)\n","  lulc_class_8: categorical (one-hot)\n","  lulc_class_9: categorical (one-hot)\n","  sin_doy: non_standardized\n","  cos_doy: non_standardized\n","  weekday_weight: non_standardized\n","  u10: mean=-0.2344, std=1.5692, count=1052695\n","  v10: mean=0.1392, std=1.4423, count=1052695\n","  blh: mean=951.2973, std=558.4578, count=1052695\n","  tp: mean=0.0003, std=0.0009, count=1052695\n","  t2m: mean=15.8182, std=9.8316, count=1052695\n","  sp: No statistics collected, using default values\n","  str: mean=-29437.2015, std=32870.4722, count=1631\n","  ssr_clr: No statistics collected, using default values\n","  ws: mean=1.7724, std=1.2206, count=1052695\n","  wd_sin: non_standardized\n","  wd_cos: non_standardized\n","  no2_lag_1day: mean=0.0000, std=0.0001, count=1033884\n","  no2_neighbor: mean=0.0001, std=0.0001, count=1052695\n","\n","‚úÖ NO2 standardization parameters saved to: /content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\n"," Feature statistics:\n","  Total features: 29\n","  Continuous features: 14\n","  Categorical features: 10\n","  Non-standardized features: 5\n","\n","üîç Standardization parameters for first 5 continuous features:\n","  dem: mean=606.2273, std=771.7322\n","  slope: mean=11.3112, std=11.0288\n","  pop: mean=135.9125, std=369.5149\n","  u10: mean=-0.2344, std=1.5692\n","  v10: mean=0.1392, std=1.4423\n","\n"," Next step: Ready to train NO2 model!\n"]}]},{"cell_type":"code","source":["# Quick Check NO2 Scaler Results\n","import os\n","import numpy as np\n","\n","def quick_check_no2_scaler_results():\n","    \"\"\"Âø´ÈÄüÊ£ÄÊü•NO2 scalerÁªìÊûú\"\"\"\n","    print(\"üîç Quick NO2 Scaler Results Check\")\n","    print(\"=\" * 40)\n","\n","    # 1. Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â≠òÂú®\n","    scaler_file = \"/content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(f\"‚ùå Scaler file not found: {scaler_file}\")\n","        return False\n","\n","    print(f\"‚úÖ Scaler file exists\")\n","\n","    # 2. Âä†ËΩΩÂπ∂Ê£ÄÊü•Âü∫Êú¨ÂÜÖÂÆπ\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            scalers = data['scalers'].item()\n","            feature_order = data['feature_order']\n","            metadata = data['metadata'].item()\n","\n","        print(f\"‚úÖ File loaded successfully\")\n","        print(f\"üìä Total features: {len(feature_order)}\")\n","        print(f\"üìä Years: {metadata.get('years', 'unknown')}\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Failed to load: {e}\")\n","        return False\n","\n","    # 3. Ê£ÄÊü•ÂÖ≥ÈîÆËøûÁª≠ÁâπÂæÅ\n","    expected_continuous = [\n","        'dem', 'slope', 'pop', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","        'ws', 'no2_lag_1day', 'no2_neighbor'\n","    ]\n","\n","    print(f\"\\nüéØ Key Continuous Features Analysis:\")\n","    continuous_ok = 0\n","    low_sample_features = []\n","    zero_value_features = []\n","\n","    for feat in expected_continuous:\n","        if feat in scalers and 'mean' in scalers[feat]:\n","            count = scalers[feat]['count']\n","            mean = scalers[feat]['mean']\n","            std = scalers[feat]['std']\n","\n","            if count > 0:\n","                print(f\"  ‚úÖ {feat}: {count:,} samples, mean={mean:.4f}, std={std:.4f}\")\n","                continuous_ok += 1\n","\n","                # Ê£ÄÊü•ÂºÇÂ∏∏ÊÉÖÂÜµ\n","                if count < 100000:  # Ê†∑Êú¨Êï∞Â§™Â∞ë\n","                    low_sample_features.append((feat, count))\n","                if abs(mean) < 0.001 and std < 0.001:  # ÂÄºÂá†‰πé‰∏∫0\n","                    zero_value_features.append((feat, mean, std))\n","            else:\n","                print(f\"  ‚ö†Ô∏è {feat}: default values (no samples)\")\n","        else:\n","            print(f\"  ‚ùå {feat}: missing\")\n","\n","    # 4. Ê£ÄÊü•ÂÖ∂‰ªñÁâπÂæÅÁ±ªÂûã\n","    categorical_count = sum(1 for feat in feature_order\n","                           if feat in scalers and scalers[feat].get('type') == 'categorical')\n","    non_std_count = sum(1 for feat in feature_order\n","                       if feat in scalers and scalers[feat].get('type') == 'non_standardized')\n","\n","    print(f\"\\nüìä Feature Type Summary:\")\n","    print(f\"  Continuous features: {continuous_ok}/14\")\n","    print(f\"  Categorical features: {categorical_count}\")\n","    print(f\"  Non-standardized features: {non_std_count}\")\n","\n","    # 5. ÂºÇÂ∏∏ÊÉÖÂÜµÊä•Âëä\n","    if low_sample_features:\n","        print(f\"\\n‚ö†Ô∏è Features with low sample counts:\")\n","        for feat, count in low_sample_features:\n","            print(f\"  {feat}: {count:,} samples\")\n","\n","    if zero_value_features:\n","        print(f\"\\n‚ö†Ô∏è Features with near-zero values:\")\n","        for feat, mean, std in zero_value_features:\n","            print(f\"  {feat}: mean={mean:.6f}, std={std:.6f}\")\n","\n","    # 6. ÊÄª‰ΩìËØÑ‰º∞\n","    success_rate = (continuous_ok / 14) * 100\n","    print(f\"\\nüéØ Success Rate: {success_rate:.1f}%\")\n","\n","    if success_rate >= 80:\n","        print(f\"üéâ NO2 Scaler looks good!\")\n","        if low_sample_features or zero_value_features:\n","            print(f\"üí° Note: Some features have unusual values, but this might be normal for the dataset\")\n","        return True\n","    else:\n","        print(f\"‚ö†Ô∏è NO2 Scaler needs attention\")\n","        return False\n","\n","if __name__ == \"__main__\":\n","    quick_check_no2_scaler_results()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JwdLGyehm12","executionInfo":{"status":"ok","timestamp":1758059724542,"user_tz":-120,"elapsed":255,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"f33bf732-a954-4771-9e98-334cb0308e21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç Quick NO2 Scaler Results Check\n","========================================\n","‚úÖ Scaler file exists\n","‚úÖ File loaded successfully\n","üìä Total features: 29\n","üìä Years: [2019, 2020, 2021]\n","\n","üéØ Key Continuous Features Analysis:\n","  ‚úÖ dem: 1,052,695 samples, mean=606.2273, std=771.7322\n","  ‚úÖ slope: 994,379 samples, mean=11.3112, std=11.0288\n","  ‚úÖ pop: 992,667 samples, mean=135.9125, std=369.5149\n","  ‚úÖ u10: 1,052,695 samples, mean=-0.2344, std=1.5692\n","  ‚úÖ v10: 1,052,695 samples, mean=0.1392, std=1.4423\n","  ‚úÖ blh: 1,052,695 samples, mean=951.2973, std=558.4578\n","  ‚úÖ tp: 1,052,695 samples, mean=0.0003, std=0.0009\n","  ‚úÖ t2m: 1,052,695 samples, mean=15.8182, std=9.8316\n","  ‚ö†Ô∏è sp: default values (no samples)\n","  ‚úÖ str: 1,631 samples, mean=-29437.2015, std=32870.4722\n","  ‚ö†Ô∏è ssr_clr: default values (no samples)\n","  ‚úÖ ws: 1,052,695 samples, mean=1.7724, std=1.2206\n","  ‚úÖ no2_lag_1day: 1,033,884 samples, mean=0.0000, std=0.0001\n","  ‚úÖ no2_neighbor: 1,052,695 samples, mean=0.0001, std=0.0001\n","\n","üìä Feature Type Summary:\n","  Continuous features: 12/14\n","  Categorical features: 10\n","  Non-standardized features: 5\n","\n","‚ö†Ô∏è Features with low sample counts:\n","  str: 1,631 samples\n","\n","‚ö†Ô∏è Features with near-zero values:\n","  tp: mean=0.000289, std=0.000872\n","  no2_lag_1day: mean=0.000004, std=0.000100\n","  no2_neighbor: mean=0.000055, std=0.000100\n","\n","üéØ Success Rate: 85.7%\n","üéâ NO2 Scaler looks good!\n","üí° Note: Some features have unusual values, but this might be normal for the dataset\n"]}]},{"cell_type":"code","source":["# Validate NO2 Scaler Results\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","\n","def validate_no2_scaler_results():\n","    \"\"\"È™åËØÅNO2 scalerËøêË°åÁªìÊûú\"\"\"\n","    print(\"üîç NO2 Scaler Results Validation\")\n","    print(\"=\" * 50)\n","\n","    # 1. Ê£ÄÊü•ËæìÂá∫Êñá‰ª∂ÊòØÂê¶Â≠òÂú®\n","    scaler_file = \"/content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(f\"‚ùå Scaler file not found: {scaler_file}\")\n","        return False\n","\n","    print(f\"‚úÖ Scaler file found: {scaler_file}\")\n","\n","    # 2. Âä†ËΩΩÂπ∂Ê£ÄÊü•scalerÂÜÖÂÆπ\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            scalers = data['scalers'].item()\n","            feature_order = data['feature_order']\n","            metadata = data['metadata'].item()\n","\n","        print(f\"‚úÖ Successfully loaded scaler data\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Failed to load scaler file: {e}\")\n","        return False\n","\n","    # 3. Ê£ÄÊü•ÂÖÉÊï∞ÊçÆ\n","    print(f\"\\nüìä Metadata:\")\n","    print(f\"  Data type: {metadata.get('data_type', 'unknown')}\")\n","    print(f\"  Years: {metadata.get('years', 'unknown')}\")\n","    print(f\"  Mask logic: {metadata.get('mask_logic', 'unknown')}\")\n","    print(f\"  Total features: {metadata.get('total_features', 'unknown')}\")\n","    print(f\"  Continuous features: {metadata.get('continuous_features', 'unknown')}\")\n","\n","    # 4. Ê£ÄÊü•ÁâπÂæÅÈ°∫Â∫è\n","    print(f\"\\nüìã Feature Order ({len(feature_order)} features):\")\n","    for i, feat in enumerate(feature_order):\n","        print(f\"  {i+1:2d}. {feat}\")\n","\n","    # 5. Ê£ÄÊü•scalerÂèÇÊï∞\n","    print(f\"\\nüî¢ Scaler Parameters:\")\n","\n","    continuous_count = 0\n","    categorical_count = 0\n","    non_standardized_count = 0\n","    default_count = 0\n","\n","    for feat_name in feature_order:\n","        if feat_name in scalers:\n","            scaler_info = scalers[feat_name]\n","\n","            if 'mean' in scaler_info:\n","                # ËøûÁª≠ÁâπÂæÅ\n","                mean = scaler_info['mean']\n","                std = scaler_info['std']\n","                count = scaler_info['count']\n","                continuous_count += 1\n","\n","                if count > 0:\n","                    print(f\"  ‚úÖ {feat_name}: mean={mean:.4f}, std={std:.4f}, count={count:,}\")\n","                else:\n","                    print(f\"  ‚ö†Ô∏è {feat_name}: default values (mean=0, std=1, count=0)\")\n","                    default_count += 1\n","\n","            elif scaler_info.get('type') == 'categorical':\n","                categorical_count += 1\n","                print(f\"  üìä {feat_name}: categorical (one-hot)\")\n","\n","            elif scaler_info.get('type') == 'non_standardized':\n","                non_standardized_count += 1\n","                print(f\"  üö´ {feat_name}: non_standardized\")\n","\n","            else:\n","                print(f\"  ‚ùì {feat_name}: {scaler_info}\")\n","\n","    # 6. ÁªüËÆ°ÊÄªÁªì\n","    print(f\"\\nüìà Summary Statistics:\")\n","    print(f\"  Total features: {len(feature_order)}\")\n","    print(f\"  Continuous features: {continuous_count}\")\n","    print(f\"  Categorical features: {categorical_count}\")\n","    print(f\"  Non-standardized features: {non_standardized_count}\")\n","    print(f\"  Features with default values: {default_count}\")\n","\n","    # 7. È™åËØÅÂÖ≥ÈîÆËøûÁª≠ÁâπÂæÅ\n","    expected_continuous = [\n","        'dem', 'slope', 'pop', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","        'ws', 'no2_lag_1day', 'no2_neighbor'\n","    ]\n","\n","    print(f\"\\nüéØ Key Continuous Features Validation:\")\n","    missing_features = []\n","    for feat in expected_continuous:\n","        if feat in scalers and 'mean' in scalers[feat]:\n","            count = scalers[feat]['count']\n","            if count > 0:\n","                print(f\"  ‚úÖ {feat}: {count:,} samples\")\n","            else:\n","                print(f\"  ‚ö†Ô∏è {feat}: default values (no samples)\")\n","                missing_features.append(feat)\n","        else:\n","            print(f\"  ‚ùå {feat}: not found in scalers\")\n","            missing_features.append(feat)\n","\n","    # 8. Ê£ÄÊü•Êï∞ÊçÆË¥®Èáè\n","    print(f\"\\nüîç Data Quality Check:\")\n","\n","    # Ê£ÄÊü•ÊòØÂê¶ÊúâË∂≥Â§üÁöÑÊ†∑Êú¨\n","    total_samples = sum(scalers[feat]['count'] for feat in scalers\n","                       if 'count' in scalers[feat] and scalers[feat]['count'] > 0)\n","\n","    if total_samples > 0:\n","        print(f\"  ‚úÖ Total valid samples: {total_samples:,}\")\n","\n","        # Ê£ÄÊü•Ê†áÂáÜÂ∑ÆÊòØÂê¶ÂêàÁêÜ\n","        extreme_std_features = []\n","        for feat in expected_continuous:\n","            if feat in scalers and 'std' in scalers[feat]:\n","                std = scalers[feat]['std']\n","                if std < 0.001 or std > 1000:\n","                    extreme_std_features.append((feat, std))\n","\n","        if extreme_std_features:\n","            print(f\"  ‚ö†Ô∏è Features with extreme std values:\")\n","            for feat, std in extreme_std_features:\n","                print(f\"    {feat}: std={std:.6f}\")\n","        else:\n","            print(f\"  ‚úÖ All std values are reasonable\")\n","\n","    else:\n","        print(f\"  ‚ùå No valid samples found!\")\n","        return False\n","\n","    # 9. ÊµãËØïscalerÂ∫îÁî®\n","    print(f\"\\nüß™ Testing Scaler Application:\")\n","\n","    try:\n","        # ÂàõÂª∫ÊµãËØïÊï∞ÊçÆ\n","        test_X = np.random.randn(len(feature_order), 10, 10).astype(np.float32)\n","\n","        # Â∫îÁî®scaler\n","        from Generate_NO2_standardization_parameter import apply_no2_scaler\n","        X_scaled = apply_no2_scaler(test_X, feature_order, scaler_file)\n","\n","        print(f\"  ‚úÖ Scaler application test passed\")\n","        print(f\"  Input shape: {test_X.shape}\")\n","        print(f\"  Output shape: {X_scaled.shape}\")\n","\n","        # Ê£ÄÊü•Ê†áÂáÜÂåñÊïàÊûú\n","        for i, feat_name in enumerate(feature_order):\n","            if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                mean_scaled = np.mean(X_scaled[i])\n","                std_scaled = np.std(X_scaled[i])\n","                print(f\"  {feat_name}: scaled_mean={mean_scaled:.4f}, scaled_std={std_scaled:.4f}\")\n","                break  # Âè™ÊòæÁ§∫Á¨¨‰∏Ä‰∏™ËøûÁª≠ÁâπÂæÅ\n","\n","    except Exception as e:\n","        print(f\"  ‚ùå Scaler application test failed: {e}\")\n","        return False\n","\n","    # 10. ÊúÄÁªàËØÑ‰º∞\n","    print(f\"\\nüéØ Final Assessment:\")\n","\n","    success_criteria = [\n","        (continuous_count == 14, f\"14 continuous features processed\"),\n","        (categorical_count == 10, f\"10 categorical features identified\"),\n","        (non_standardized_count == 5, f\"5 non-standardized features identified\"),\n","        (total_samples > 100000, f\"Sufficient samples ({total_samples:,})\"),\n","        (len(missing_features) == 0, f\"No missing key features\"),\n","        (len(extreme_std_features) == 0, f\"No extreme std values\")\n","    ]\n","\n","    passed = 0\n","    for criterion, description in success_criteria:\n","        if criterion:\n","            print(f\"  ‚úÖ {description}\")\n","            passed += 1\n","        else:\n","            print(f\"  ‚ùå {description}\")\n","\n","    success_rate = passed / len(success_criteria) * 100\n","    print(f\"\\nüìä Overall Success Rate: {success_rate:.1f}% ({passed}/{len(success_criteria)})\")\n","\n","    if success_rate >= 80:\n","        print(f\"üéâ NO2 Scaler validation PASSED!\")\n","        return True\n","    else:\n","        print(f\"‚ö†Ô∏è NO2 Scaler validation needs attention\")\n","        return False\n","\n","def visualize_scaler_statistics():\n","    \"\"\"ÂèØËßÜÂåñscalerÁªüËÆ°‰ø°ÊÅØ\"\"\"\n","    print(f\"\\nüìä Visualizing Scaler Statistics...\")\n","\n","    scaler_file = \"/content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(f\"‚ùå Scaler file not found for visualization\")\n","        return\n","\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            scalers = data['scalers'].item()\n","            feature_order = data['feature_order']\n","\n","        # ÊèêÂèñËøûÁª≠ÁâπÂæÅÁöÑÁªüËÆ°‰ø°ÊÅØ\n","        continuous_features = []\n","        means = []\n","        stds = []\n","        counts = []\n","\n","        for feat_name in feature_order:\n","            if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                continuous_features.append(feat_name)\n","                means.append(scalers[feat_name]['mean'])\n","                stds.append(scalers[feat_name]['std'])\n","                counts.append(scalers[feat_name]['count'])\n","\n","        if not continuous_features:\n","            print(f\"‚ùå No continuous features found for visualization\")\n","            return\n","\n","        # ÂàõÂª∫ÂõæË°®\n","        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n","\n","        # 1. ÂùáÂÄºÂàÜÂ∏É\n","        ax1.bar(range(len(continuous_features)), means)\n","        ax1.set_title('Feature Means')\n","        ax1.set_xlabel('Features')\n","        ax1.set_ylabel('Mean Value')\n","        ax1.set_xticks(range(len(continuous_features)))\n","        ax1.set_xticklabels(continuous_features, rotation=45, ha='right')\n","\n","        # 2. Ê†áÂáÜÂ∑ÆÂàÜÂ∏É\n","        ax2.bar(range(len(continuous_features)), stds)\n","        ax2.set_title('Feature Standard Deviations')\n","        ax2.set_xlabel('Features')\n","        ax2.set_ylabel('Standard Deviation')\n","        ax2.set_xticks(range(len(continuous_features)))\n","        ax2.set_xticklabels(continuous_features, rotation=45, ha='right')\n","\n","        # 3. Ê†∑Êú¨Êï∞ÈáèÂàÜÂ∏É\n","        ax3.bar(range(len(continuous_features)), counts)\n","        ax3.set_title('Sample Counts')\n","        ax3.set_xlabel('Features')\n","        ax3.set_ylabel('Sample Count')\n","        ax3.set_xticks(range(len(continuous_features)))\n","        ax3.set_xticklabels(continuous_features, rotation=45, ha='right')\n","        ax3.set_yscale('log')\n","\n","        # 4. ÂùáÂÄºvsÊ†áÂáÜÂ∑ÆÊï£ÁÇπÂõæ\n","        ax4.scatter(means, stds, s=100, alpha=0.7)\n","        ax4.set_title('Mean vs Standard Deviation')\n","        ax4.set_xlabel('Mean')\n","        ax4.set_ylabel('Standard Deviation')\n","\n","        # Ê∑ªÂä†ÁâπÂæÅÊ†áÁ≠æ\n","        for i, feat in enumerate(continuous_features):\n","            ax4.annotate(feat, (means[i], stds[i]), xytext=(5, 5),\n","                        textcoords='offset points', fontsize=8)\n","\n","        plt.tight_layout()\n","        plt.savefig('/content/drive/MyDrive/Scalers/no2_scaler_statistics.png',\n","                   dpi=300, bbox_inches='tight')\n","        plt.show()\n","\n","        print(f\"‚úÖ Visualization saved to: /content/drive/MyDrive/Scalers/no2_scaler_statistics.png\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Visualization failed: {e}\")\n","\n","if __name__ == \"__main__\":\n","    # ËøêË°åÈ™åËØÅ\n","    success = validate_no2_scaler_results()\n","\n","    if success:\n","        # Â¶ÇÊûúÈ™åËØÅÊàêÂäüÔºåËøêË°åÂèØËßÜÂåñ\n","        visualize_scaler_statistics()\n","\n","    print(f\"\\nüéØ Next Steps:\")\n","    if success:\n","        print(f\"  1. ‚úÖ NO2 scaler is ready for training\")\n","        print(f\"  2. üöÄ Run SO2 scaler: python 'Generate SO2 standardization parameter.py'\")\n","        print(f\"  3. üéØ Proceed with model training\")\n","    else:\n","        print(f\"  1. üîß Fix issues identified in validation\")\n","        print(f\"  2. üîÑ Re-run NO2 scaler if needed\")\n","        print(f\"  3. ‚úÖ Validate again before proceeding\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5a3EHhqEhspJ","executionInfo":{"status":"ok","timestamp":1758059748539,"user_tz":-120,"elapsed":82,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"4ab59a8c-cc23-4154-b5a2-2e57f1bd6b82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç NO2 Scaler Results Validation\n","==================================================\n","‚úÖ Scaler file found: /content/drive/MyDrive/Scalers/no2_scalers_2019_2021.npz\n","‚úÖ Successfully loaded scaler data\n","\n","üìä Metadata:\n","  Data type: no2\n","  Years: [2019, 2020, 2021]\n","  Mask logic: mask==1 is valid, mask==0 is invalid (CONFIRMED FROM SOURCE CODE)\n","  Total features: 29\n","  Continuous features: 14\n","\n","üìã Feature Order (29 features):\n","   1. dem\n","   2. slope\n","   3. pop\n","   4. lulc_class_0\n","   5. lulc_class_1\n","   6. lulc_class_2\n","   7. lulc_class_3\n","   8. lulc_class_4\n","   9. lulc_class_5\n","  10. lulc_class_6\n","  11. lulc_class_7\n","  12. lulc_class_8\n","  13. lulc_class_9\n","  14. sin_doy\n","  15. cos_doy\n","  16. weekday_weight\n","  17. u10\n","  18. v10\n","  19. blh\n","  20. tp\n","  21. t2m\n","  22. sp\n","  23. str\n","  24. ssr_clr\n","  25. ws\n","  26. wd_sin\n","  27. wd_cos\n","  28. no2_lag_1day\n","  29. no2_neighbor\n","\n","üî¢ Scaler Parameters:\n","  ‚úÖ dem: mean=606.2273, std=771.7322, count=1,052,695\n","  ‚úÖ slope: mean=11.3112, std=11.0288, count=994,379\n","  ‚úÖ pop: mean=135.9125, std=369.5149, count=992,667\n","  üìä lulc_class_0: categorical (one-hot)\n","  üìä lulc_class_1: categorical (one-hot)\n","  üìä lulc_class_2: categorical (one-hot)\n","  üìä lulc_class_3: categorical (one-hot)\n","  üìä lulc_class_4: categorical (one-hot)\n","  üìä lulc_class_5: categorical (one-hot)\n","  üìä lulc_class_6: categorical (one-hot)\n","  üìä lulc_class_7: categorical (one-hot)\n","  üìä lulc_class_8: categorical (one-hot)\n","  üìä lulc_class_9: categorical (one-hot)\n","  üö´ sin_doy: non_standardized\n","  üö´ cos_doy: non_standardized\n","  üö´ weekday_weight: non_standardized\n","  ‚úÖ u10: mean=-0.2344, std=1.5692, count=1,052,695\n","  ‚úÖ v10: mean=0.1392, std=1.4423, count=1,052,695\n","  ‚úÖ blh: mean=951.2973, std=558.4578, count=1,052,695\n","  ‚úÖ tp: mean=0.0003, std=0.0009, count=1,052,695\n","  ‚úÖ t2m: mean=15.8182, std=9.8316, count=1,052,695\n","  ‚ö†Ô∏è sp: default values (mean=0, std=1, count=0)\n","  ‚úÖ str: mean=-29437.2015, std=32870.4722, count=1,631\n","  ‚ö†Ô∏è ssr_clr: default values (mean=0, std=1, count=0)\n","  ‚úÖ ws: mean=1.7724, std=1.2206, count=1,052,695\n","  üö´ wd_sin: non_standardized\n","  üö´ wd_cos: non_standardized\n","  ‚úÖ no2_lag_1day: mean=0.0000, std=0.0001, count=1,033,884\n","  ‚úÖ no2_neighbor: mean=0.0001, std=0.0001, count=1,052,695\n","\n","üìà Summary Statistics:\n","  Total features: 29\n","  Continuous features: 14\n","  Categorical features: 10\n","  Non-standardized features: 5\n","  Features with default values: 2\n","\n","üéØ Key Continuous Features Validation:\n","  ‚úÖ dem: 1,052,695 samples\n","  ‚úÖ slope: 994,379 samples\n","  ‚úÖ pop: 992,667 samples\n","  ‚úÖ u10: 1,052,695 samples\n","  ‚úÖ v10: 1,052,695 samples\n","  ‚úÖ blh: 1,052,695 samples\n","  ‚úÖ tp: 1,052,695 samples\n","  ‚úÖ t2m: 1,052,695 samples\n","  ‚ö†Ô∏è sp: default values (no samples)\n","  ‚úÖ str: 1,631 samples\n","  ‚ö†Ô∏è ssr_clr: default values (no samples)\n","  ‚úÖ ws: 1,052,695 samples\n","  ‚úÖ no2_lag_1day: 1,033,884 samples\n","  ‚úÖ no2_neighbor: 1,052,695 samples\n","\n","üîç Data Quality Check:\n","  ‚úÖ Total valid samples: 11,444,121\n","  ‚ö†Ô∏è Features with extreme std values:\n","    tp: std=0.000872\n","    str: std=32870.472234\n","    no2_lag_1day: std=0.000100\n","    no2_neighbor: std=0.000100\n","\n","üß™ Testing Scaler Application:\n","  ‚ùå Scaler application test failed: No module named 'Generate_NO2_standardization_parameter'\n","\n","üéØ Next Steps:\n","  1. üîß Fix issues identified in validation\n","  2. üîÑ Re-run NO2 scaler if needed\n","  3. ‚úÖ Validate again before proceeding\n"]}]},{"cell_type":"code","source":["# Diagnose NO2 Feature Issues\n","import os\n","import numpy as np\n","from collections import defaultdict\n","\n","def diagnose_no2_feature_issues():\n","    \"\"\"ËØäÊñ≠NO2ÁâπÂæÅÈóÆÈ¢ò\"\"\"\n","    print(\"üîç NO2 Feature Issues Diagnosis\")\n","    print(\"=\" * 50)\n","\n","    BASE_NO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","\n","    # Ê£ÄÊü•ÁöÑÁâπÂæÅ\n","    problem_features = ['sp', 'ssr_clr', 'str']\n","\n","    # ÁªüËÆ°‰ø°ÊÅØ\n","    feature_stats = defaultdict(lambda: {\n","        'total_files': 0,\n","        'files_with_feature': 0,\n","        'files_with_nan': 0,\n","        'files_with_finite': 0,\n","        'total_pixels': 0,\n","        'finite_pixels': 0,\n","        'nan_pixels': 0,\n","        'zero_pixels': 0,\n","        'negative_pixels': 0,\n","        'positive_pixels': 0\n","    })\n","\n","    # Ê£ÄÊü•Âá†‰∏™Ê†∑Êú¨Êñá‰ª∂\n","    sample_files = []\n","    for year in [2019, 2020, 2021]:\n","        year_dir = os.path.join(BASE_NO2, f\"NO2_{year}\")\n","        if os.path.exists(year_dir):\n","            files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","            if files:\n","                sample_files.append(os.path.join(year_dir, files[0]))  # ÂèñÁ¨¨‰∏Ä‰∏™Êñá‰ª∂\n","                if len(sample_files) >= 3:  # ÊúÄÂ§öÊ£ÄÊü•3‰∏™Êñá‰ª∂\n","                    break\n","\n","    print(f\"üìÅ Checking {len(sample_files)} sample files:\")\n","    for file_path in sample_files:\n","        print(f\"  {os.path.basename(file_path)}\")\n","\n","    print(f\"\\nüîç Detailed Analysis:\")\n","\n","    for file_path in sample_files:\n","        print(f\"\\nüìÑ File: {os.path.basename(file_path)}\")\n","\n","        try:\n","            with np.load(file_path) as data:\n","                print(f\"  üìä Available keys: {list(data.files)}\")\n","\n","                # Ê£ÄÊü•ÁõÆÊ†áÊï∞ÊçÆÂíåÊé©ËÜú\n","                if 'no2_target' in data.files and 'no2_mask' in data.files:\n","                    target = data['no2_target']\n","                    mask = data['no2_mask']\n","\n","                    # ËÆ°ÁÆóÊúâÊïàÂÉèÁ¥†\n","                    valid_mask = (mask == 1) & (target > 0)\n","                    valid_pixels = valid_mask.sum()\n","                    total_pixels = mask.size\n","\n","                    print(f\"  üéØ Valid pixels: {valid_pixels:,}/{total_pixels:,} ({100*valid_pixels/total_pixels:.1f}%)\")\n","\n","                    # Ê£ÄÊü•ÈóÆÈ¢òÁâπÂæÅ\n","                    for feat_name in problem_features:\n","                        if feat_name in data.files:\n","                            feature_data = data[feat_name]\n","\n","                            # Âú®ÊúâÊïàÂÉèÁ¥†‰∏äÁöÑÁªüËÆ°\n","                            valid_feature_data = feature_data[valid_mask]\n","\n","                            # ÁªüËÆ°‰ø°ÊÅØ\n","                            total_valid = len(valid_feature_data)\n","                            finite_count = np.isfinite(valid_feature_data).sum()\n","                            nan_count = np.isnan(valid_feature_data).sum()\n","                            zero_count = (valid_feature_data == 0).sum()\n","                            negative_count = (valid_feature_data < 0).sum()\n","                            positive_count = (valid_feature_data > 0).sum()\n","\n","                            print(f\"    {feat_name}:\")\n","                            print(f\"      Total valid pixels: {total_valid:,}\")\n","                            print(f\"      Finite values: {finite_count:,} ({100*finite_count/total_valid:.1f}%)\")\n","                            print(f\"      NaN values: {nan_count:,} ({100*nan_count/total_valid:.1f}%)\")\n","                            print(f\"      Zero values: {zero_count:,} ({100*zero_count/total_valid:.1f}%)\")\n","                            print(f\"      Negative values: {negative_count:,} ({100*negative_count/total_valid:.1f}%)\")\n","                            print(f\"      Positive values: {positive_count:,} ({100*positive_count/total_valid:.1f}%)\")\n","\n","                            if finite_count > 0:\n","                                finite_data = valid_feature_data[np.isfinite(valid_feature_data)]\n","                                print(f\"      Range: [{finite_data.min():.6f}, {finite_data.max():.6f}]\")\n","                                print(f\"      Mean: {finite_data.mean():.6f}\")\n","                                print(f\"      Std: {finite_data.std():.6f}\")\n","\n","                            # Êõ¥Êñ∞ÁªüËÆ°\n","                            feature_stats[feat_name]['total_files'] += 1\n","                            feature_stats[feat_name]['files_with_feature'] += 1\n","                            feature_stats[feat_name]['total_pixels'] += total_valid\n","                            feature_stats[feat_name]['finite_pixels'] += finite_count\n","                            feature_stats[feat_name]['nan_pixels'] += nan_count\n","\n","                            if finite_count > 0:\n","                                feature_stats[feat_name]['files_with_finite'] += 1\n","                            if nan_count > 0:\n","                                feature_stats[feat_name]['files_with_nan'] += 1\n","                        else:\n","                            print(f\"    {feat_name}: ‚ùå NOT FOUND in file\")\n","                            feature_stats[feat_name]['total_files'] += 1\n","\n","        except Exception as e:\n","            print(f\"  ‚ùå Error loading file: {e}\")\n","\n","    # ÊÄªÁªìÁªüËÆ°\n","    print(f\"\\nüìä Summary Statistics:\")\n","    for feat_name in problem_features:\n","        stats = feature_stats[feat_name]\n","        print(f\"\\n  {feat_name}:\")\n","        print(f\"    Files checked: {stats['total_files']}\")\n","        print(f\"    Files with feature: {stats['files_with_feature']}\")\n","        print(f\"    Files with finite values: {stats['files_with_finite']}\")\n","        print(f\"    Files with NaN values: {stats['files_with_nan']}\")\n","\n","        if stats['total_pixels'] > 0:\n","            finite_ratio = stats['finite_pixels'] / stats['total_pixels']\n","            nan_ratio = stats['nan_pixels'] / stats['total_pixels']\n","            print(f\"    Finite ratio: {finite_ratio:.1%}\")\n","            print(f\"    NaN ratio: {nan_ratio:.1%}\")\n","\n","            if finite_ratio < 0.1:\n","                print(f\"    ‚ö†Ô∏è Very low finite ratio - consider removing from CONTINUOUS_FEATURES\")\n","            elif nan_ratio > 0.9:\n","                print(f\"    ‚ö†Ô∏è Very high NaN ratio - consider removing from CONTINUOUS_FEATURES\")\n","            else:\n","                print(f\"    ‚úÖ Data quality acceptable\")\n","\n","def check_feature_naming_consistency():\n","    \"\"\"Ê£ÄÊü•ÁâπÂæÅÂëΩÂêç‰∏ÄËá¥ÊÄß\"\"\"\n","    print(f\"\\nüîç Feature Naming Consistency Check:\")\n","    print(\"=\" * 50)\n","\n","    BASE_NO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","\n","    # Ê£ÄÊü•‰∏çÂêåÂπ¥‰ªΩÁöÑÊñá‰ª∂ÈîÆÂêç\n","    all_keys = set()\n","    year_keys = {}\n","\n","    for year in [2019, 2020, 2021]:\n","        year_dir = os.path.join(BASE_NO2, f\"NO2_{year}\")\n","        if os.path.exists(year_dir):\n","            files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","            if files:\n","                # Ê£ÄÊü•Á¨¨‰∏Ä‰∏™Êñá‰ª∂\n","                file_path = os.path.join(year_dir, files[0])\n","                try:\n","                    with np.load(file_path) as data:\n","                        keys = set(data.files)\n","                        all_keys.update(keys)\n","                        year_keys[year] = keys\n","                        print(f\"  {year}: {len(keys)} keys\")\n","                except Exception as e:\n","                    print(f\"  {year}: Error - {e}\")\n","\n","    # Ê£ÄÊü•ÂÖ≥ÈîÆÁâπÂæÅ\n","    key_features = ['sp', 'ssr_clr', 'ssr_clear', 'str']\n","    print(f\"\\nüéØ Key Feature Analysis:\")\n","\n","    for feat in key_features:\n","        found_in_years = []\n","        for year, keys in year_keys.items():\n","            if feat in keys:\n","                found_in_years.append(year)\n","\n","        if found_in_years:\n","            print(f\"  {feat}: Found in years {found_in_years}\")\n","        else:\n","            print(f\"  {feat}: ‚ùå NOT FOUND in any year\")\n","\n","    # Ê£ÄÊü•ÂèØËÉΩÁöÑÂëΩÂêçÂèò‰Ωì\n","    print(f\"\\nüîç Possible naming variants:\")\n","    for year, keys in year_keys.items():\n","        radiation_keys = [k for k in keys if 'ssr' in k.lower() or 'radiation' in k.lower()]\n","        pressure_keys = [k for k in keys if 'sp' in k.lower() or 'pressure' in k.lower()]\n","        str_keys = [k for k in keys if 'str' in k.lower()]\n","\n","        if radiation_keys:\n","            print(f\"  {year} radiation-related keys: {radiation_keys}\")\n","        if pressure_keys:\n","            print(f\"  {year} pressure-related keys: {pressure_keys}\")\n","        if str_keys:\n","            print(f\"  {year} str-related keys: {str_keys}\")\n","\n","if __name__ == \"__main__\":\n","    diagnose_no2_feature_issues()\n","    check_feature_naming_consistency()\n","\n","    print(f\"\\nüí° Recommendations:\")\n","    print(\"=\" * 50)\n","    print(\"1. If sp/ssr_clr/str have >90% NaN values ‚Üí Remove from CONTINUOUS_FEATURES\")\n","    print(\"2. If naming inconsistency found ‚Üí Update feature names in scaler\")\n","    print(\"3. If data coverage is poor ‚Üí Consider alternative features or imputation\")\n","    print(\"4. For str with low samples ‚Üí Consider removing or using alternative radiation variable\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y33NQ6QfjDBm","executionInfo":{"status":"ok","timestamp":1758061412817,"user_tz":-120,"elapsed":175,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"974423a0-90a1-4d8c-af0a-220d14c25cea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç NO2 Feature Issues Diagnosis\n","==================================================\n","üìÅ Checking 3 sample files:\n","  NO2_stack_20190102.npz\n","  NO2_stack_20200101.npz\n","  NO2_stack_20210101.npz\n","\n","üîç Detailed Analysis:\n","\n","üìÑ File: NO2_stack_20190102.npz\n","  üìä Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","  üéØ Valid pixels: 73,631/186,300 (39.5%)\n","    sp:\n","      Total valid pixels: 73,631\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 0 (0.0%)\n","      Positive values: 73,631 (100.0%)\n","    ssr_clr:\n","      Total valid pixels: 73,631\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 0 (0.0%)\n","      Positive values: 73,631 (100.0%)\n","    str:\n","      Total valid pixels: 73,631\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 73,631 (100.0%)\n","      Positive values: 0 (0.0%)\n","\n","üìÑ File: NO2_stack_20200101.npz\n","  üìä Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","  üéØ Valid pixels: 88,336/186,300 (47.4%)\n","    sp:\n","      Total valid pixels: 88,336\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 0 (0.0%)\n","      Positive values: 88,336 (100.0%)\n","    ssr_clr:\n","      Total valid pixels: 88,336\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 0 (0.0%)\n","      Positive values: 88,336 (100.0%)\n","    str:\n","      Total valid pixels: 88,336\n","      Finite values: 0 (0.0%)\n","      NaN values: 0 (0.0%)\n","      Zero values: 0 (0.0%)\n","      Negative values: 88,336 (100.0%)\n","      Positive values: 0 (0.0%)\n","\n","üìÑ File: NO2_stack_20210101.npz\n","  üìä Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","  üéØ Valid pixels: 0/186,300 (0.0%)\n","    sp:\n","      Total valid pixels: 0\n","      Finite values: 0 (nan%)\n","      NaN values: 0 (nan%)\n","      Zero values: 0 (nan%)\n","      Negative values: 0 (nan%)\n","      Positive values: 0 (nan%)\n","    ssr_clr:\n","      Total valid pixels: 0\n","      Finite values: 0 (nan%)\n","      NaN values: 0 (nan%)\n","      Zero values: 0 (nan%)\n","      Negative values: 0 (nan%)\n","      Positive values: 0 (nan%)\n","    str:\n","      Total valid pixels: 0\n","      Finite values: 0 (nan%)\n","      NaN values: 0 (nan%)\n","      Zero values: 0 (nan%)\n","      Negative values: 0 (nan%)\n","      Positive values: 0 (nan%)\n","\n","üìä Summary Statistics:\n","\n","  sp:\n","    Files checked: 3\n","    Files with feature: 3\n","    Files with finite values: 0\n","    Files with NaN values: 0\n","    Finite ratio: 0.0%\n","    NaN ratio: 0.0%\n","    ‚ö†Ô∏è Very low finite ratio - consider removing from CONTINUOUS_FEATURES\n","\n","  ssr_clr:\n","    Files checked: 3\n","    Files with feature: 3\n","    Files with finite values: 0\n","    Files with NaN values: 0\n","    Finite ratio: 0.0%\n","    NaN ratio: 0.0%\n","    ‚ö†Ô∏è Very low finite ratio - consider removing from CONTINUOUS_FEATURES\n","\n","  str:\n","    Files checked: 3\n","    Files with feature: 3\n","    Files with finite values: 0\n","    Files with NaN values: 0\n","    Finite ratio: 0.0%\n","    NaN ratio: 0.0%\n","    ‚ö†Ô∏è Very low finite ratio - consider removing from CONTINUOUS_FEATURES\n","\n","üîç Feature Naming Consistency Check:\n","==================================================\n","  2019: 37 keys\n","  2020: 37 keys\n","  2021: 37 keys\n","\n","üéØ Key Feature Analysis:\n","  sp: Found in years [2019, 2020, 2021]\n","  ssr_clr: Found in years [2019, 2020, 2021]\n","  ssr_clear: ‚ùå NOT FOUND in any year\n","  str: Found in years [2019, 2020, 2021]\n","\n","üîç Possible naming variants:\n","  2019 radiation-related keys: ['ssr_clr']\n","  2019 pressure-related keys: ['sp']\n","  2019 str-related keys: ['str']\n","  2020 radiation-related keys: ['ssr_clr']\n","  2020 pressure-related keys: ['sp']\n","  2020 str-related keys: ['str']\n","  2021 radiation-related keys: ['ssr_clr']\n","  2021 pressure-related keys: ['sp']\n","  2021 str-related keys: ['str']\n","\n","üí° Recommendations:\n","==================================================\n","1. If sp/ssr_clr/str have >90% NaN values ‚Üí Remove from CONTINUOUS_FEATURES\n","2. If naming inconsistency found ‚Üí Update feature names in scaler\n","3. If data coverage is poor ‚Üí Consider alternative features or imputation\n","4. For str with low samples ‚Üí Consider removing or using alternative radiation variable\n"]}]},{"cell_type":"code","source":["# Quick Check Problem Features\n","import os\n","import numpy as np\n","\n","def quick_check_problem_features():\n","    \"\"\"Âø´ÈÄüÊ£ÄÊü•ÈóÆÈ¢òÁâπÂæÅ\"\"\"\n","    print(\"üîç Quick Check Problem Features\")\n","    print(\"=\" * 40)\n","\n","    BASE_NO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","\n","    # Ê£ÄÊü•‰∏Ä‰∏™Ê†∑Êú¨Êñá‰ª∂\n","    sample_file = None\n","    for year in [2019, 2020, 2021]:\n","        year_dir = os.path.join(BASE_NO2, f\"NO2_{year}\")\n","        if os.path.exists(year_dir):\n","            files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","            if files:\n","                sample_file = os.path.join(year_dir, files[0])\n","                break\n","\n","    if not sample_file:\n","        print(\"‚ùå No sample file found\")\n","        return\n","\n","    print(f\"üìÑ Checking file: {os.path.basename(sample_file)}\")\n","\n","    try:\n","        with np.load(sample_file) as data:\n","            print(f\"üìä Available keys: {list(data.files)}\")\n","\n","            # Ê£ÄÊü•ÁõÆÊ†áÊï∞ÊçÆÂíåÊé©ËÜú\n","            if 'no2_target' in data.files and 'no2_mask' in data.files:\n","                target = data['no2_target']\n","                mask = data['no2_mask']\n","\n","                # ËÆ°ÁÆóÊúâÊïàÂÉèÁ¥†\n","                valid_mask = (mask == 1) & (target > 0)\n","                valid_pixels = valid_mask.sum()\n","                total_pixels = mask.size\n","\n","                print(f\"üéØ Valid pixels: {valid_pixels:,}/{total_pixels:,} ({100*valid_pixels/total_pixels:.1f}%)\")\n","\n","                # Ê£ÄÊü•ÈóÆÈ¢òÁâπÂæÅ\n","                problem_features = ['sp', 'ssr_clr', 'str']\n","\n","                for feat_name in problem_features:\n","                    print(f\"\\nüîç {feat_name}:\")\n","\n","                    if feat_name in data.files:\n","                        feature_data = data[feat_name]\n","                        valid_feature_data = feature_data[valid_mask]\n","\n","                        # Âü∫Êú¨ÁªüËÆ°\n","                        total_valid = len(valid_feature_data)\n","                        finite_count = np.isfinite(valid_feature_data).sum()\n","                        nan_count = np.isnan(valid_feature_data).sum()\n","\n","                        print(f\"  Total valid pixels: {total_valid:,}\")\n","                        print(f\"  Finite values: {finite_count:,} ({100*finite_count/total_valid:.1f}%)\")\n","                        print(f\"  NaN values: {nan_count:,} ({100*nan_count/total_valid:.1f}%)\")\n","\n","                        if finite_count > 0:\n","                            finite_data = valid_feature_data[np.isfinite(valid_feature_data)]\n","                            print(f\"  Range: [{finite_data.min():.6f}, {finite_data.max():.6f}]\")\n","                            print(f\"  Mean: {finite_data.mean():.6f}\")\n","                            print(f\"  Std: {finite_data.std():.6f}\")\n","\n","                            # Âà§Êñ≠Êï∞ÊçÆË¥®Èáè\n","                            if finite_count / total_valid < 0.1:\n","                                print(f\"  ‚ö†Ô∏è Very low finite ratio - should be removed from CONTINUOUS_FEATURES\")\n","                            elif nan_count / total_valid > 0.9:\n","                                print(f\"  ‚ö†Ô∏è Very high NaN ratio - should be removed from CONTINUOUS_FEATURES\")\n","                            else:\n","                                print(f\"  ‚úÖ Data quality acceptable\")\n","                        else:\n","                            print(f\"  ‚ùå No finite values - should be removed from CONTINUOUS_FEATURES\")\n","                    else:\n","                        print(f\"  ‚ùå Feature not found in file\")\n","\n","                        # Ê£ÄÊü•ÂèØËÉΩÁöÑÂëΩÂêçÂèò‰Ωì\n","                        possible_names = []\n","                        if feat_name == 'ssr_clr':\n","                            possible_names = ['ssr_clear', 'ssr', 'clear_sky_radiation']\n","                        elif feat_name == 'sp':\n","                            possible_names = ['pressure', 'surface_pressure', 'sp_pressure']\n","                        elif feat_name == 'str':\n","                            possible_names = ['surface_thermal_radiation', 'thermal_radiation', 'str_radiation']\n","\n","                        for possible_name in possible_names:\n","                            if possible_name in data.files:\n","                                print(f\"  üí° Found possible variant: {possible_name}\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error loading file: {e}\")\n","\n","if __name__ == \"__main__\":\n","    quick_check_problem_features()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WXVlJNi6mWfE","executionInfo":{"status":"ok","timestamp":1758061412924,"user_tz":-120,"elapsed":24,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"ae4e7fd5-dbf8-411d-a338-3440fd67a503"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç Quick Check Problem Features\n","========================================\n","üìÑ Checking file: NO2_stack_20190102.npz\n","üìä Available keys: ['no2_target', 'no2_mask', 'dem', 'slope', 'pop', 'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4', 'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9', 'sin_doy', 'cos_doy', 'weekday_weight', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr', 'ws', 'wd_sin', 'wd_cos', 'no2_lag_1day', 'no2_neighbor', 'coverage_rate', 'valid_pixels', 'total_pixels', 'coverage_in_aoi', 'aoi_pixels', 'trainable']\n","üéØ Valid pixels: 73,631/186,300 (39.5%)\n","\n","üîç sp:\n","  Total valid pixels: 73,631\n","  Finite values: 0 (0.0%)\n","  NaN values: 0 (0.0%)\n","  ‚ùå No finite values - should be removed from CONTINUOUS_FEATURES\n","\n","üîç ssr_clr:\n","  Total valid pixels: 73,631\n","  Finite values: 0 (0.0%)\n","  NaN values: 0 (0.0%)\n","  ‚ùå No finite values - should be removed from CONTINUOUS_FEATURES\n","\n","üîç str:\n","  Total valid pixels: 73,631\n","  Finite values: 0 (0.0%)\n","  NaN values: 0 (0.0%)\n","  ‚ùå No finite values - should be removed from CONTINUOUS_FEATURES\n"]}]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/GeoGapFiller/regenerate_no2_scaler_fixed.py\n","#!/usr/bin/env python3\n","\"\"\"\n","NO2 ScalerÈáçÊñ∞ÁîüÊàêËÑöÊú¨Ôºà‰øÆÂ§çÁâàÔºâ\n","Regenerate NO2 Scaler (Fixed Version)\n","\n","‰ΩøÁî®‰øÆÂ§çÂêéÁöÑÁâπÂæÅÊ†àÈáçÊñ∞ÁîüÊàêNO2Ê†áÂáÜÂåñÂèÇÊï∞\n","\"\"\"\n","\n","import os\n","import numpy as np\n","import sys\n","from datetime import datetime\n","import time\n","\n","# Ê∑ªÂä†ÂΩìÂâçÁõÆÂΩïÂà∞PythonË∑ØÂæÑ\n","sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n","\n","def regenerate_no2_scaler():\n","    \"\"\"ÈáçÊñ∞ÁîüÊàêNO2 scaler\"\"\"\n","    print(\"üöÄ Regenerating NO2 Scaler (Fixed Version)\")\n","    print(\"=\" * 60)\n","\n","    # ÈÖçÁΩÆË∑ØÂæÑ\n","    base_path = \"/content/drive/MyDrive\"\n","    feature_stack_dir = os.path.join(base_path, \"Feature_Stacks\", \"NO2_Independent\")\n","    output_dir = os.path.join(base_path, \"Scalers\")\n","\n","    # Á°Æ‰øùËæìÂá∫ÁõÆÂΩïÂ≠òÂú®\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # ËæìÂá∫Êñá‰ª∂Ë∑ØÂæÑ\n","    scaler_file = os.path.join(output_dir, \"no2_scalers_2019_2021_fixed.npz\")\n","\n","    print(f\"üìÅ Feature stack directory: {feature_stack_dir}\")\n","    print(f\"üìÅ Output directory: {output_dir}\")\n","    print(f\"üìÅ Scaler file: {scaler_file}\")\n","\n","    # Ê£ÄÊü•ÁâπÂæÅÊ†àÁõÆÂΩï\n","    if not os.path.exists(feature_stack_dir):\n","        print(f\"‚ùå Feature stack directory not found: {feature_stack_dir}\")\n","        return False\n","\n","    # Ëé∑ÂèñÁâπÂæÅÊ†àÊñá‰ª∂ÂàóË°®\n","    feature_files = [f for f in os.listdir(feature_stack_dir) if f.startswith(\"NO2_stack_\") and f.endswith(\".npz\")]\n","    feature_files.sort()\n","\n","    print(f\"üìä Found {len(feature_files)} feature stack files\")\n","\n","    if len(feature_files) == 0:\n","        print(\"‚ùå No feature stack files found!\")\n","        return False\n","\n","    # ÂÆö‰πâÁâπÂæÅÁ±ªÂûã\n","    CONTINUOUS_FEATURES = [\n","        'dem', 'slope', 'pop', 'u10', 'v10', 'blh', 'tp', 't2m',\n","        'sp', 'str', 'ssr_clr', 'ws', 'no2_lag_1day', 'no2_neighbor'\n","    ]\n","\n","    CATEGORICAL_FEATURES = [\n","        'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4',\n","        'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9'\n","    ]\n","\n","    NON_STANDARDIZED = [\n","        'wd_sin', 'wd_cos', 'sin_doy', 'cos_doy', 'weekday_weight'\n","    ]\n","\n","    # ÂàùÂßãÂåñÁªüËÆ°‰ø°ÊÅØ\n","    scalers = {}\n","    feature_order = []\n","    total_samples = 0\n","    processed_files = 0\n","    failed_files = 0\n","\n","    # ‰∏∫ÊØè‰∏™ËøûÁª≠ÁâπÂæÅÂàùÂßãÂåñÁªüËÆ°‰ø°ÊÅØ\n","    for feat in CONTINUOUS_FEATURES:\n","        scalers[feat] = {\n","            'sum': 0.0,\n","            'sum_sq': 0.0,\n","            'count': 0,\n","            'min': float('inf'),\n","            'max': float('-inf')\n","        }\n","\n","    print(f\"\\nüîç Processing feature stack files...\")\n","    start_time = time.time()\n","\n","    # Â§ÑÁêÜÊØè‰∏™ÁâπÂæÅÊ†àÊñá‰ª∂\n","    for i, filename in enumerate(feature_files):\n","        try:\n","            file_path = os.path.join(feature_stack_dir, filename)\n","\n","            # Âä†ËΩΩÁâπÂæÅÊ†à\n","            with np.load(file_path, allow_pickle=True) as data:\n","                # Ê£ÄÊü•ÂøÖË¶ÅÁöÑÈîÆ\n","                if 'no2_target' not in data or 'no2_mask' not in data:\n","                    print(f\"   ‚ö†Ô∏è {filename}: Missing target or mask, skipping\")\n","                    failed_files += 1\n","                    continue\n","\n","                # Ëé∑ÂèñÁõÆÊ†áÂèòÈáèÂíåÊé©ËÜú\n","                y = data['no2_target']\n","                mask = data['no2_mask']\n","\n","                # ÂàõÂª∫ÊúâÊïàÊé©ËÜúÔºàmask==1‰∏îy>0Ôºâ\n","                valid_mask = (mask == 1) & (y > 0)\n","\n","                if not np.any(valid_mask):\n","                    print(f\"   ‚ö†Ô∏è {filename}: No valid pixels, skipping\")\n","                    failed_files += 1\n","                    continue\n","\n","                # ÊûÑÂª∫ÁâπÂæÅÁü©Èòµ\n","                X = []\n","                current_feature_order = []\n","\n","                # Ê∑ªÂä†ËøûÁª≠ÁâπÂæÅ\n","                for feat in CONTINUOUS_FEATURES:\n","                    if feat in data:\n","                        X.append(data[feat])\n","                        current_feature_order.append(feat)\n","                    else:\n","                        print(f\"   ‚ö†Ô∏è {filename}: Missing continuous feature {feat}\")\n","                        # ‰ΩøÁî®NaNÂ°´ÂÖÖ\n","                        X.append(np.full_like(y, np.nan))\n","                        current_feature_order.append(feat)\n","\n","                # Ê∑ªÂä†ÂàÜÁ±ªÁâπÂæÅÔºàone-hotÁºñÁ†ÅÔºâ\n","                for feat in CATEGORICAL_FEATURES:\n","                    if feat in data:\n","                        X.append(data[feat])\n","                        current_feature_order.append(feat)\n","                    else:\n","                        print(f\"   ‚ö†Ô∏è {filename}: Missing categorical feature {feat}\")\n","                        # ‰ΩøÁî®0Â°´ÂÖÖ\n","                        X.append(np.zeros_like(y))\n","                        current_feature_order.append(feat)\n","\n","                # Ê∑ªÂä†ÈùûÊ†áÂáÜÂåñÁâπÂæÅ\n","                for feat in NON_STANDARDIZED:\n","                    if feat in data:\n","                        X.append(data[feat])\n","                        current_feature_order.append(feat)\n","                    else:\n","                        print(f\"   ‚ö†Ô∏è {filename}: Missing non-standardized feature {feat}\")\n","                        # ‰ΩøÁî®ÈªòËÆ§ÂÄºÂ°´ÂÖÖ\n","                        if feat in ['wd_sin', 'wd_cos']:\n","                            X.append(np.zeros_like(y))\n","                        elif feat in ['sin_doy', 'cos_doy']:\n","                            X.append(np.zeros_like(y))\n","                        elif feat == 'weekday_weight':\n","                            X.append(np.ones_like(y))\n","                        current_feature_order.append(feat)\n","\n","                # ËΩ¨Êç¢‰∏∫numpyÊï∞ÁªÑ\n","                X = np.array(X)\n","\n","                # Á°Æ‰øùÁâπÂæÅÈ°∫Â∫è‰∏ÄËá¥\n","                if not feature_order:\n","                    feature_order = current_feature_order\n","                elif feature_order != current_feature_order:\n","                    print(f\"   ‚ö†Ô∏è {filename}: Feature order mismatch, skipping\")\n","                    failed_files += 1\n","                    continue\n","\n","                # Â±ïÂπ≥Êï∞ÊçÆ\n","                X_flat = X.reshape(X.shape[0], -1)\n","                valid_mask_flat = valid_mask.flatten()\n","\n","                # Êõ¥Êñ∞ÁªüËÆ°‰ø°ÊÅØ\n","                for j, feat in enumerate(CONTINUOUS_FEATURES):\n","                    if j < X_flat.shape[0]:\n","                        feat_data = X_flat[j, valid_mask_flat]\n","                        finite_mask = np.isfinite(feat_data)\n","\n","                        if np.any(finite_mask):\n","                            finite_data = feat_data[finite_mask]\n","                            scalers[feat]['sum'] += np.sum(finite_data)\n","                            scalers[feat]['sum_sq'] += np.sum(finite_data**2)\n","                            scalers[feat]['count'] += len(finite_data)\n","                            scalers[feat]['min'] = min(scalers[feat]['min'], np.min(finite_data))\n","                            scalers[feat]['max'] = max(scalers[feat]['max'], np.max(finite_data))\n","\n","                processed_files += 1\n","                total_samples += np.sum(valid_mask)\n","\n","                if (i + 1) % 100 == 0:\n","                    elapsed = time.time() - start_time\n","                    rate = (i + 1) / elapsed if elapsed > 0 else 0\n","                    print(f\"   üìä Processed {i + 1}/{len(feature_files)} files, rate: {rate:.1f} files/sec\")\n","\n","        except Exception as e:\n","            print(f\"   ‚ùå {filename}: Error - {e}\")\n","            failed_files += 1\n","\n","    # ËÆ°ÁÆóÊúÄÁªàÁªüËÆ°‰ø°ÊÅØ\n","    print(f\"\\nüî¢ Computing final statistics...\")\n","\n","    final_scalers = {}\n","    for feat in CONTINUOUS_FEATURES:\n","        if scalers[feat]['count'] > 0:\n","            mean = scalers[feat]['sum'] / scalers[feat]['count']\n","            variance = (scalers[feat]['sum_sq'] / scalers[feat]['count']) - (mean ** 2)\n","            std = np.sqrt(max(0, variance))\n","\n","            final_scalers[feat] = {\n","                'mean': mean,\n","                'std': std,\n","                'count': scalers[feat]['count'],\n","                'min': scalers[feat]['min'],\n","                'max': scalers[feat]['max']\n","            }\n","        else:\n","            # Ê≤°ÊúâÊ†∑Êú¨ÁöÑÁâπÂæÅ‰ΩøÁî®ÈªòËÆ§ÂÄº\n","            final_scalers[feat] = {\n","                'mean': 0.0,\n","                'std': 1.0,\n","                'count': 0,\n","                'min': 0.0,\n","                'max': 0.0\n","            }\n","\n","    # ‰øùÂ≠òscaler\n","    metadata = {\n","        'years': [2019, 2020, 2021],\n","        'total_files': len(feature_files),\n","        'processed_files': processed_files,\n","        'failed_files': failed_files,\n","        'total_samples': total_samples,\n","        'processing_time': time.time() - start_time,\n","        'mask_logic': 'mask==1 is valid, mask==0 is invalid',\n","        'target_logic': 'y > 0 for physical validity'\n","    }\n","\n","    np.savez_compressed(\n","        scaler_file,\n","        scalers=final_scalers,\n","        feature_order=feature_order,\n","        metadata=metadata\n","    )\n","\n","    # ÊòæÁ§∫ÁªìÊûú\n","    print(f\"\\nüìä NO2 Scaler Regeneration Summary:\")\n","    print(f\"   - Total files: {len(feature_files)}\")\n","    print(f\"   - Processed: {processed_files}\")\n","    print(f\"   - Failed: {failed_files}\")\n","    print(f\"   - Success rate: {processed_files/len(feature_files)*100:.1f}%\")\n","    print(f\"   - Total samples: {total_samples:,}\")\n","    print(f\"   - Processing time: {(time.time() - start_time)/60:.1f} minutes\")\n","    print(f\"   - Scaler file: {scaler_file}\")\n","\n","    # ÊòæÁ§∫ÂÖ≥ÈîÆÁâπÂæÅÁªüËÆ°\n","    print(f\"\\nüîç Key Features Statistics:\")\n","    key_features = ['sp', 'str', 'ssr_clr', 'dem', 't2m', 'u10', 'v10']\n","    for feat in key_features:\n","        if feat in final_scalers:\n","            stats = final_scalers[feat]\n","            print(f\"   {feat}: mean={stats['mean']:.4f}, std={stats['std']:.4f}, count={stats['count']:,}\")\n","\n","    return True\n","\n","def main():\n","    \"\"\"‰∏ªÂáΩÊï∞\"\"\"\n","    print(\"üöÄ NO2 Scaler Regeneration (Fixed Version)\")\n","    print(\"=\" * 60)\n","\n","    # Á°ÆËÆ§ÂºÄÂßã\n","    response = input(\"‚ùì Do you want to regenerate NO2 scaler? (y/n): \").lower().strip()\n","\n","    if response == 'y':\n","        success = regenerate_no2_scaler()\n","\n","        if success:\n","            print(\"\\nüéâ NO2 Scaler regeneration completed successfully!\")\n","            print(\"üìã Next steps:\")\n","            print(\"   1. ‚úÖ Scaler regenerated with fixed data\")\n","            print(\"   2. üîÑ Ready for model training\")\n","            print(\"   3. üîÑ Ready for performance evaluation\")\n","        else:\n","            print(\"\\n‚ùå NO2 Scaler regeneration failed!\")\n","    else:\n","        print(\"\\n‚è∏Ô∏è NO2 Scaler regeneration cancelled\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDgWKM4z5wSw","executionInfo":{"status":"ok","timestamp":1758066098558,"user_tz":-120,"elapsed":50,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"541f6b02-177d-400b-fc63-4385c35d18a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/GeoGapFiller/regenerate_no2_scaler_fixed.py\n"]}]},{"cell_type":"code","source":["!python /content/drive/MyDrive/GeoGapFiller/regenerate_no2_scaler_fixed.py"],"metadata":{"id":"cdKsZ8fW5_pd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","\"\"\"\n","NO2 Scaler for Dictionary Structure\n","‰∏ìÈó®Â§ÑÁêÜNO2Â≠óÂÖ∏Âºè.npzÊñá‰ª∂ÁªìÊûÑÁöÑscalerÁîüÊàê\n","\n","NO2Êñá‰ª∂ÁªìÊûÑ: {'no2_target': array, 'dem': array, 'u10': array, ...}\n","ÈúÄË¶ÅÂÖàÊûÑÂª∫ÁâπÂæÅÁü©ÈòµÔºåÂÜçËøõË°åÊ†áÂáÜÂåñ\n","\"\"\"\n","\n","import os\n","import numpy as np\n","import glob\n","from datetime import datetime\n","import time\n","from collections import defaultdict\n","\n","def build_feature_matrix_from_dict(data_dict, feature_order):\n","    \"\"\"\n","    ‰ªéÂ≠óÂÖ∏ÁªìÊûÑÊûÑÂª∫ÁâπÂæÅÁü©Èòµ\n","\n","    Args:\n","        data_dict: NO2Â≠óÂÖ∏ÂºèÊï∞ÊçÆ {'no2_target': array, 'dem': array, ...}\n","        feature_order: ÁâπÂæÅÈ°∫Â∫èÂàóË°®\n","\n","    Returns:\n","        X: ÁâπÂæÅÁü©Èòµ (C, H, W)\n","    \"\"\"\n","    X_list = []\n","\n","    for feat_name in feature_order:\n","        if feat_name in data_dict:\n","            X_list.append(data_dict[feat_name].astype(np.float32))\n","        else:\n","            # Â§ÑÁêÜÁº∫Â§±ÁâπÂæÅ\n","            if feat_name.startswith('lulc_class_'):\n","                # LULCÁâπÂæÅÁº∫Â§±Áî®0Â°´ÂÖÖ\n","                fill_value = 0.0\n","            elif feat_name in ['sin_doy', 'cos_doy', 'wd_sin', 'wd_cos']:\n","                # Êó∂Èó¥/È£éÂêëÁâπÂæÅÁº∫Â§±Áî®0Â°´ÂÖÖ\n","                fill_value = 0.0\n","            elif feat_name == 'weekday_weight':\n","                # Â∑•‰ΩúÊó•ÊùÉÈáçÁº∫Â§±Áî®1Â°´ÂÖÖ\n","                fill_value = 1.0\n","            else:\n","                # ÂÖ∂‰ªñÁâπÂæÅÁº∫Â§±Áî®NaNÂ°´ÂÖÖ\n","                fill_value = np.nan\n","\n","            # Ëé∑ÂèñÁª¥Â∫¶ - ÊîπËøõÔºö‰ºòÂÖà‰ªéno2_targetÊé®Êñ≠\n","            if X_list:\n","                H, W = X_list[0].shape\n","            elif 'no2_target' in data_dict:\n","                H, W = data_dict['no2_target'].shape\n","            elif 'no2_mask' in data_dict:\n","                H, W = data_dict['no2_mask'].shape\n","            else:\n","                H, W = 300, 621  # ÊúÄÂêéÂÖúÂ∫ï\n","\n","            X_list.append(np.full((H, W), fill_value, dtype=np.float32))\n","\n","    return np.stack(X_list, axis=0)  # (C, H, W)\n","\n","def compute_no2_scaler_dictionary_structure(years=[2019, 2020, 2021], sample_per_file=1000):\n","    \"\"\"\n","    ËÆ°ÁÆóNO2Ê†áÂáÜÂåñÂèÇÊï∞ - ‰∏ìÈó®Â§ÑÁêÜÂ≠óÂÖ∏ÁªìÊûÑ\n","\n","    Args:\n","        years: ËÆ≠ÁªÉÂπ¥‰ªΩ\n","        sample_per_file: ÊØè‰∏™Êñá‰ª∂ÈááÊ†∑ÁöÑÂÉèÁ¥†Êï∞\n","    \"\"\"\n","    print(\"üöÄ Computing NO2 Scaler for Dictionary Structure\")\n","    print(\"=\" * 60)\n","    print(\"üìã NO2 file structure: Dictionary format\")\n","    print(\"üìã Process: Build feature matrix ‚Üí Compute statistics ‚Üí Generate scaler\")\n","\n","    # ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠êÁ°Æ‰øùÂèØÂ§çÁé∞ÊÄß\n","    np.random.seed(42)\n","\n","    # ÈÖçÁΩÆË∑ØÂæÑ\n","    base_path = \"/content/drive/MyDrive\"\n","    feature_stack_base = os.path.join(base_path, \"Feature_Stacks\")\n","    output_dir = os.path.join(base_path, \"Scalers\")\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # ÂÆö‰πâÁâπÂæÅÁ±ªÂûãÂíåÈ°∫Â∫è\n","    CONTINUOUS_FEATURES = [\n","        # Ê∞îË±°ÁâπÂæÅ\n","        'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clr',\n","        # È£éÂú∫Ê¥æÁîüÁâπÂæÅ\n","        'ws',  # ÁßªÈô§ wd_sin, wd_cos\n","        # NO2Áõ∏ÂÖ≥ÁâπÂæÅ\n","        'no2_lag_1day', 'no2_neighbor',\n","        # ÈùôÊÄÅÁâπÂæÅ\n","        'dem', 'slope', 'pop'\n","    ]\n","\n","    CATEGORICAL_FEATURES = [\n","        'lulc_class_0', 'lulc_class_1', 'lulc_class_2', 'lulc_class_3', 'lulc_class_4',\n","        'lulc_class_5', 'lulc_class_6', 'lulc_class_7', 'lulc_class_8', 'lulc_class_9'\n","    ]\n","\n","    NON_STANDARDIZED = [\n","        'sin_doy', 'cos_doy', 'weekday_weight',\n","        'wd_sin', 'wd_cos'  # Êñ∞Â¢ûÔºöÈ£éÂêëÁâπÂæÅ‰∏çÊ†áÂáÜÂåñ\n","    ]\n","\n","    # ÂÆåÊï¥ÁöÑÁâπÂæÅÈ°∫Â∫è\n","    feature_order = (\n","        CONTINUOUS_FEATURES +\n","        CATEGORICAL_FEATURES +\n","        NON_STANDARDIZED\n","    )\n","\n","    print(f\"üìä Feature configuration:\")\n","    print(f\"   - Continuous features: {len(CONTINUOUS_FEATURES)}\")\n","    print(f\"   - Categorical features: {len(CATEGORICAL_FEATURES)}\")\n","    print(f\"   - Non-standardized features: {len(NON_STANDARDIZED)}\")\n","    print(f\"   - Total features: {len(feature_order)}\")\n","    print(f\"   - ÊîπËøõ: È£éÂêëÁâπÂæÅ(wd_sin, wd_cos)‰∏çÊ†áÂáÜÂåñÔºå‰øùÊåÅ[-1,1]ËåÉÂõ¥\")\n","    print(f\"   - ÊîπËøõ: ÊúâÊïàÂÉèÁ¥†Âà§ÂÆöÂèØÈÖçÁΩÆÔºåÈªòËÆ§‰∏çËøáÊª§y<=0\")\n","\n","    # ÂàùÂßãÂåñÁªüËÆ°‰ø°ÊÅØ\n","    stats = defaultdict(lambda: {'sum': 0.0, 'sumsq': 0.0, 'count': 0})\n","\n","    total_files = 0\n","    processed_files = 0\n","    failed_files = 0\n","\n","    print(f\"\\nüîç Processing NO2 dictionary files...\")\n","    start_time = time.time()\n","\n","    # ÊåâÂπ¥‰ªΩÂ§ÑÁêÜ\n","    for year in years:\n","        year_dir = os.path.join(feature_stack_base, f\"NO2_{year}\")\n","        if not os.path.exists(year_dir):\n","            print(f\"‚ö†Ô∏è Year directory not found: {year_dir}\")\n","            continue\n","\n","        files = [f for f in os.listdir(year_dir) if f.startswith(\"NO2_stack_\") and f.endswith(\".npz\")]\n","        files.sort()\n","        total_files += len(files)\n","\n","        print(f\"üìÖ Processing {year}: {len(files)} files\")\n","\n","        for i, filename in enumerate(files):\n","            try:\n","                file_path = os.path.join(year_dir, filename)\n","\n","                # Âä†ËΩΩÂ≠óÂÖ∏ÂºèÊï∞ÊçÆ\n","                with np.load(file_path, allow_pickle=True) as data:\n","                    # Ê£ÄÊü•ÂøÖË¶ÅÈîÆ\n","                    if 'no2_target' not in data or 'no2_mask' not in data:\n","                        print(f\"   ‚ö†Ô∏è {filename}: Missing target or mask\")\n","                        failed_files += 1\n","                        continue\n","\n","                    # Ëé∑ÂèñÁõÆÊ†áÂèòÈáèÂíåÊé©ËÜú\n","                    y = data['no2_target']\n","                    mask = data['no2_mask']\n","\n","                    # ÂàõÂª∫ÊúâÊïàÊé©ËÜú - ÊîπËøõÁâàÔºöÂèØÈÖçÁΩÆÊòØÂê¶ËøáÊª§ÈùûÊ≠£ÂÄº\n","                    FILTER_NONPOSITIVE_TARGET = False  # ÂèØÈÖçÁΩÆÔºöÊòØÂê¶ËøáÊª§y<=0ÁöÑÂÉèÁ¥†\n","                    valid_mask = (mask == 1)\n","                    if FILTER_NONPOSITIVE_TARGET:\n","                        valid_mask &= (y > 0)\n","\n","                    if not np.any(valid_mask):\n","                        print(f\"   ‚ö†Ô∏è {filename}: No valid pixels\")\n","                        failed_files += 1\n","                        continue\n","\n","                    # ‰ªéÂ≠óÂÖ∏ÊûÑÂª∫ÁâπÂæÅÁü©Èòµ\n","                    X = build_feature_matrix_from_dict(data, feature_order)\n","\n","                    # ÂΩ¢Áä∂‰∏ÄËá¥ÊÄßÊ£ÄÊü•\n","                    if X.shape[0] != len(feature_order):\n","                        print(f\"   ‚ö†Ô∏è {filename}: Feature matrix shape mismatch: X.shape[0]={X.shape[0]}, feature_order length={len(feature_order)}\")\n","                        failed_files += 1\n","                        continue\n","\n","                    # ÊèêÂèñÊúâÊïàÂÉèÁ¥†\n","                    C, H, W = X.shape\n","                    X_flat = X.reshape(C, -1)  # (C, H*W)\n","                    valid_flat = valid_mask.ravel()  # (H*W,)\n","                    valid_pixels = X_flat[:, valid_flat]  # (C, N_valid)\n","\n","                    if valid_pixels.shape[1] == 0:\n","                        continue\n","\n","                    # ÈöèÊú∫ÈááÊ†∑\n","                    n_samples = min(sample_per_file, valid_pixels.shape[1])\n","                    if n_samples < valid_pixels.shape[1]:\n","                        indices = np.random.choice(valid_pixels.shape[1], n_samples, replace=False)\n","                        sampled = valid_pixels[:, indices]\n","                    else:\n","                        sampled = valid_pixels\n","\n","                    # ËøáÊª§NaN/inf\n","                    finite_mask = np.isfinite(sampled)\n","\n","                    # Á¥ØÁßØÁªüËÆ°‰ø°ÊÅØÔºàÂè™ÂØπËøûÁª≠ÁâπÂæÅÔºâ\n","                    for j, feat_name in enumerate(feature_order):\n","                        if feat_name in CONTINUOUS_FEATURES:\n","                            values = sampled[j]\n","                            finite_values = values[finite_mask[j]]\n","\n","                            if len(finite_values) > 0:\n","                                # 1-99ÁôæÂàÜ‰Ωçwinsorization\n","                                q1, q99 = np.percentile(finite_values, [1, 99])\n","                                clipped = np.clip(finite_values, q1, q99)\n","\n","                                stats[feat_name]['sum'] += np.sum(clipped)\n","                                stats[feat_name]['sumsq'] += np.sum(clipped**2)\n","                                stats[feat_name]['count'] += len(clipped)\n","\n","                    processed_files += 1\n","\n","                    if (i + 1) % 50 == 0:\n","                        elapsed = time.time() - start_time\n","                        rate = (i + 1) / elapsed if elapsed > 0 else 0\n","                        progress_pct = (i + 1) / len(files) * 100\n","                        print(f\"   üìä {year}: {i + 1}/{len(files)} files ({progress_pct:.1f}%), rate: {rate:.1f} files/sec\")\n","\n","            except Exception as e:\n","                print(f\"   ‚ùå {filename}: Error - {e}\")\n","                failed_files += 1\n","\n","    print(f\"\\nüìä Processing statistics:\")\n","    print(f\"   - Total files: {total_files}\")\n","    print(f\"   - Processed: {processed_files}\")\n","    print(f\"   - Failed: {failed_files}\")\n","    print(f\"   - Success rate: {processed_files/total_files*100:.1f}%\")\n","\n","    # ÁâπÂæÅ‰∏ÄËá¥ÊÄßÊ£ÄÊü•\n","    print(f\"\\nüîç Feature consistency check:\")\n","    all_defined_features = set(CONTINUOUS_FEATURES + CATEGORICAL_FEATURES + NON_STANDARDIZED)\n","    feature_order_set = set(feature_order)\n","\n","    missing_in_order = all_defined_features - feature_order_set\n","    extra_in_order = feature_order_set - all_defined_features\n","\n","    if missing_in_order:\n","        print(f\"   ‚ö†Ô∏è Features defined but missing in feature_order: {missing_in_order}\")\n","    if extra_in_order:\n","        print(f\"   ‚ö†Ô∏è Features in feature_order but not defined: {extra_in_order}\")\n","    if not missing_in_order and not extra_in_order:\n","        print(f\"   ‚úÖ All feature names are consistent!\")\n","\n","    if not stats:\n","        print(\"‚ùå No valid samples found\")\n","        return None, None\n","\n","    # ËÆ°ÁÆóÊúÄÁªàÁªüËÆ°‰ø°ÊÅØ\n","    scalers = {}\n","    print(f\"\\nüî¢ Computing standardization parameters...\")\n","\n","    for feat_name in feature_order:\n","        if feat_name in CONTINUOUS_FEATURES:\n","            if feat_name in stats and stats[feat_name]['count'] > 0:\n","                count = stats[feat_name]['count']\n","                mean = stats[feat_name]['sum'] / count\n","                variance = (stats[feat_name]['sumsq'] / count) - (mean**2)\n","                std = np.sqrt(max(variance, 1e-8))\n","\n","                scalers[feat_name] = {\n","                    'mean': float(mean),\n","                    'std': float(std),\n","                    'count': int(count)\n","                }\n","                print(f\"   {feat_name}: mean={mean:.4f}, std={std:.4f}, count={count}\")\n","            else:\n","                scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                print(f\"   {feat_name}: No valid samples, using defaults\")\n","        elif feat_name in CATEGORICAL_FEATURES:\n","            scalers[feat_name] = {'type': 'categorical'}\n","            print(f\"   {feat_name}: categorical (one-hot)\")\n","        elif feat_name in NON_STANDARDIZED:\n","            scalers[feat_name] = {'type': 'non_standardized'}\n","            print(f\"   {feat_name}: non_standardized\")\n","\n","    return scalers, feature_order\n","\n","def apply_no2_scaler_dictionary_structure(data_dict, feature_order, scalers):\n","    \"\"\"\n","    ÂØπÂ≠óÂÖ∏ÂºèNO2Êï∞ÊçÆÂ∫îÁî®Ê†áÂáÜÂåñ\n","\n","    Args:\n","        data_dict: NO2Â≠óÂÖ∏ÂºèÊï∞ÊçÆ\n","        feature_order: ÁâπÂæÅÈ°∫Â∫è\n","        scalers: Ê†áÂáÜÂåñÂèÇÊï∞\n","\n","    Returns:\n","        X_scaled: Ê†áÂáÜÂåñÂêéÁöÑÁâπÂæÅÁü©Èòµ (C, H, W)\n","    \"\"\"\n","    # ÊûÑÂª∫ÁâπÂæÅÁü©Èòµ\n","    X = build_feature_matrix_from_dict(data_dict, feature_order)\n","    X_scaled = X.copy().astype(np.float32)\n","\n","    # Â∫îÁî®Ê†áÂáÜÂåñ\n","    for i, feat_name in enumerate(feature_order):\n","        if feat_name in scalers and 'mean' in scalers[feat_name]:\n","            # ÂØπËøûÁª≠ÁâπÂæÅÂ∫îÁî®z-scoreÊ†áÂáÜÂåñ\n","            mean = scalers[feat_name]['mean']\n","            std = scalers[feat_name]['std']\n","            X_scaled[i] = (X[i] - mean) / std\n","        # Ë∑≥ËøáÂàÜÁ±ªÂíåÈùûÊ†áÂáÜÂåñÁâπÂæÅ\n","\n","    return X_scaled\n","\n","def save_no2_scaler_dictionary_structure(scalers, feature_order, years):\n","    \"\"\"‰øùÂ≠òNO2 scalerÂèÇÊï∞\"\"\"\n","    output_dir = \"/content/drive/MyDrive/Scalers\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    output_file = os.path.join(output_dir, f\"no2_scalers_dict_{years[0]}_{years[-1]}.npz\")\n","\n","    metadata = {\n","        'data_type': 'no2_dictionary',\n","        'years': years,\n","        'file_structure': 'dictionary format: {no2_target: array, dem: array, ...}',\n","        'processing': 'build feature matrix ‚Üí standardize',\n","        'mask_logic': 'mask==1 is valid, mask==0 is invalid',\n","        'target_filtering': 'FILTER_NONPOSITIVE_TARGET=False (includes y=0 as valid)',\n","        'wind_features': 'wd_sin, wd_cos are non-standardized (preserve [-1,1] range)',\n","        'total_features': len(feature_order),\n","        'continuous_features': len([f for f in feature_order if 'mean' in scalers.get(f, {})]),\n","        'categorical_features': len([f for f in feature_order if scalers.get(f, {}).get('type') == 'categorical']),\n","        'non_standardized_features': len([f for f in feature_order if scalers.get(f, {}).get('type') == 'non_standardized'])\n","    }\n","\n","    np.savez_compressed(\n","        output_file,\n","        scalers=scalers,\n","        feature_order=feature_order,\n","        metadata=metadata\n","    )\n","\n","    print(f\"\\n‚úÖ NO2 Dictionary Scaler saved to: {output_file}\")\n","    return output_file\n","\n","def main():\n","    \"\"\"‰∏ªÂáΩÊï∞\"\"\"\n","    print(\"üöÄ NO2 Scaler for Dictionary Structure\")\n","    print(\"=\" * 60)\n","    print(\"üìã This script handles NO2 dictionary-style .npz files\")\n","    print(\"üìã Process: Dictionary ‚Üí Feature Matrix ‚Üí Standardization\")\n","\n","    # ËÆ°ÁÆóscaler\n","    scalers, feature_order = compute_no2_scaler_dictionary_structure(years=[2019, 2020, 2021])\n","\n","    if scalers is not None:\n","        # ‰øùÂ≠òÁªìÊûú\n","        output_file = save_no2_scaler_dictionary_structure(scalers, feature_order, [2019, 2020, 2021])\n","\n","        # ÊòæÁ§∫ÁªüËÆ°‰ø°ÊÅØ\n","        print(f\"\\nüìä NO2 Dictionary Scaler Summary:\")\n","        print(f\"   - Total features: {len(feature_order)}\")\n","        print(f\"   - Continuous features: {len([f for f in feature_order if 'mean' in scalers.get(f, {})])}\")\n","        print(f\"   - Categorical features: {len([f for f in feature_order if scalers.get(f, {}).get('type') == 'categorical'])}\")\n","        print(f\"   - Non-standardized features: {len([f for f in feature_order if scalers.get(f, {}).get('type') == 'non_standardized'])}\")\n","\n","        # ÊòæÁ§∫ÂÖ≥ÈîÆÁâπÂæÅÂèÇÊï∞\n","        print(f\"\\nüîç Key Features Standardization Parameters:\")\n","        key_features = ['dem', 'slope', 'pop', 'u10', 'v10', 't2m', 'no2_lag_1day', 'no2_neighbor']\n","        for feat in key_features:\n","            if feat in scalers and 'mean' in scalers[feat]:\n","                print(f\"   {feat}: mean={scalers[feat]['mean']:.4f}, std={scalers[feat]['std']:.4f}\")\n","            elif feat in scalers:\n","                print(f\"   {feat}: {scalers[feat].get('type', 'unknown type')}\")\n","\n","        print(f\"\\nüéâ NO2 Dictionary Scaler generation completed!\")\n","        print(f\"üìã Next steps:\")\n","        print(f\"   1. ‚úÖ NO2 scaler ready for dictionary structure\")\n","        print(f\"   2. üîÑ Use apply_no2_scaler_dictionary_structure() for standardization\")\n","        print(f\"   3. üîÑ Ready for 3D CNN training\")\n","\n","    else:\n","        print(\"‚ùå NO2 scaler computation failed\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29e0zqYSjYB3","executionInfo":{"status":"ok","timestamp":1758163243386,"user_tz":-120,"elapsed":981749,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"9abbb9cc-114a-4e50-dab3-207e2b8a60f4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ NO2 Scaler for Dictionary Structure\n","============================================================\n","üìã This script handles NO2 dictionary-style .npz files\n","üìã Process: Dictionary ‚Üí Feature Matrix ‚Üí Standardization\n","üöÄ Computing NO2 Scaler for Dictionary Structure\n","============================================================\n","üìã NO2 file structure: Dictionary format\n","üìã Process: Build feature matrix ‚Üí Compute statistics ‚Üí Generate scaler\n","üìä Feature configuration:\n","   - Continuous features: 14\n","   - Categorical features: 10\n","   - Non-standardized features: 5\n","   - Total features: 29\n","   - ÊîπËøõ: È£éÂêëÁâπÂæÅ(wd_sin, wd_cos)‰∏çÊ†áÂáÜÂåñÔºå‰øùÊåÅ[-1,1]ËåÉÂõ¥\n","   - ÊîπËøõ: ÊúâÊïàÂÉèÁ¥†Âà§ÂÆöÂèØÈÖçÁΩÆÔºåÈªòËÆ§‰∏çËøáÊª§y<=0\n","\n","üîç Processing NO2 dictionary files...\n","üìÖ Processing 2019: 365 files\n","   ‚ö†Ô∏è NO2_stack_20190201.npz: No valid pixels\n","   üìä 2019: 50/365 files (13.7%), rate: 8.9 files/sec\n","   ‚ö†Ô∏è NO2_stack_20190403.npz: No valid pixels\n","   üìä 2019: 100/365 files (27.4%), rate: 9.5 files/sec\n","   ‚ö†Ô∏è NO2_stack_20190415.npz: No valid pixels\n","   üìä 2019: 150/365 files (41.1%), rate: 9.8 files/sec\n","   ‚ö†Ô∏è NO2_stack_20190715.npz: No valid pixels\n","   üìä 2019: 200/365 files (54.8%), rate: 9.6 files/sec\n","   üìä 2019: 250/365 files (68.5%), rate: 9.7 files/sec\n","   üìä 2019: 300/365 files (82.2%), rate: 9.6 files/sec\n","   ‚ö†Ô∏è NO2_stack_20191112.npz: No valid pixels\n","   üìä 2019: 350/365 files (95.9%), rate: 9.8 files/sec\n","   ‚ö†Ô∏è NO2_stack_20191220.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20191231.npz: No valid pixels\n","üìÖ Processing 2020: 366 files\n","   üìä 2020: 50/366 files (13.7%), rate: 0.5 files/sec\n","   üìä 2020: 100/366 files (27.3%), rate: 0.6 files/sec\n","   üìä 2020: 150/366 files (41.0%), rate: 0.7 files/sec\n","   ‚ö†Ô∏è NO2_stack_20200612.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20200704.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20200715.npz: No valid pixels\n","   üìä 2020: 200/366 files (54.6%), rate: 0.7 files/sec\n","   ‚ö†Ô∏è NO2_stack_20200722.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20200726.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20200801.npz: No valid pixels\n","   üìä 2020: 250/366 files (68.3%), rate: 0.7 files/sec\n","   ‚ö†Ô∏è NO2_stack_20200914.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20201002.npz: No valid pixels\n","   üìä 2020: 300/366 files (82.0%), rate: 0.7 files/sec\n","   ‚ö†Ô∏è NO2_stack_20201204.npz: No valid pixels\n","   üìä 2020: 350/366 files (95.6%), rate: 0.7 files/sec\n","   ‚ö†Ô∏è NO2_stack_20201231.npz: No valid pixels\n","üìÖ Processing 2021: 365 files\n","   ‚ö†Ô∏è NO2_stack_20210101.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20210105.npz: No valid pixels\n","   üìä 2021: 50/365 files (13.7%), rate: 0.1 files/sec\n","   üìä 2021: 100/365 files (27.4%), rate: 0.2 files/sec\n","   üìä 2021: 150/365 files (41.1%), rate: 0.2 files/sec\n","   üìä 2021: 200/365 files (54.8%), rate: 0.3 files/sec\n","   üìä 2021: 250/365 files (68.5%), rate: 0.3 files/sec\n","   ‚ö†Ô∏è NO2_stack_20210926.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20210929.npz: No valid pixels\n","   üìä 2021: 300/365 files (82.2%), rate: 0.3 files/sec\n","   ‚ö†Ô∏è NO2_stack_20211208.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20211211.npz: No valid pixels\n","   üìä 2021: 350/365 files (95.9%), rate: 0.4 files/sec\n","   ‚ö†Ô∏è NO2_stack_20211225.npz: No valid pixels\n","   ‚ö†Ô∏è NO2_stack_20211231.npz: No valid pixels\n","\n","üìä Processing statistics:\n","   - Total files: 1096\n","   - Processed: 1071\n","   - Failed: 25\n","   - Success rate: 97.7%\n","\n","üîç Feature consistency check:\n","   ‚úÖ All feature names are consistent!\n","\n","üî¢ Computing standardization parameters...\n","   u10: mean=-0.2277, std=1.5682, count=1053253\n","   v10: mean=0.1378, std=1.4416, count=1053253\n","   blh: mean=950.0508, std=559.0005, count=1053253\n","   tp: mean=0.0003, std=0.0009, count=1053253\n","   t2m: mean=15.7258, std=9.9254, count=1053253\n","   sp: mean=93981.8056, std=7744.4218, count=1053253\n","   str: mean=-1345259.2311, std=429521.3468, count=1053253\n","   ssr_clr: mean=6696537.3109, std=2633092.0839, count=1053253\n","   ws: mean=1.7714, std=1.2186, count=1053253\n","   no2_lag_1day: mean=0.0000, std=0.0001, count=1033986\n","   no2_neighbor: mean=0.0001, std=0.0001, count=1053253\n","   dem: mean=618.7810, std=781.1017, count=1053253\n","   slope: mean=11.4541, std=11.0903, count=993467\n","   pop: mean=135.3734, std=370.0605, count=991758\n","   lulc_class_0: categorical (one-hot)\n","   lulc_class_1: categorical (one-hot)\n","   lulc_class_2: categorical (one-hot)\n","   lulc_class_3: categorical (one-hot)\n","   lulc_class_4: categorical (one-hot)\n","   lulc_class_5: categorical (one-hot)\n","   lulc_class_6: categorical (one-hot)\n","   lulc_class_7: categorical (one-hot)\n","   lulc_class_8: categorical (one-hot)\n","   lulc_class_9: categorical (one-hot)\n","   sin_doy: non_standardized\n","   cos_doy: non_standardized\n","   weekday_weight: non_standardized\n","   wd_sin: non_standardized\n","   wd_cos: non_standardized\n","\n","‚úÖ NO2 Dictionary Scaler saved to: /content/drive/MyDrive/Scalers/no2_scalers_dict_2019_2021.npz\n","\n","üìä NO2 Dictionary Scaler Summary:\n","   - Total features: 29\n","   - Continuous features: 14\n","   - Categorical features: 10\n","   - Non-standardized features: 5\n","\n","üîç Key Features Standardization Parameters:\n","   dem: mean=618.7810, std=781.1017\n","   slope: mean=11.4541, std=11.0903\n","   pop: mean=135.3734, std=370.0605\n","   u10: mean=-0.2277, std=1.5682\n","   v10: mean=0.1378, std=1.4416\n","   t2m: mean=15.7258, std=9.9254\n","   no2_lag_1day: mean=0.0000, std=0.0001\n","   no2_neighbor: mean=0.0001, std=0.0001\n","\n","üéâ NO2 Dictionary Scaler generation completed!\n","üìã Next steps:\n","   1. ‚úÖ NO2 scaler ready for dictionary structure\n","   2. üîÑ Use apply_no2_scaler_dictionary_structure() for standardization\n","   3. üîÑ Ready for 3D CNN training\n"]}]},{"cell_type":"markdown","source":["# 3. Generate SO2 standardization parameters"],"metadata":{"id":"9-fCu_qJhMfq"}},{"cell_type":"code","source":["# Generate SO2 standardization parameters\n","import os, numpy as np\n","from collections import defaultdict\n","\n","# Create output directory\n","os.makedirs(\"/content/drive/MyDrive/Scalers\", exist_ok=True)\n","\n","BASE_SO2 = \"/content/drive/MyDrive/Feature_Stacks\"\n","OUT_DIR = \"/content/drive/MyDrive/Scalers\"\n","\n","# Continuous features that need standardization (SO2 version)\n","CONTINUOUS_FEATURES = [\n","    'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clear',\n","    'so2_lag1', 'so2_neighbor', 'so2_climate_prior',  # SO2 related features (removed so2_neighbor_5x5)\n","    'dem', 'slope', 'population',\n","    'ws'  # Wind speed\n","]\n","\n","# Features that should not be standardized\n","NON_STANDARDIZED = ['sin_doy', 'cos_doy', 'weekday_weight', 'wd_sin', 'wd_cos']\n","\n","# Categorical features (one-hot encoded) - SO2 uses lulc_class_10,20,...,100\n","CATEGORICAL_FEATURES = [\n","    'lulc_class_10', 'lulc_class_20', 'lulc_class_30', 'lulc_class_40', 'lulc_class_50',\n","    'lulc_class_60', 'lulc_class_70', 'lulc_class_80', 'lulc_class_90', 'lulc_class_100'\n","]\n","\n","def compute_so2_scalers(years=[2019, 2020, 2021], sample_per_file=1000):\n","    \"\"\"Compute standardization parameters for SO2\"\"\"\n","    print(\"üöÄ Starting SO2 standardization parameter computation...\")\n","    print(f\"Data path: {BASE_SO2}\")\n","    print(f\"Training years: {years}\")\n","    print(f\"Samples per file: {sample_per_file}\")\n","\n","    # Set random seed for reproducibility\n","    rng = np.random.default_rng(42)\n","\n","    # Accumulate statistics\n","    stats = defaultdict(lambda: {'sum': 0.0, 'sumsq': 0.0, 'count': 0})\n","    feature_order = None\n","\n","    total_files = 0\n","    processed_files = 0\n","\n","    for year in years:\n","        year_dir = os.path.join(BASE_SO2, f\"SO2_{year}\")\n","        if not os.path.exists(year_dir):\n","            print(f\"‚ö†Ô∏è Year directory not found: {year_dir}\")\n","            continue\n","\n","        files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","        total_files += len(files)\n","        print(f\"\\n Processing {year}: {len(files)} files\")\n","\n","        for i, fname in enumerate(files):\n","            if i % 50 == 0:\n","                print(f\"  {i}/{len(files)} ({i/len(files)*100:.1f}%)\")\n","\n","            try:\n","                with np.load(os.path.join(year_dir, fname)) as data:\n","                    X = data['X']  # (C, H, W)\n","\n","                    # CONFIRMED: SO2: mask==1 is valid pixels, mask==0 is invalid\n","                    # Additional robustness: ensure target > 0 for physical validity\n","                    valid_mask = (data['mask'] == 1) & (data['y'] > 0)\n","\n","                    if feature_order is None:\n","                        # Get feature names\n","                        if 'feature_order' in data.files:\n","                            feature_order = data['feature_order'].tolist()\n","                        elif 'feature_names' in data.files:\n","                            feature_order = data['feature_names'].tolist()\n","                        else:\n","                            print(f\"‚ö†Ô∏è No feature names found: {fname}\")\n","                            continue\n","\n","                    # --- SO2 È≤ÅÊ£íÊÄßÊ£ÄÊü•ÔºöÈ™åËØÅÁâπÂæÅÁü©Èòµ‰∏éÁâπÂæÅÂêç‰∏ÄËá¥ÊÄß ---\n","                    if X.shape[0] != len(feature_order):\n","                        print(f\"‚ö†Ô∏è Feature matrix shape mismatch in {fname}: X.shape[0]={X.shape[0]}, feature_order length={len(feature_order)}\")\n","                        # ÂèØ‰ª•ÈÄâÊã©Ë∑≥ËøáÊàñË∞ÉÊï¥ÔºåËøôÈáåÈÄâÊã©Ë∑≥Ëøá\n","                        continue\n","\n","                    # --- ÂÖ≥ÈîÆ‰øÆÊ≠£ÔºöÊ≠£Á°ÆÂ§ÑÁêÜ2DÊé©ËÜúÁ¥¢Âºï ---\n","                    C, H, W = X.shape\n","                    X_flat = X.reshape(C, -1)  # (C, H*W) - ÊâìÂπ≥Á©∫Èó¥Áª¥Â∫¶\n","                    valid_flat = valid_mask.ravel()  # (H*W,) - ÊâìÂπ≥Êé©ËÜú\n","                    valid_pixels = X_flat[:, valid_flat]  # (C, N_valid) - Ê≠£Á°ÆÊèêÂèñÊúâÊïàÂÉèÁ¥†\n","\n","                    if valid_pixels.shape[1] == 0:\n","                        continue\n","\n","                    # Random sampling to avoid memory issues (using fixed seed for reproducibility)\n","                    n_samples = min(sample_per_file, valid_pixels.shape[1])\n","                    if n_samples < valid_pixels.shape[1]:\n","                        indices = rng.choice(valid_pixels.shape[1], n_samples, replace=False)\n","                        sampled = valid_pixels[:, indices]\n","                    else:\n","                        sampled = valid_pixels\n","\n","                    # Filter NaN/inf\n","                    finite_mask = np.isfinite(sampled)\n","\n","                    # Accumulate statistics\n","                    for j, feat_name in enumerate(feature_order):\n","                        if feat_name in CONTINUOUS_FEATURES:\n","                            values = sampled[j]\n","                            finite_values = values[finite_mask[j]]\n","\n","                            if len(finite_values) > 0:\n","                                # 1-99 percentile winsorization\n","                                q1, q99 = np.percentile(finite_values, [1, 99])\n","                                clipped = np.clip(finite_values, q1, q99)\n","\n","                                stats[feat_name]['sum'] += np.sum(clipped)\n","                                stats[feat_name]['sumsq'] += np.sum(clipped**2)\n","                                stats[feat_name]['count'] += len(clipped)\n","                        elif feat_name in CATEGORICAL_FEATURES:\n","                            # Categorical features don't need standardization\n","                            pass\n","                        elif feat_name in NON_STANDARDIZED:\n","                            # Non-standardized features don't need standardization\n","                            pass\n","\n","                    processed_files += 1\n","\n","            except Exception as e:\n","                print(f\"‚ùå Failed to load file {fname}: {e}\")\n","                continue\n","\n","    print(f\"\\nüìä Processing statistics:\")\n","    print(f\"Total files: {total_files}\")\n","    print(f\"Successfully processed: {processed_files}\")\n","    print(f\"Processing rate: {processed_files/total_files*100:.1f}%\")\n","\n","    if not stats:\n","        print(\"‚ùå No valid samples found\")\n","        return None\n","\n","    # Compute final statistics\n","    scalers = {}\n","    print(f\"\\nüî¢ Computing standardization parameters...\")\n","\n","    # --- ÂêçÂ≠ó‰∏ÄËá¥ÊÄßËá™Ê£Ä ---\n","    print(f\"\\nüîç Feature name consistency check:\")\n","    all_defined_features = set(CONTINUOUS_FEATURES + CATEGORICAL_FEATURES + NON_STANDARDIZED)\n","    feature_order_set = set(feature_order)\n","\n","    missing_in_order = all_defined_features - feature_order_set\n","    extra_in_order = feature_order_set - all_defined_features\n","\n","    if missing_in_order:\n","        print(f\"  ‚ö†Ô∏è Features defined but missing in feature_order: {missing_in_order}\")\n","    if extra_in_order:\n","        print(f\"  ‚ö†Ô∏è Features in feature_order but not defined: {extra_in_order}\")\n","    if not missing_in_order and not extra_in_order:\n","        print(f\"  ‚úÖ All feature names are consistent!\")\n","\n","    # Ê£ÄÊü•ÂÖ≥ÈîÆÂ≠óÊÆµÂêç\n","    if 'ssr_clear' in feature_order:\n","        print(f\"  ‚úÖ SO2 uses correct field name: ssr_clear\")\n","    else:\n","        print(f\"  ‚ùå Missing ssr_clear in feature_order\")\n","\n","    for feat_name in feature_order:\n","        if feat_name in CONTINUOUS_FEATURES:\n","            # ËøûÁª≠ÁâπÂæÅÔºöÁªü‰∏ÄÂ§ÑÁêÜÔºåÂç≥‰ΩøÊ≤°ÊúâÁªüËÆ°Âà∞‰πüÁªôÈªòËÆ§ÂÄº\n","            if feat_name in stats:\n","                count = stats[feat_name]['count']\n","                if count > 0:\n","                    mean = stats[feat_name]['sum'] / count\n","                    variance = (stats[feat_name]['sumsq'] / count) - (mean**2)\n","                    std = np.sqrt(max(variance, 1e-8))  # Avoid division by zero\n","\n","                    scalers[feat_name] = {\n","                        'mean': float(mean),\n","                        'std': float(std),\n","                        'count': int(count)\n","                    }\n","                    print(f\"  {feat_name}: mean={mean:.4f}, std={std:.4f}, count={count}\")\n","                else:\n","                    # ÊúâÁªüËÆ°‰ΩÜcount=0ÔºåÁªôÈªòËÆ§ÂÄº\n","                    scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                    print(f\"  {feat_name}: No valid samples, using default values\")\n","            else:\n","                # ÂÆåÂÖ®Ê≤°ÊúâÁªüËÆ°Âà∞Ôºå‰πüÁªôÈªòËÆ§ÂÄº\n","                scalers[feat_name] = {'mean': 0.0, 'std': 1.0, 'count': 0}\n","                print(f\"  {feat_name}: No statistics collected, using default values\")\n","        elif feat_name in CATEGORICAL_FEATURES:\n","            scalers[feat_name] = {'type': 'categorical'}\n","            print(f\"  {feat_name}: categorical (one-hot)\")\n","        elif feat_name in NON_STANDARDIZED:\n","            scalers[feat_name] = {'type': 'non_standardized'}\n","            print(f\"  {feat_name}: non_standardized\")\n","        else:\n","            # ËøôÁßçÊÉÖÂÜµÁêÜËÆ∫‰∏ä‰∏çÂ∫îËØ•ÂèëÁîüÔºåÂõ†‰∏∫feature_orderÂ∫îËØ•ÂåÖÂê´ÊâÄÊúâÁâπÂæÅ\n","            scalers[feat_name] = {'type': 'unexpected'}\n","            print(f\"  {feat_name}: unexpected type - check feature_order definition\")\n","\n","    return scalers, feature_order\n","\n","def apply_so2_scaler(X: np.ndarray, feature_names: list, scaler_file: str = None):\n","    \"\"\"\n","    Apply SO2 standardization to feature matrix\n","\n","    Args:\n","        X: Feature matrix (C, H, W) or (C, N)\n","        feature_names: List of feature names\n","        scaler_file: Path to scaler file (if None, auto-detect)\n","\n","    Returns:\n","        X_scaled: Standardized feature matrix\n","    \"\"\"\n","    if scaler_file is None:\n","        scaler_file = os.path.join(OUT_DIR, \"so2_scalers_2019_2021.npz\")\n","\n","    if not os.path.exists(scaler_file):\n","        raise FileNotFoundError(f\"SO2 scaler file not found: {scaler_file}\")\n","\n","    with np.load(scaler_file, allow_pickle=True) as data:\n","        scalers = data['scalers'].item()\n","\n","    X_scaled = X.copy().astype(np.float32)\n","\n","    for i, feat_name in enumerate(feature_names):\n","        if feat_name in scalers and 'mean' in scalers[feat_name]:\n","            # Apply z-score normalization for continuous features\n","            mean = scalers[feat_name]['mean']\n","            std = scalers[feat_name]['std']\n","            X_scaled[i] = (X[i] - mean) / std\n","        # Skip categorical and non-standardized features (they remain unchanged)\n","\n","    return X_scaled\n","\n","# Compute SO2 standardization parameters\n","print(\"=\" * 60)\n","print(\"üöÄ SO2 Standardization Parameter Computation\")\n","print(\"=\" * 60)\n","\n","so2_scalers, so2_features = compute_so2_scalers()\n","\n","if so2_scalers is not None:\n","    # Save results\n","    output_file = os.path.join(OUT_DIR, \"so2_scalers_2019_2021.npz\")\n","    np.savez_compressed(\n","        output_file,\n","        scalers=so2_scalers,\n","        feature_order=so2_features,\n","        metadata={\n","            'data_type': 'so2',\n","            'years': [2019, 2020, 2021],\n","            'mask_logic': 'mask==1 is valid, mask==0 is invalid (CONFIRMED FROM SOURCE CODE)',\n","            'total_features': len(so2_features),\n","            'continuous_features': len([f for f in so2_features if f in CONTINUOUS_FEATURES])\n","        }\n","    )\n","\n","    print(f\"\\n‚úÖ SO2 standardization parameters saved to: {output_file}\")\n","    print(f\" Feature statistics:\")\n","    print(f\"  Total features: {len(so2_features)}\")\n","    print(f\"  Continuous features: {len([f for f in so2_features if f in CONTINUOUS_FEATURES])}\")\n","    print(f\"  Categorical features: {len([f for f in so2_features if f in CATEGORICAL_FEATURES])}\")\n","    print(f\"  Non-standardized features: {len([f for f in so2_features if f in NON_STANDARDIZED])}\")\n","\n","    # Show parameters for first few continuous features\n","    print(f\"\\nüîç Standardization parameters for first 5 continuous features:\")\n","    continuous_count = 0\n","    for feat_name in so2_features:\n","        if feat_name in CONTINUOUS_FEATURES and feat_name in so2_scalers:\n","            if 'mean' in so2_scalers[feat_name]:\n","                print(f\"  {feat_name}: mean={so2_scalers[feat_name]['mean']:.4f}, std={so2_scalers[feat_name]['std']:.4f}\")\n","                continuous_count += 1\n","                if continuous_count >= 5:\n","                    break\n","else:\n","    print(\"‚ùå Computation failed\")\n","\n","print(\"\\n Next step: Ready to train SO2 model!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dX5Ck19yfRnW","executionInfo":{"status":"ok","timestamp":1758061412625,"user_tz":-120,"elapsed":1555087,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"814fe962-1817-4bcb-b5b4-b680f77c00cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","üöÄ SO2 Standardization Parameter Computation\n","============================================================\n","üöÄ Starting SO2 standardization parameter computation...\n","Data path: /content/drive/MyDrive/Feature_Stacks\n","Training years: [2019, 2020, 2021]\n","Samples per file: 1000\n","\n"," Processing 2019: 365 files\n","  0/365 (0.0%)\n","  50/365 (13.7%)\n","  100/365 (27.4%)\n","  150/365 (41.1%)\n","  200/365 (54.8%)\n","  250/365 (68.5%)\n","  300/365 (82.2%)\n","  350/365 (95.9%)\n","\n"," Processing 2020: 366 files\n","  0/366 (0.0%)\n","  50/366 (13.7%)\n","  100/366 (27.3%)\n","  150/366 (41.0%)\n","  200/366 (54.6%)\n","  250/366 (68.3%)\n","  300/366 (82.0%)\n","  350/366 (95.6%)\n","\n"," Processing 2021: 365 files\n","  0/365 (0.0%)\n","  50/365 (13.7%)\n","  100/365 (27.4%)\n","  150/365 (41.1%)\n","  200/365 (54.8%)\n","  250/365 (68.5%)\n","  300/365 (82.2%)\n","  350/365 (95.9%)\n","\n","üìä Processing statistics:\n","Total files: 1096\n","Successfully processed: 779\n","Processing rate: 71.1%\n","\n","üî¢ Computing standardization parameters...\n","\n","üîç Feature name consistency check:\n","  ‚úÖ All feature names are consistent!\n","  ‚úÖ SO2 uses correct field name: ssr_clear\n","  dem: mean=400.9618, std=533.4883, count=761779\n","  slope: mean=9.2690, std=9.5274, count=761779\n","  population: mean=145.4318, std=384.2961, count=761779\n","  lulc_class_10: categorical (one-hot)\n","  lulc_class_20: categorical (one-hot)\n","  lulc_class_30: categorical (one-hot)\n","  lulc_class_40: categorical (one-hot)\n","  lulc_class_50: categorical (one-hot)\n","  lulc_class_60: categorical (one-hot)\n","  lulc_class_70: categorical (one-hot)\n","  lulc_class_80: categorical (one-hot)\n","  lulc_class_90: categorical (one-hot)\n","  lulc_class_100: categorical (one-hot)\n","  u10: mean=-0.4001, std=1.7508, count=761779\n","  v10: mean=0.2341, std=1.5989, count=761779\n","  ws: mean=2.0222, std=1.3279, count=761779\n","  wd_sin: non_standardized\n","  wd_cos: non_standardized\n","  blh: mean=1191.6626, std=466.9831, count=761779\n","  tp: mean=0.0003, std=0.0010, count=761779\n","  t2m: mean=20.3405, std=6.8925, count=761779\n","  sp: mean=96270.7153, std=5620.0365, count=761779\n","  str: mean=-1467077.4248, std=368888.7481, count=761779\n","  ssr_clear: mean=8178523.0930, std=1625752.2839, count=761779\n","  so2_lag1: mean=0.0000, std=0.0001, count=761779\n","  so2_neighbor: mean=0.0006, std=0.0009, count=761779\n","  so2_climate_prior: mean=0.0006, std=0.0004, count=761779\n","  sin_doy: non_standardized\n","  cos_doy: non_standardized\n","  weekday_weight: non_standardized\n","\n","‚úÖ SO2 standardization parameters saved to: /content/drive/MyDrive/Scalers/so2_scalers_2019_2021.npz\n"," Feature statistics:\n","  Total features: 30\n","  Continuous features: 15\n","  Categorical features: 10\n","  Non-standardized features: 5\n","\n","üîç Standardization parameters for first 5 continuous features:\n","  dem: mean=400.9618, std=533.4883\n","  slope: mean=9.2690, std=9.5274\n","  population: mean=145.4318, std=384.2961\n","  u10: mean=-0.4001, std=1.7508\n","  v10: mean=0.2341, std=1.5989\n","\n"," Next step: Ready to train SO2 model!\n"]}]},{"cell_type":"code","source":["# Quick Check SO2 Scaler Results\n","import os\n","import numpy as np\n","\n","def quick_check_so2_scaler_results():\n","    \"\"\"Âø´ÈÄüÊ£ÄÊü•SO2 scalerÁªìÊûú\"\"\"\n","    print(\"üîç Quick SO2 Scaler Results Check\")\n","    print(\"=\" * 50)\n","\n","    # Ê£ÄÊü•scalerÊñá‰ª∂\n","    scaler_file = \"/content/drive/MyDrive/Scalers/so2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(\"‚ùå SO2 scaler file not found\")\n","        return\n","\n","    print(\"‚úÖ Scaler file exists\")\n","\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            print(\"‚úÖ File loaded successfully\")\n","\n","            # Ê£ÄÊü•Âü∫Êú¨ÁªìÊûÑ\n","            print(f\"üìä Available keys: {list(data.files)}\")\n","\n","            # Ê£ÄÊü•ÂÖÉÊï∞ÊçÆ\n","            if 'metadata' in data.files:\n","                metadata = data['metadata'].item()\n","                print(f\"üìÖ Years: {metadata.get('years', 'Unknown')}\")\n","                print(f\"üìÅ Total files: {metadata.get('total_files', 'Unknown')}\")\n","                print(f\"üéØ Success rate: {metadata.get('success_rate', 'Unknown')}\")\n","                print(f\"üìä Total samples: {metadata.get('total_samples', 'Unknown')}\")\n","\n","            # Ê£ÄÊü•scalerÂèÇÊï∞\n","            if 'scalers' in data.files:\n","                scalers = data['scalers'].item()\n","                print(f\"üìä Total features: {len(scalers)}\")\n","\n","                # ÂàÜÊûêËøûÁª≠ÁâπÂæÅ\n","                continuous_features = []\n","                categorical_features = []\n","                non_standardized_features = []\n","\n","                for feat_name, feat_info in scalers.items():\n","                    if isinstance(feat_info, dict):\n","                        if 'mean' in feat_info and 'std' in feat_info:\n","                            continuous_features.append(feat_name)\n","                        elif 'categorical' in feat_info:\n","                            categorical_features.append(feat_name)\n","                        else:\n","                            non_standardized_features.append(feat_name)\n","\n","                print(f\"üìä Feature type summary:\")\n","                print(f\"  Continuous features: {len(continuous_features)}\")\n","                print(f\"  Categorical features: {len(categorical_features)}\")\n","                print(f\"  Non-standardized features: {len(non_standardized_features)}\")\n","\n","                # Ê£ÄÊü•ÂÖ≥ÈîÆËøûÁª≠ÁâπÂæÅ\n","                print(f\"\\nüéØ Key Continuous Features Analysis:\")\n","                key_features = ['dem', 'slope', 'population', 'u10', 'v10', 'blh', 'tp', 't2m', 'sp', 'str', 'ssr_clear', 'ws', 'so2_lag1', 'so2_neighbor', 'so2_climate_prior']\n","\n","                for feat_name in key_features:\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","                        count = scalers[feat_name].get('count', 'Unknown')\n","                        print(f\"  ‚úÖ {feat_name}: {count:,} samples, mean={mean:.4f}, std={std:.4f}\")\n","                    else:\n","                        print(f\"  ‚ùå {feat_name}: Not found or no statistics\")\n","\n","                # Ê£ÄÊü•ÈóÆÈ¢òÁâπÂæÅ\n","                print(f\"\\nüîç Problem Feature Check:\")\n","                problem_features = ['sp', 'str', 'ssr_clear']\n","\n","                for feat_name in problem_features:\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","                        count = scalers[feat_name].get('count', 'Unknown')\n","\n","                        if abs(mean) < 1e-6 and abs(std) < 1e-6:\n","                            print(f\"  ‚ö†Ô∏è {feat_name}: Near-zero values (mean={mean:.6f}, std={std:.6f})\")\n","                        elif count == 0:\n","                            print(f\"  ‚ö†Ô∏è {feat_name}: No samples\")\n","                        else:\n","                            print(f\"  ‚úÖ {feat_name}: Normal values (mean={mean:.4f}, std={std:.4f})\")\n","                    else:\n","                        print(f\"  ‚ùå {feat_name}: Not found\")\n","\n","                # ËÆ°ÁÆóÊàêÂäüÁéá\n","                total_continuous = len(continuous_features)\n","                valid_continuous = sum(1 for f in key_features if f in scalers and 'mean' in scalers[f])\n","                success_rate = valid_continuous / len(key_features) * 100\n","\n","                print(f\"\\nüéØ Success Rate: {success_rate:.1f}%\")\n","\n","                if success_rate >= 90:\n","                    print(\"üéâ SO2 Scaler looks excellent!\")\n","                elif success_rate >= 80:\n","                    print(\"‚úÖ SO2 Scaler looks good!\")\n","                elif success_rate >= 70:\n","                    print(\"‚ö†Ô∏è SO2 Scaler has some issues\")\n","                else:\n","                    print(\"‚ùå SO2 Scaler has significant issues\")\n","\n","                # Ê£ÄÊü•Êï∞ÊçÆË¥®Èáè\n","                print(f\"\\nüìä Data Quality Assessment:\")\n","\n","                # Ê£ÄÊü•Ê†∑Êú¨Êï∞Èáè\n","                sample_counts = [scalers[f].get('count', 0) for f in continuous_features if f in scalers and 'count' in scalers[f]]\n","                if sample_counts:\n","                    min_samples = min(sample_counts)\n","                    max_samples = max(sample_counts)\n","                    avg_samples = sum(sample_counts) / len(sample_counts)\n","\n","                    print(f\"  Sample count range: {min_samples:,} - {max_samples:,}\")\n","                    print(f\"  Average samples: {avg_samples:,.0f}\")\n","\n","                    if min_samples < 1000:\n","                        print(f\"  ‚ö†Ô∏è Some features have very low sample counts\")\n","                    elif max_samples - min_samples > max_samples * 0.1:\n","                        print(f\"  ‚ö†Ô∏è Sample count variation is high\")\n","                    else:\n","                        print(f\"  ‚úÖ Sample counts are consistent\")\n","\n","                # Ê£ÄÊü•Êï∞ÂÄºËåÉÂõ¥\n","                print(f\"\\nüîç Value Range Analysis:\")\n","                for feat_name in ['sp', 'str', 'ssr_clear']:\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","\n","                        if feat_name == 'sp':\n","                            if 90000 < mean < 110000:\n","                                print(f\"  ‚úÖ {feat_name}: Pressure values look normal\")\n","                            else:\n","                                print(f\"  ‚ö†Ô∏è {feat_name}: Pressure values may be unusual\")\n","                        elif feat_name == 'str':\n","                            if mean < 0:\n","                                print(f\"  ‚úÖ {feat_name}: Thermal radiation values look normal (negative)\")\n","                            else:\n","                                print(f\"  ‚ö†Ô∏è {feat_name}: Thermal radiation values may be unusual\")\n","                        elif feat_name == 'ssr_clear':\n","                            if 0 < mean < 10000000:\n","                                print(f\"  ‚úÖ {feat_name}: Solar radiation values look normal\")\n","                            else:\n","                                print(f\"  ‚ö†Ô∏è {feat_name}: Solar radiation values may be unusual\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error loading scaler file: {e}\")\n","\n","if __name__ == \"__main__\":\n","    quick_check_so2_scaler_results()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NRhXOil3px5d","executionInfo":{"status":"ok","timestamp":1758061864726,"user_tz":-120,"elapsed":74,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"81463410-fc4d-4c79-df6f-092dc3c09786"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç Quick SO2 Scaler Results Check\n","==================================================\n","‚úÖ Scaler file exists\n","‚úÖ File loaded successfully\n","üìä Available keys: ['scalers', 'feature_order', 'metadata']\n","üìÖ Years: [2019, 2020, 2021]\n","üìÅ Total files: Unknown\n","üéØ Success rate: Unknown\n","üìä Total samples: Unknown\n","üìä Total features: 30\n","üìä Feature type summary:\n","  Continuous features: 15\n","  Categorical features: 0\n","  Non-standardized features: 15\n","\n","üéØ Key Continuous Features Analysis:\n","  ‚úÖ dem: 761,779 samples, mean=400.9618, std=533.4883\n","  ‚úÖ slope: 761,779 samples, mean=9.2690, std=9.5274\n","  ‚úÖ population: 761,779 samples, mean=145.4318, std=384.2961\n","  ‚úÖ u10: 761,779 samples, mean=-0.4001, std=1.7508\n","  ‚úÖ v10: 761,779 samples, mean=0.2341, std=1.5989\n","  ‚úÖ blh: 761,779 samples, mean=1191.6626, std=466.9831\n","  ‚úÖ tp: 761,779 samples, mean=0.0003, std=0.0010\n","  ‚úÖ t2m: 761,779 samples, mean=20.3405, std=6.8925\n","  ‚úÖ sp: 761,779 samples, mean=96270.7153, std=5620.0365\n","  ‚úÖ str: 761,779 samples, mean=-1467077.4248, std=368888.7481\n","  ‚úÖ ssr_clear: 761,779 samples, mean=8178523.0930, std=1625752.2839\n","  ‚úÖ ws: 761,779 samples, mean=2.0222, std=1.3279\n","  ‚úÖ so2_lag1: 761,779 samples, mean=0.0000, std=0.0001\n","  ‚úÖ so2_neighbor: 761,779 samples, mean=0.0006, std=0.0009\n","  ‚úÖ so2_climate_prior: 761,779 samples, mean=0.0006, std=0.0004\n","\n","üîç Problem Feature Check:\n","  ‚úÖ sp: Normal values (mean=96270.7153, std=5620.0365)\n","  ‚úÖ str: Normal values (mean=-1467077.4248, std=368888.7481)\n","  ‚úÖ ssr_clear: Normal values (mean=8178523.0930, std=1625752.2839)\n","\n","üéØ Success Rate: 100.0%\n","üéâ SO2 Scaler looks excellent!\n","\n","üìä Data Quality Assessment:\n","  Sample count range: 761,779 - 761,779\n","  Average samples: 761,779\n","  ‚úÖ Sample counts are consistent\n","\n","üîç Value Range Analysis:\n","  ‚úÖ sp: Pressure values look normal\n","  ‚úÖ str: Thermal radiation values look normal (negative)\n","  ‚úÖ ssr_clear: Solar radiation values look normal\n"]}]},{"cell_type":"code","source":["# Validate SO2 Scaler Results\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def validate_so2_scaler_results():\n","    \"\"\"È™åËØÅSO2 scalerÁªìÊûú\"\"\"\n","    print(\"üîç SO2 Scaler Results Validation\")\n","    print(\"=\" * 50)\n","\n","    # Ê£ÄÊü•scalerÊñá‰ª∂\n","    scaler_file = \"/content/drive/MyDrive/Scalers/so2_scalers_2019_2021.npz\"\n","\n","    if not os.path.exists(scaler_file):\n","        print(\"‚ùå SO2 scaler file not found\")\n","        return\n","\n","    print(\"‚úÖ Scaler file exists\")\n","\n","    try:\n","        with np.load(scaler_file, allow_pickle=True) as data:\n","            print(\"‚úÖ File loaded successfully\")\n","\n","            # Ê£ÄÊü•Âü∫Êú¨ÁªìÊûÑ\n","            print(f\"üìä Available keys: {list(data.files)}\")\n","\n","            # Ê£ÄÊü•ÂÖÉÊï∞ÊçÆ\n","            if 'metadata' in data.files:\n","                metadata = data['metadata'].item()\n","                print(f\"\\nüìã Metadata:\")\n","                print(f\"  Years: {metadata.get('years', 'Unknown')}\")\n","                print(f\"  Total files: {metadata.get('total_files', 'Unknown')}\")\n","                print(f\"  Success rate: {metadata.get('success_rate', 'Unknown')}\")\n","                print(f\"  Total samples: {metadata.get('total_samples', 'Unknown')}\")\n","                print(f\"  Processing time: {metadata.get('processing_time', 'Unknown')}\")\n","                print(f\"  Mask logic: {metadata.get('mask_logic', 'Unknown')}\")\n","\n","            # Ê£ÄÊü•scalerÂèÇÊï∞\n","            if 'scalers' in data.files:\n","                scalers = data['scalers'].item()\n","                print(f\"\\nüìä Scaler Parameters:\")\n","                print(f\"  Total features: {len(scalers)}\")\n","\n","                # ÂàÜÊûêÁâπÂæÅÁ±ªÂûã\n","                continuous_features = []\n","                categorical_features = []\n","                non_standardized_features = []\n","\n","                for feat_name, feat_info in scalers.items():\n","                    if isinstance(feat_info, dict):\n","                        if 'mean' in feat_info and 'std' in feat_info:\n","                            continuous_features.append(feat_name)\n","                        elif 'categorical' in feat_info:\n","                            categorical_features.append(feat_name)\n","                        else:\n","                            non_standardized_features.append(feat_name)\n","\n","                print(f\"  Continuous features: {len(continuous_features)}\")\n","                print(f\"  Categorical features: {len(categorical_features)}\")\n","                print(f\"  Non-standardized features: {len(non_standardized_features)}\")\n","\n","                # ËØ¶ÁªÜÂàÜÊûêËøûÁª≠ÁâπÂæÅ\n","                print(f\"\\nüîç Continuous Features Analysis:\")\n","                for feat_name in continuous_features:\n","                    if feat_name in scalers:\n","                        feat_info = scalers[feat_name]\n","                        mean = feat_info.get('mean', 0)\n","                        std = feat_info.get('std', 0)\n","                        count = feat_info.get('count', 0)\n","\n","                        print(f\"  {feat_name}:\")\n","                        print(f\"    Mean: {mean:.6f}\")\n","                        print(f\"    Std: {std:.6f}\")\n","                        print(f\"    Count: {count:,}\")\n","\n","                        # Ê£ÄÊü•Êï∞ÊçÆË¥®Èáè\n","                        if count == 0:\n","                            print(f\"    ‚ö†Ô∏è No samples\")\n","                        elif abs(mean) < 1e-6 and abs(std) < 1e-6:\n","                            print(f\"    ‚ö†Ô∏è Near-zero values\")\n","                        elif std == 0:\n","                            print(f\"    ‚ö†Ô∏è Zero standard deviation\")\n","                        else:\n","                            print(f\"    ‚úÖ Normal statistics\")\n","\n","                # Ê£ÄÊü•ÂÖ≥ÈîÆÁâπÂæÅ\n","                print(f\"\\nüéØ Key Features Check:\")\n","                key_features = {\n","                    'dem': 'Elevation (m)',\n","                    'slope': 'Slope (degrees)',\n","                    'population': 'Population density',\n","                    'u10': 'U-wind component (m/s)',\n","                    'v10': 'V-wind component (m/s)',\n","                    'blh': 'Boundary layer height (m)',\n","                    'tp': 'Total precipitation (m)',\n","                    't2m': 'Temperature (K)',\n","                    'sp': 'Surface pressure (Pa)',\n","                    'str': 'Surface thermal radiation (J/m¬≤)',\n","                    'ssr_clear': 'Clear-sky solar radiation (J/m¬≤)',\n","                    'ws': 'Wind speed (m/s)',\n","                    'so2_lag1': 'SO2 lag-1 day',\n","                    'so2_neighbor': 'SO2 neighbor mean',\n","                    'so2_climate_prior': 'SO2 climate prior'\n","                }\n","\n","                for feat_name, description in key_features.items():\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","                        count = scalers[feat_name].get('count', 0)\n","\n","                        print(f\"  ‚úÖ {feat_name} ({description}):\")\n","                        print(f\"    Mean: {mean:.4f}\")\n","                        print(f\"    Std: {std:.4f}\")\n","                        print(f\"    Samples: {count:,}\")\n","\n","                        # Áâ©ÁêÜÂêàÁêÜÊÄßÊ£ÄÊü•\n","                        if feat_name == 'sp' and (mean < 80000 or mean > 110000):\n","                            print(f\"    ‚ö†Ô∏è Pressure values may be unusual\")\n","                        elif feat_name == 't2m' and (mean < 250 or mean > 320):\n","                            print(f\"    ‚ö†Ô∏è Temperature values may be unusual\")\n","                        elif feat_name == 'str' and mean > 0:\n","                            print(f\"    ‚ö†Ô∏è Thermal radiation should be negative\")\n","                        elif feat_name == 'ssr_clear' and (mean < 0 or mean > 20000000):\n","                            print(f\"    ‚ö†Ô∏è Solar radiation values may be unusual\")\n","                    else:\n","                        print(f\"  ‚ùå {feat_name}: Not found\")\n","\n","                # ËÆ°ÁÆóË¥®ÈáèËØÑÂàÜ\n","                print(f\"\\nüìä Quality Assessment:\")\n","\n","                # Ê£ÄÊü•Ê†∑Êú¨Êï∞Èáè‰∏ÄËá¥ÊÄß\n","                sample_counts = [scalers[f].get('count', 0) for f in continuous_features if f in scalers and 'count' in scalers[f]]\n","                if sample_counts:\n","                    min_samples = min(sample_counts)\n","                    max_samples = max(sample_counts)\n","                    avg_samples = sum(sample_counts) / len(sample_counts)\n","\n","                    print(f\"  Sample count analysis:\")\n","                    print(f\"    Min: {min_samples:,}\")\n","                    print(f\"    Max: {max_samples:,}\")\n","                    print(f\"    Average: {avg_samples:,.0f}\")\n","                    print(f\"    Variation: {((max_samples - min_samples) / avg_samples * 100):.1f}%\")\n","\n","                    if (max_samples - min_samples) / avg_samples < 0.05:\n","                        print(f\"    ‚úÖ Sample counts are very consistent\")\n","                    elif (max_samples - min_samples) / avg_samples < 0.1:\n","                        print(f\"    ‚úÖ Sample counts are consistent\")\n","                    else:\n","                        print(f\"    ‚ö†Ô∏è Sample counts vary significantly\")\n","\n","                # Ê£ÄÊü•Êï∞ÂÄºËåÉÂõ¥\n","                print(f\"\\nüîç Value Range Analysis:\")\n","                for feat_name in ['sp', 'str', 'ssr_clear']:\n","                    if feat_name in scalers and 'mean' in scalers[feat_name]:\n","                        mean = scalers[feat_name]['mean']\n","                        std = scalers[feat_name]['std']\n","\n","                        if feat_name == 'sp':\n","                            if 90000 < mean < 110000:\n","                                print(f\"  ‚úÖ {feat_name}: Pressure values are in normal range\")\n","                            else:\n","                                print(f\"  ‚ö†Ô∏è {feat_name}: Pressure values may be unusual\")\n","                        elif feat_name == 'str':\n","                            if mean < 0:\n","                                print(f\"  ‚úÖ {feat_name}: Thermal radiation values are normal (negative)\")\n","                            else:\n","                                print(f\"  ‚ö†Ô∏è {feat_name}: Thermal radiation values may be unusual\")\n","                        elif feat_name == 'ssr_clear':\n","                            if 0 < mean < 20000000:\n","                                print(f\"  ‚úÖ {feat_name}: Solar radiation values are in normal range\")\n","                            else:\n","                                print(f\"  ‚ö†Ô∏è {feat_name}: Solar radiation values may be unusual\")\n","\n","                # ÊÄª‰ΩìË¥®ÈáèËØÑÂàÜ\n","                quality_score = calculate_quality_score(scalers, continuous_features)\n","                print(f\"\\nüéØ Overall Quality Score: {quality_score:.1f}/100\")\n","\n","                if quality_score >= 90:\n","                    print(\"üéâ Excellent quality! Ready for training.\")\n","                elif quality_score >= 80:\n","                    print(\"‚úÖ Good quality! Ready for training.\")\n","                elif quality_score >= 70:\n","                    print(\"‚ö†Ô∏è Acceptable quality. Consider reviewing some features.\")\n","                else:\n","                    print(\"‚ùå Poor quality. Significant issues need to be addressed.\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error loading scaler file: {e}\")\n","\n","def calculate_quality_score(scalers, continuous_features):\n","    \"\"\"ËÆ°ÁÆóË¥®ÈáèËØÑÂàÜ\"\"\"\n","    score = 100.0\n","\n","    # Ê£ÄÊü•ËøûÁª≠ÁâπÂæÅ\n","    for feat_name in continuous_features:\n","        if feat_name in scalers:\n","            feat_info = scalers[feat_name]\n","            count = feat_info.get('count', 0)\n","            mean = feat_info.get('mean', 0)\n","            std = feat_info.get('std', 0)\n","\n","            # Ê†∑Êú¨Êï∞ÈáèÊ£ÄÊü•\n","            if count == 0:\n","                score -= 10\n","            elif count < 1000:\n","                score -= 5\n","\n","            # Êï∞ÂÄºÂêàÁêÜÊÄßÊ£ÄÊü•\n","            if abs(mean) < 1e-6 and abs(std) < 1e-6:\n","                score -= 15\n","            elif std == 0:\n","                score -= 10\n","\n","            # Áâ©ÁêÜÂêàÁêÜÊÄßÊ£ÄÊü•\n","            if feat_name == 'sp' and (mean < 80000 or mean > 110000):\n","                score -= 5\n","            elif feat_name == 't2m' and (mean < 250 or mean > 320):\n","                score -= 5\n","            elif feat_name == 'str' and mean > 0:\n","                score -= 5\n","            elif feat_name == 'ssr_clear' and (mean < 0 or mean > 20000000):\n","                score -= 5\n","\n","    return max(0, score)\n","\n","if __name__ == \"__main__\":\n","    validate_so2_scaler_results()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAtUl5mgp2ZD","executionInfo":{"status":"ok","timestamp":1758061883016,"user_tz":-120,"elapsed":56,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"45336405-2ba5-4086-bd5b-81be9ecc0523"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç SO2 Scaler Results Validation\n","==================================================\n","‚úÖ Scaler file exists\n","‚úÖ File loaded successfully\n","üìä Available keys: ['scalers', 'feature_order', 'metadata']\n","\n","üìã Metadata:\n","  Years: [2019, 2020, 2021]\n","  Total files: Unknown\n","  Success rate: Unknown\n","  Total samples: Unknown\n","  Processing time: Unknown\n","  Mask logic: mask==1 is valid, mask==0 is invalid (CONFIRMED FROM SOURCE CODE)\n","\n","üìä Scaler Parameters:\n","  Total features: 30\n","  Continuous features: 15\n","  Categorical features: 0\n","  Non-standardized features: 15\n","\n","üîç Continuous Features Analysis:\n","  dem:\n","    Mean: 400.961806\n","    Std: 533.488280\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  slope:\n","    Mean: 9.268992\n","    Std: 9.527353\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  population:\n","    Mean: 145.431770\n","    Std: 384.296097\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  u10:\n","    Mean: -0.400131\n","    Std: 1.750753\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  v10:\n","    Mean: 0.234117\n","    Std: 1.598915\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  ws:\n","    Mean: 2.022176\n","    Std: 1.327896\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  blh:\n","    Mean: 1191.662584\n","    Std: 466.983097\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  tp:\n","    Mean: 0.000326\n","    Std: 0.000961\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  t2m:\n","    Mean: 20.340453\n","    Std: 6.892523\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  sp:\n","    Mean: 96270.715305\n","    Std: 5620.036544\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  str:\n","    Mean: -1467077.424804\n","    Std: 368888.748057\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  ssr_clear:\n","    Mean: 8178523.092988\n","    Std: 1625752.283885\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  so2_lag1:\n","    Mean: 0.000001\n","    Std: 0.000100\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  so2_neighbor:\n","    Mean: 0.000626\n","    Std: 0.000903\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","  so2_climate_prior:\n","    Mean: 0.000623\n","    Std: 0.000367\n","    Count: 761,779\n","    ‚úÖ Normal statistics\n","\n","üéØ Key Features Check:\n","  ‚úÖ dem (Elevation (m)):\n","    Mean: 400.9618\n","    Std: 533.4883\n","    Samples: 761,779\n","  ‚úÖ slope (Slope (degrees)):\n","    Mean: 9.2690\n","    Std: 9.5274\n","    Samples: 761,779\n","  ‚úÖ population (Population density):\n","    Mean: 145.4318\n","    Std: 384.2961\n","    Samples: 761,779\n","  ‚úÖ u10 (U-wind component (m/s)):\n","    Mean: -0.4001\n","    Std: 1.7508\n","    Samples: 761,779\n","  ‚úÖ v10 (V-wind component (m/s)):\n","    Mean: 0.2341\n","    Std: 1.5989\n","    Samples: 761,779\n","  ‚úÖ blh (Boundary layer height (m)):\n","    Mean: 1191.6626\n","    Std: 466.9831\n","    Samples: 761,779\n","  ‚úÖ tp (Total precipitation (m)):\n","    Mean: 0.0003\n","    Std: 0.0010\n","    Samples: 761,779\n","  ‚úÖ t2m (Temperature (K)):\n","    Mean: 20.3405\n","    Std: 6.8925\n","    Samples: 761,779\n","    ‚ö†Ô∏è Temperature values may be unusual\n","  ‚úÖ sp (Surface pressure (Pa)):\n","    Mean: 96270.7153\n","    Std: 5620.0365\n","    Samples: 761,779\n","  ‚úÖ str (Surface thermal radiation (J/m¬≤)):\n","    Mean: -1467077.4248\n","    Std: 368888.7481\n","    Samples: 761,779\n","  ‚úÖ ssr_clear (Clear-sky solar radiation (J/m¬≤)):\n","    Mean: 8178523.0930\n","    Std: 1625752.2839\n","    Samples: 761,779\n","  ‚úÖ ws (Wind speed (m/s)):\n","    Mean: 2.0222\n","    Std: 1.3279\n","    Samples: 761,779\n","  ‚úÖ so2_lag1 (SO2 lag-1 day):\n","    Mean: 0.0000\n","    Std: 0.0001\n","    Samples: 761,779\n","  ‚úÖ so2_neighbor (SO2 neighbor mean):\n","    Mean: 0.0006\n","    Std: 0.0009\n","    Samples: 761,779\n","  ‚úÖ so2_climate_prior (SO2 climate prior):\n","    Mean: 0.0006\n","    Std: 0.0004\n","    Samples: 761,779\n","\n","üìä Quality Assessment:\n","  Sample count analysis:\n","    Min: 761,779\n","    Max: 761,779\n","    Average: 761,779\n","    Variation: 0.0%\n","    ‚úÖ Sample counts are very consistent\n","\n","üîç Value Range Analysis:\n","  ‚úÖ sp: Pressure values are in normal range\n","  ‚úÖ str: Thermal radiation values are normal (negative)\n","  ‚úÖ ssr_clear: Solar radiation values are in normal range\n","\n","üéØ Overall Quality Score: 95.0/100\n","üéâ Excellent quality! Ready for training.\n"]}]},{"cell_type":"code","source":["# Ê£ÄÊü•ÂÆûÈôÖÊñá‰ª∂Áä∂ÊÄÅ\n","import os\n","\n","def check_manifest_status():\n","    \"\"\"Ê£ÄÊü•SO2ÂíåNO2ÁöÑmanifestÊñá‰ª∂Áä∂ÊÄÅ\"\"\"\n","    print(\"üîç Ê£ÄÊü•manifestÊñá‰ª∂Áä∂ÊÄÅ...\")\n","\n","    # Ê£ÄÊü•SO2\n","    so2_manifest_paths = [\n","        \"/content/drive/MyDrive/Feature_Stacks/so2_feature_stacks/so2_feature_stacks_manifest.csv\",\n","        \"/content/drive/MyDrive/so2_train_20250911_080237.txt\",\n","        \"/content/drive/MyDrive/so2_val_20250911_080237.txt\",\n","        \"/content/drive/MyDrive/so2_test_20250911_080237.txt\"\n","    ]\n","\n","    print(\"\\nüìä SO2Êñá‰ª∂Áä∂ÊÄÅ:\")\n","    for path in so2_manifest_paths:\n","        exists = os.path.exists(path)\n","        print(f\"  {path}: {'‚úÖ' if exists else '‚ùå'}\")\n","\n","    # Ê£ÄÊü•NO2\n","    no2_manifest_paths = [\n","        \"/content/drive/MyDrive/Feature_Stacks/no2_feature_stacks/no2_feature_stacks_manifest.csv\",\n","        \"/content/drive/MyDrive/no2_train_*.txt\",\n","        \"/content/drive/MyDrive/no2_val_*.txt\",\n","        \"/content/drive/MyDrive/no2_test_*.txt\"\n","    ]\n","\n","    print(\"\\n NO2Êñá‰ª∂Áä∂ÊÄÅ:\")\n","    for path in no2_manifest_paths:\n","        if \"*\" in path:\n","            # Ê£ÄÊü•ÈÄöÈÖçÁ¨¶Êñá‰ª∂\n","            import glob\n","            files = glob.glob(path)\n","            print(f\"  {path}: {'‚úÖ' if files else '‚ùå'} ({len(files)} files)\")\n","        else:\n","            exists = os.path.exists(path)\n","            print(f\"  {path}: {'‚úÖ' if exists else '‚ùå'}\")\n","\n","# ËøêË°åÊ£ÄÊü•\n","check_manifest_status()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IzWSQxZDwxAJ","executionInfo":{"status":"ok","timestamp":1757664118634,"user_tz":-120,"elapsed":387,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"fc16b767-1b80-438d-ce23-4afbf45b80f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç Ê£ÄÊü•manifestÊñá‰ª∂Áä∂ÊÄÅ...\n","\n","üìä SO2Êñá‰ª∂Áä∂ÊÄÅ:\n","  /content/drive/MyDrive/Feature_Stacks/so2_feature_stacks/so2_feature_stacks_manifest.csv: ‚ùå\n","  /content/drive/MyDrive/so2_train_20250911_080237.txt: ‚úÖ\n","  /content/drive/MyDrive/so2_val_20250911_080237.txt: ‚úÖ\n","  /content/drive/MyDrive/so2_test_20250911_080237.txt: ‚úÖ\n","\n"," NO2Êñá‰ª∂Áä∂ÊÄÅ:\n","  /content/drive/MyDrive/Feature_Stacks/no2_feature_stacks/no2_feature_stacks_manifest.csv: ‚ùå\n","  /content/drive/MyDrive/no2_train_*.txt: ‚ùå (0 files)\n","  /content/drive/MyDrive/no2_val_*.txt: ‚ùå (0 files)\n","  /content/drive/MyDrive/no2_test_*.txt: ‚ùå (0 files)\n"]}]},{"cell_type":"code","source":["# ‰øÆÊ≠£ÁâàÔºöÁîüÊàêSO2ÂíåNO2ÁöÑmanifestÊñá‰ª∂ÔºàÁ¨¶ÂêàÊúÄ‰Ω≥ÂÆûË∑µÔºâ\n","import os\n","import pandas as pd\n","import numpy as np\n","import re\n","from datetime import datetime\n","import glob\n","\n","BASE_SO2 = \"/content/drive/MyDrive/Variables/so2_feature_stacks\"\n","BASE_NO2 = \"/content/drive/MyDrive/Variables/no2_feature_stacks\"\n","OUT_DIR = \"/content/drive/MyDrive\"\n","\n","def create_manifest_improved(data_type, base_path, output_dir):\n","    \"\"\"ÂàõÂª∫manifestÊñá‰ª∂ÔºàÊîπËøõÁâàÔºâ\"\"\"\n","    print(f\"\\nüìã ÂàõÂª∫{data_type} manifestÊñá‰ª∂ÔºàÊîπËøõÁâàÔºâ...\")\n","\n","    manifest_data = []\n","    coverage_issues = []\n","\n","    for year in [2019, 2020, 2021, 2022, 2023]:\n","        year_dir = os.path.join(base_path, str(year))\n","        if not os.path.exists(year_dir):\n","            print(f\"‚ö†Ô∏è Âπ¥‰ªΩÁõÆÂΩï‰∏çÂ≠òÂú®: {year_dir}\")\n","            continue\n","\n","        files = [f for f in os.listdir(year_dir) if f.endswith('.npz')]\n","        files.sort()\n","\n","        print(f\"  {year}: {len(files)} ‰∏™Êñá‰ª∂\")\n","\n","        for file in files:\n","            try:\n","                # ÊîπËøõ1: Êõ¥robustÁöÑÊó•ÊúüËß£Êûê\n","                m = re.search(r'(\\d{8})', file)\n","                if not m:\n","                    print(f\"‚ö†Ô∏è Êó†Ê≥ïËß£ÊûêÊó•Êúü: {file}\")\n","                    continue\n","\n","                date8 = m.group(1)\n","                date_formatted = f\"{date8[:4]}-{date8[4:6]}-{date8[6:8]}\"\n","\n","                file_path = os.path.join(year_dir, file)\n","\n","                # ÊîπËøõ3: Ë¶ÜÁõñÁéá‰∏ÄËá¥ÊÄßÊ†°È™å\n","                try:\n","                    with np.load(file_path, allow_pickle=True) as d:\n","                        # Âü∫‰∫émaskÈáçÊñ∞ËÆ°ÁÆóË¶ÜÁõñÁéá\n","                        if 'mask' in d.files:\n","                            cov_chk = float((d['mask']==0).sum())/d['mask'].size\n","                        else:\n","                            cov_chk = np.nan\n","\n","                        # ‰ΩøÁî®Êñá‰ª∂‰∏≠ÁöÑcoverageÔºåÂ¶ÇÊûúÊ≤°ÊúâÂàô‰ΩøÁî®ËÆ°ÁÆóÂÄº\n","                        coverage = float(d['coverage']) if 'coverage' in d.files else cov_chk\n","\n","                        # ‰∏ÄËá¥ÊÄßÊ£ÄÊü•\n","                        if 'coverage' in d.files and not np.isnan(cov_chk):\n","                            diff = abs(coverage - cov_chk)\n","                            if diff > 0.01:  # Â∑ÆÂºÇË∂ÖËøá1%\n","                                coverage_issues.append({\n","                                    'file': file,\n","                                    'stored_coverage': coverage,\n","                                    'calculated_coverage': cov_chk,\n","                                    'difference': diff\n","                                })\n","\n","                        trainable = bool(d['trainable']) if 'trainable' in d.files else False\n","                        season = d['season'].item() if 'season' in d.files else 'unknown'\n","\n","                except Exception as e:\n","                    print(f\"‚ö†Ô∏è Âä†ËΩΩÊñá‰ª∂Â§±Ë¥• {file}: {e}\")\n","                    continue\n","\n","                manifest_data.append({\n","                    'date': date_formatted,\n","                    'path': file_path,\n","                    'coverage': coverage,\n","                    'trainable': trainable,\n","                    'season': season,\n","                    'year': int(date8[:4]),\n","                    'month': int(date8[4:6]),\n","                    'day': int(date8[6:8])\n","                })\n","\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è Â§ÑÁêÜÊñá‰ª∂Â§±Ë¥• {file}: {e}\")\n","                continue\n","\n","    # ÂàõÂª∫DataFrameÂπ∂‰øùÂ≠ò\n","    manifest_df = pd.DataFrame(manifest_data)\n","    manifest_file = os.path.join(output_dir, f'{data_type.lower()}_manifest.csv')\n","    manifest_df.to_csv(manifest_file, index=False)\n","\n","    print(f\"‚úÖ {data_type} manifest‰øùÂ≠ò: {manifest_file}\")\n","    print(f\"   ÊÄªÊñá‰ª∂Êï∞: {len(manifest_df)}\")\n","    print(f\"   Êó•ÊúüËåÉÂõ¥: {manifest_df['date'].min()} Âà∞ {manifest_df['date'].max()}\")\n","    print(f\"   ÂèØËÆ≠ÁªÉÊñá‰ª∂: {manifest_df['trainable'].sum()}\")\n","\n","    # Êä•ÂëäË¶ÜÁõñÁéáÈóÆÈ¢ò\n","    if coverage_issues:\n","        print(f\"‚ö†Ô∏è ÂèëÁé∞ {len(coverage_issues)} ‰∏™Ë¶ÜÁõñÁéá‰∏ç‰∏ÄËá¥Êñá‰ª∂\")\n","        for issue in coverage_issues[:5]:  # ÊòæÁ§∫Ââç5‰∏™\n","            print(f\"   {issue['file']}: Â≠òÂÇ®={issue['stored_coverage']:.3f}, ËÆ°ÁÆó={issue['calculated_coverage']:.3f}, Â∑ÆÂºÇ={issue['difference']:.3f}\")\n","\n","    return manifest_df\n","\n","def create_train_val_test_splits_improved(manifest_df, data_type, output_dir):\n","    \"\"\"ÂàõÂª∫train/val/testÂàíÂàÜÔºàÊîπËøõÁâàÔºâ\"\"\"\n","    print(f\"\\n ÂàõÂª∫{data_type} train/val/testÂàíÂàÜÔºàÊîπËøõÁâàÔºâ...\")\n","\n","    # ËøáÊª§ÂèØËÆ≠ÁªÉÊñá‰ª∂\n","    trainable_df = manifest_df[manifest_df['trainable'] == True].copy()\n","    print(f\"   ÂèØËÆ≠ÁªÉÊñá‰ª∂: {len(trainable_df)}\")\n","\n","    if len(trainable_df) == 0:\n","        print(f\"‚ùå Ê≤°ÊúâÂèØËÆ≠ÁªÉÊñá‰ª∂\")\n","        return\n","\n","    # ÊåâÂπ¥‰ªΩÂàíÂàÜ\n","    train_files = trainable_df[trainable_df['year'].isin([2019, 2020, 2021])]\n","    val_files = trainable_df[trainable_df['year'] == 2022]\n","    test_files = trainable_df[trainable_df['year'] == 2023]\n","\n","    print(f\"   ËÆ≠ÁªÉÈõÜ: {len(train_files)} Êñá‰ª∂\")\n","    print(f\"   È™åËØÅÈõÜ: {len(val_files)} Êñá‰ª∂\")\n","    print(f\"   ÊµãËØïÈõÜ: {len(test_files)} Êñá‰ª∂\")\n","\n","    # ‰øùÂ≠òÂàíÂàÜÊñá‰ª∂\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","    for split_name, split_df in [('train', train_files), ('val', val_files), ('test', test_files)]:\n","        if len(split_df) > 0:\n","            # ÊîπËøõ2: ÂêåÊó∂‰øùÂ≠òTXTÂíåCSVÊñá‰ª∂\n","            # TXTÊñá‰ª∂Ôºà‰ªÖÊó•ÊúüÔºåÂÖºÂÆπÁé∞Êúâ‰ª£Á†ÅÔºâ\n","            split_txt = os.path.join(output_dir, f'{data_type.lower()}_{split_name}_{timestamp}.txt')\n","            with open(split_txt, 'w') as f:\n","                for _, row in split_df.iterrows():\n","                    f.write(f\"{row['date']}\\n\")\n","\n","            # CSVÊñá‰ª∂ÔºàÂåÖÂê´ÂÆåÊï¥ÂÖÉÊï∞ÊçÆÔºâ\n","            split_csv = os.path.join(output_dir, f'{data_type.lower()}_{split_name}_{timestamp}.csv')\n","            split_df[['date', 'path', 'coverage', 'season']].to_csv(split_csv, index=False)\n","\n","            print(f\"   ‚úÖ {split_name}: {split_txt} + {split_csv}\")\n","\n","    return train_files, val_files, test_files\n","\n","def main():\n","    \"\"\"‰∏ªÂáΩÊï∞\"\"\"\n","    print(\" ÁîüÊàêSO2ÂíåNO2ÁöÑmanifestÊñá‰ª∂‰ª•Âèätrain/val/testÂàíÂàÜÔºàÊîπËøõÁâàÔºâ...\")\n","\n","    # ÂàõÂª∫SO2 manifestÂíåÂàíÂàÜ\n","    if os.path.exists(BASE_SO2):\n","        so2_manifest = create_manifest_improved(\"SO2\", BASE_SO2, OUT_DIR)\n","        create_train_val_test_splits_improved(so2_manifest, \"SO2\", OUT_DIR)\n","    else:\n","        print(f\"‚ùå SO2Êï∞ÊçÆÁõÆÂΩï‰∏çÂ≠òÂú®: {BASE_SO2}\")\n","\n","    # ÂàõÂª∫NO2 manifestÂíåÂàíÂàÜ\n","    if os.path.exists(BASE_NO2):\n","        no2_manifest = create_manifest_improved(\"NO2\", BASE_NO2, OUT_DIR)\n","        create_train_val_test_splits_improved(no2_manifest, \"NO2\", OUT_DIR)\n","    else:\n","        print(f\"‚ùå NO2Êï∞ÊçÆÁõÆÂΩï‰∏çÂ≠òÂú®: {BASE_NO2}\")\n","\n","    print(\"\\n‚úÖ ÊâÄÊúâmanifestÂíåÂàíÂàÜÊñá‰ª∂ÁîüÊàêÂÆåÊàêÔºÅ\")\n","\n","# ËøêË°å‰∏ªÂáΩÊï∞\n","main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5e1LGRVO8yDJ","executionInfo":{"status":"ok","timestamp":1757667587244,"user_tz":-120,"elapsed":3258810,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"fcc0ef08-2d33-4f62-a96c-b3f0a9b98565"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" ÁîüÊàêSO2ÂíåNO2ÁöÑmanifestÊñá‰ª∂‰ª•Âèätrain/val/testÂàíÂàÜÔºàÊîπËøõÁâàÔºâ...\n","\n","üìã ÂàõÂª∫SO2 manifestÊñá‰ª∂ÔºàÊîπËøõÁâàÔºâ...\n","  2019: 365 ‰∏™Êñá‰ª∂\n","  2020: 366 ‰∏™Êñá‰ª∂\n","  2021: 365 ‰∏™Êñá‰ª∂\n","  2022: 365 ‰∏™Êñá‰ª∂\n","  2023: 365 ‰∏™Êñá‰ª∂\n","‚úÖ SO2 manifest‰øùÂ≠ò: /content/drive/MyDrive/so2_manifest.csv\n","   ÊÄªÊñá‰ª∂Êï∞: 1826\n","   Êó•ÊúüËåÉÂõ¥: 2019-01-01 Âà∞ 2023-12-31\n","   ÂèØËÆ≠ÁªÉÊñá‰ª∂: 852\n","‚ö†Ô∏è ÂèëÁé∞ 1024 ‰∏™Ë¶ÜÁõñÁéá‰∏ç‰∏ÄËá¥Êñá‰ª∂\n","   so2_features_20190206.npz: Â≠òÂÇ®=0.012, ËÆ°ÁÆó=0.023, Â∑ÆÂºÇ=0.011\n","   so2_features_20190207.npz: Â≠òÂÇ®=0.015, ËÆ°ÁÆó=0.046, Â∑ÆÂºÇ=0.031\n","   so2_features_20190211.npz: Â≠òÂÇ®=0.050, ËÆ°ÁÆó=0.097, Â∑ÆÂºÇ=0.047\n","   so2_features_20190212.npz: Â≠òÂÇ®=0.076, ËÆ°ÁÆó=0.239, Â∑ÆÂºÇ=0.164\n","   so2_features_20190213.npz: Â≠òÂÇ®=0.048, ËÆ°ÁÆó=0.169, Â∑ÆÂºÇ=0.121\n","\n"," ÂàõÂª∫SO2 train/val/testÂàíÂàÜÔºàÊîπËøõÁâàÔºâ...\n","   ÂèØËÆ≠ÁªÉÊñá‰ª∂: 852\n","   ËÆ≠ÁªÉÈõÜ: 507 Êñá‰ª∂\n","   È™åËØÅÈõÜ: 175 Êñá‰ª∂\n","   ÊµãËØïÈõÜ: 170 Êñá‰ª∂\n","   ‚úÖ train: /content/drive/MyDrive/so2_train_20250912_083216.txt + /content/drive/MyDrive/so2_train_20250912_083216.csv\n","   ‚úÖ val: /content/drive/MyDrive/so2_val_20250912_083216.txt + /content/drive/MyDrive/so2_val_20250912_083216.csv\n","   ‚úÖ test: /content/drive/MyDrive/so2_test_20250912_083216.txt + /content/drive/MyDrive/so2_test_20250912_083216.csv\n","\n","üìã ÂàõÂª∫NO2 manifestÊñá‰ª∂ÔºàÊîπËøõÁâàÔºâ...\n","  2019: 365 ‰∏™Êñá‰ª∂\n","  2020: 366 ‰∏™Êñá‰ª∂\n","  2021: 365 ‰∏™Êñá‰ª∂\n","  2022: 365 ‰∏™Êñá‰ª∂\n","  2023: 365 ‰∏™Êñá‰ª∂\n","‚úÖ NO2 manifest‰øùÂ≠ò: /content/drive/MyDrive/no2_manifest.csv\n","   ÊÄªÊñá‰ª∂Êï∞: 1826\n","   Êó•ÊúüËåÉÂõ¥: 2019-01-01 Âà∞ 2023-12-31\n","   ÂèØËÆ≠ÁªÉÊñá‰ª∂: 570\n","\n"," ÂàõÂª∫NO2 train/val/testÂàíÂàÜÔºàÊîπËøõÁâàÔºâ...\n","   ÂèØËÆ≠ÁªÉÊñá‰ª∂: 570\n","   ËÆ≠ÁªÉÈõÜ: 302 Êñá‰ª∂\n","   È™åËØÅÈõÜ: 146 Êñá‰ª∂\n","   ÊµãËØïÈõÜ: 122 Êñá‰ª∂\n","   ‚úÖ train: /content/drive/MyDrive/no2_train_20250912_085947.txt + /content/drive/MyDrive/no2_train_20250912_085947.csv\n","   ‚úÖ val: /content/drive/MyDrive/no2_val_20250912_085947.txt + /content/drive/MyDrive/no2_val_20250912_085947.csv\n","   ‚úÖ test: /content/drive/MyDrive/no2_test_20250912_085947.txt + /content/drive/MyDrive/no2_test_20250912_085947.csv\n","\n","‚úÖ ÊâÄÊúâmanifestÂíåÂàíÂàÜÊñá‰ª∂ÁîüÊàêÂÆåÊàêÔºÅ\n"]}]},{"cell_type":"code","source":["# Unified manifest + split builder (with per-dataset thresholds and QC guards)\n","import os, re, numpy as np, pandas as pd, random\n","from datetime import datetime\n","\n","# ----------------------------- Config -----------------------------\n","BASE_NO2 = \"/content/drive/MyDrive/Variables/no2_feature_stacks\"\n","BASE_SO2 = \"/content/drive/MyDrive/Variables/so2_feature_stacks\"\n","OUT_DIR   = \"/content/drive/MyDrive/Variables\"\n","YEARS     = [2019, 2020, 2021, 2022, 2023]\n","\n","# Year-based splits\n","TRAIN_YRS, VAL_YRS, TEST_YRS = [2019, 2020, 2021], [2022], [2023]\n","\n","# Per-dataset default trainable thresholds (overridable by file field 'trainable_threshold')\n","DEFAULT_THR = {\"NO2\": 0.40, \"SO2\": 0.10}\n","\n","# QC guards only when coverage_source == 'mask'\n","MIN_YPOS_FRAC = 0.01     # require at least 1% positive y pixels globally (very loose)\n","MAX_COV_DIFF  = 0.05     # |mean(mask==0) - mean(y>0 & finite)| should be < 0.05\n","\n","# Repro\n","random.seed(42); np.random.seed(42)\n","\n","# ----------------------------- Helpers -----------------------------\n","def parse_date_from_filename(fname: str) -> str | None:\n","    m = re.search(r'(\\d{8})', fname)\n","    if not m:\n","        return None\n","    d = m.group(1)\n","    return f\"{d[:4]}-{d[4:6]}-{d[6:8]}\"\n","\n","def compute_coverage_unified(fp: str, data_type: str):\n","    \"\"\"\n","    Returns:\n","      coverage: float\n","      trainable: bool (after applying threshold and QC if needed)\n","      threshold_used: float\n","      season: str\n","      coverage_source: str in {'domain+data','data_coverage','mask','none','error'}\n","      y_positive_frac: float (only meaningful when source=='mask', else np.nan)\n","      qc_pass: bool\n","    \"\"\"\n","    try:\n","        with np.load(fp, allow_pickle=True) as d:\n","            # 1) coverage compute with unified precedence\n","            if {'data_mask','domain_mask'}.issubset(d.files):\n","                source = 'domain+data'\n","                dom = (d['domain_mask'] == 1)\n","                dat = (d['data_mask'] == 1)\n","                denom = max(dom.sum(), 1)\n","                cov = float((dat & dom).sum() / denom)\n","            elif 'data_coverage' in d.files:\n","                source = 'data_coverage'\n","                cov = float(d['data_coverage'])\n","            elif 'mask' in d.files:\n","                source = 'mask'\n","                cov = float((d['mask']==0).sum() / d['mask'].size)\n","            else:\n","                source = 'none'\n","                cov = 0.0\n","\n","            # 2) threshold\n","            thr = float(d['trainable_threshold']) if 'trainable_threshold' in d.files else DEFAULT_THR[data_type]\n","            season = d['season'].item() if 'season' in d.files else 'unknown'\n","\n","            # 3) QC guard if we fell back to 'mask' (no AOI info)\n","            qc_pass = True\n","            y_pos_frac = np.nan\n","            if source == 'mask':\n","                try:\n","                    y = d['y']\n","                    mask = d['mask']\n","                    pos = np.isfinite(y) & (y > 0)\n","                    y_pos_frac = float(pos.sum() / y.size)\n","                    cov_mask = float((mask==0).sum() / mask.size)\n","                    qc_pass = (y_pos_frac >= MIN_YPOS_FRAC) and (abs(cov_mask - y_pos_frac) < MAX_COV_DIFF)\n","                except Exception:\n","                    qc_pass = False\n","\n","            trainable = bool((cov >= thr) and qc_pass)\n","            return cov, trainable, thr, season, source, y_pos_frac, qc_pass\n","\n","    except Exception:\n","        return 0.0, False, DEFAULT_THR[data_type], 'unknown', 'error', np.nan, False\n","\n","def quick_summary(df: pd.DataFrame, name: str):\n","    if df.empty:\n","        print(f\"\\n[{name}] files=0\")\n","        return\n","    print(f\"\\n[{name}] files={len(df)}  cov_mean={df['coverage'].mean():.3f}  cov_med={df['coverage'].median():.3f}\")\n","    print(\" season counts:\\n\", df['season'].value_counts())\n","    print(\" coverage_source breakdown:\\n\", df['coverage_source'].value_counts())\n","\n","def build_manifest(data_type: str, base_dir: str) -> pd.DataFrame:\n","    print(f\"\\nüìã Building manifest for {data_type} ...\")\n","    rows = []\n","    for y in YEARS:\n","        ydir = os.path.join(base_dir, str(y))\n","        if not os.path.isdir(ydir):\n","            continue\n","        files = sorted([f for f in os.listdir(ydir) if f.endswith(\".npz\")])\n","        print(f\"  {y}: {len(files)} files\")\n","        for f in files:\n","            date = parse_date_from_filename(f)\n","            if not date:\n","                continue\n","            fp = os.path.join(ydir, f)\n","            cov, trn, thr, season, source, ypf, qc = compute_coverage_unified(fp, data_type)\n","            rows.append({\n","                'date': date,\n","                'path': fp,\n","                'coverage': cov,\n","                'trainable': trn,\n","                'threshold': thr,\n","                'season': season,\n","                'coverage_source': source,\n","                'y_positive_frac': ypf,\n","                'qc_pass': qc,\n","                'year': int(date[:4]),\n","                'month': int(date[5:7]),\n","                'day': int(date[8:10]),\n","            })\n","    df = pd.DataFrame(rows).sort_values('date').reset_index(drop=True)\n","    out_csv = os.path.join(OUT_DIR, f\"{data_type.lower()}_manifest.csv\")\n","    df.to_csv(out_csv, index=False)\n","    print(f\"‚úÖ {data_type} manifest saved: {out_csv}\")\n","    print(f\"   total={len(df)}, trainable={int(df['trainable'].sum())}, date_range={df['date'].min()}..{df['date'].max()}\")\n","    quick_summary(df, f\"{data_type} manifest\")\n","    return df\n","\n","def save_splits(df: pd.DataFrame, data_type: str):\n","    splits = {\n","        'train': df[(df['trainable'] == True) & (df['year'].isin(TRAIN_YRS))],\n","        'val':   df[(df['trainable'] == True) & (df['year'].isin(VAL_YRS))],\n","        'test':  df[(df['trainable'] == True) & (df['year'].isin(TEST_YRS))],\n","    }\n","    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    for name, sdf in splits.items():\n","        quick_summary(sdf, f\"{data_type} {name}\")\n","        if sdf.empty:\n","            continue\n","        txt = os.path.join(OUT_DIR, f\"{data_type.lower()}_{name}_{ts}.txt\")\n","        csv = os.path.join(OUT_DIR, f\"{data_type.lower()}_{name}_{ts}.csv\")\n","        with open(txt, \"w\") as f:\n","            for d in sdf['date'].tolist():\n","                f.write(f\"{d}\\n\")\n","        sdf[['date','path','coverage','season','coverage_source','threshold','qc_pass']].to_csv(csv, index=False)\n","        print(f\"   ‚úÖ {name}: {len(sdf)} -> {txt} + {csv}\")\n","\n","# ----------------------------- Run -----------------------------\n","print(\"üöÄ Building unified manifests and splits with QC guards...\")\n","\n","so2_df = build_manifest(\"SO2\", BASE_SO2)\n","save_splits(so2_df, \"SO2\")\n","\n","no2_df = build_manifest(\"NO2\", BASE_NO2)\n","save_splits(no2_df, \"NO2\")\n","\n","print(\"\\nüéâ Done.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjy86FfNKYUk","executionInfo":{"status":"ok","timestamp":1757671059341,"user_tz":-120,"elapsed":3260135,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"49929340-29d8-445b-d2e4-2cefecc4ea52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Building unified manifests and splits with QC guards...\n","\n","üìã Building manifest for SO2 ...\n","  2019: 365 files\n","  2020: 366 files\n","  2021: 365 files\n","  2022: 365 files\n","  2023: 365 files\n","‚úÖ SO2 manifest saved: /content/drive/MyDrive/Variables/so2_manifest.csv\n","   total=1826, trainable=852, date_range=2019-01-01..2023-12-31\n","\n","[SO2 manifest] files=1826  cov_mean=0.107  cov_med=0.086\n"," season counts:\n"," season\n","spring    460\n","summer    460\n","autumn    455\n","winter    451\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","domain+data    1826\n","Name: count, dtype: int64\n","\n","[SO2 train] files=507  cov_mean=0.199  cov_med=0.188\n"," season counts:\n"," season\n","summer    224\n","spring    168\n","autumn     93\n","winter     22\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","domain+data    507\n","Name: count, dtype: int64\n","   ‚úÖ train: 507 -> /content/drive/MyDrive/Variables/so2_train_20250912_093025.txt + /content/drive/MyDrive/Variables/so2_train_20250912_093025.csv\n","\n","[SO2 val] files=175  cov_mean=0.219  cov_med=0.219\n"," season counts:\n"," season\n","summer    81\n","spring    49\n","autumn    38\n","winter     7\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","domain+data    175\n","Name: count, dtype: int64\n","   ‚úÖ val: 175 -> /content/drive/MyDrive/Variables/so2_val_20250912_093025.txt + /content/drive/MyDrive/Variables/so2_val_20250912_093025.csv\n","\n","[SO2 test] files=170  cov_mean=0.206  cov_med=0.180\n"," season counts:\n"," season\n","summer    71\n","spring    56\n","autumn    38\n","winter     5\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","domain+data    170\n","Name: count, dtype: int64\n","   ‚úÖ test: 170 -> /content/drive/MyDrive/Variables/so2_test_20250912_093025.txt + /content/drive/MyDrive/Variables/so2_test_20250912_093025.csv\n","\n","üìã Building manifest for NO2 ...\n","  2019: 365 files\n","  2020: 366 files\n","  2021: 365 files\n","  2022: 365 files\n","  2023: 365 files\n","‚úÖ NO2 manifest saved: /content/drive/MyDrive/Variables/no2_manifest.csv\n","   total=1826, trainable=570, date_range=2019-01-01..2023-12-31\n","\n","[NO2 manifest] files=1826  cov_mean=0.281  cov_med=0.310\n"," season counts:\n"," season\n","spring    460\n","summer    460\n","autumn    455\n","winter    451\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","mask    1826\n","Name: count, dtype: int64\n","\n","[NO2 train] files=302  cov_mean=0.456  cov_med=0.453\n"," season counts:\n"," season\n","summer    94\n","spring    78\n","autumn    68\n","winter    62\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","mask    302\n","Name: count, dtype: int64\n","   ‚úÖ train: 302 -> /content/drive/MyDrive/Variables/no2_train_20250912_095739.txt + /content/drive/MyDrive/Variables/no2_train_20250912_095739.csv\n","\n","[NO2 val] files=146  cov_mean=0.456  cov_med=0.454\n"," season counts:\n"," season\n","summer    53\n","spring    37\n","autumn    33\n","winter    23\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","mask    146\n","Name: count, dtype: int64\n","   ‚úÖ val: 146 -> /content/drive/MyDrive/Variables/no2_val_20250912_095739.txt + /content/drive/MyDrive/Variables/no2_val_20250912_095739.csv\n","\n","[NO2 test] files=122  cov_mean=0.462  cov_med=0.468\n"," season counts:\n"," season\n","autumn    40\n","summer    32\n","winter    29\n","spring    21\n","Name: count, dtype: int64\n"," coverage_source breakdown:\n"," coverage_source\n","mask    122\n","Name: count, dtype: int64\n","   ‚úÖ test: 122 -> /content/drive/MyDrive/Variables/no2_test_20250912_095739.txt + /content/drive/MyDrive/Variables/no2_test_20250912_095739.csv\n","\n","üéâ Done.\n"]}]},{"cell_type":"code","source":["import os, json, numpy as np, pandas as pd\n","\n","OUT = \"/content/drive/MyDrive/Variables\"\n","\n","def freeze_scaler(path, name):\n","    with np.load(path, allow_pickle=True) as d:\n","        scalers = d['scalers'].item()\n","        feats = [k for k in d['feature_order'].tolist()]\n","        meta = d['metadata'].item() if 'metadata' in d.files else {}\n","    info = {\n","        \"file\": path, \"num_features\": len(feats), \"features\": feats,\n","        \"scaler_keys\": sorted([k for k,v in scalers.items() if 'mean' in v]),\n","        \"metadata\": meta\n","    }\n","    with open(os.path.join(OUT, f\"{name}_scaler_frozen.json\"), \"w\") as f:\n","        json.dump(info, f, indent=2)\n","    print(f\"‚úÖ freeze: {name} -> {os.path.join(OUT, f'{name}_scaler_frozen.json')}\")\n","\n","def split_summary(csv_path, tag):\n","    df = pd.read_csv(csv_path)\n","    summ = {\n","        \"files\": len(df),\n","        \"cov_mean\": float(df['coverage'].mean()),\n","        \"cov_median\": float(df['coverage'].median()),\n","        \"season_counts\": df['season'].value_counts().to_dict(),\n","        \"source_counts\": (df['coverage_source'].value_counts().to_dict()\n","                          if 'coverage_source' in df.columns else {})\n","    }\n","    out = csv_path.replace(\".csv\", \"_summary.json\")\n","    with open(out, \"w\") as f: json.dump(summ, f, indent=2)\n","    print(f\"‚úÖ summary: {tag} -> {out}\")\n","\n","# 1) ÂÜªÁªì‰∏§‰ªΩscaler\n","freeze_scaler(\"/content/drive/MyDrive/no2_scalers_2019_2021.npz\", \"no2\")\n","freeze_scaler(\"/content/drive/MyDrive/so2_scalers_2019_2021.npz\", \"so2\")\n","\n","# 2) ‰∏∫ÊØè‰∏™split‰øùÂ≠òÊëòË¶ÅÔºàNO2/SO2 ÂêÑËá™ train/val/testÔºâ\n","for p in [\n","    \"so2_train\", \"so2_val\", \"so2_test\",\n","    \"no2_train\", \"no2_val\", \"no2_test\"\n","]:\n","    # Ëã•Êñá‰ª∂ÂêçÂ∏¶Êó∂Èó¥Êà≥ÔºåÂèØÁî®ÊúÄÊñ∞‰∏Ä‰∏™\n","    from glob import glob\n","    cand = sorted(glob(os.path.join(OUT, f\"{p}_*.csv\")))\n","    if cand:\n","        split_summary(cand[-1], p)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vagez9-YcGZy","executionInfo":{"status":"ok","timestamp":1757672411807,"user_tz":-120,"elapsed":776,"user":{"displayName":"ZHANBIN WU","userId":"01710248331646451624"}},"outputId":"ba3f6a15-65e6-4efd-bbd0-1159e0b89ef2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ freeze: no2 -> /content/drive/MyDrive/Variables/no2_scaler_frozen.json\n","‚úÖ freeze: so2 -> /content/drive/MyDrive/Variables/so2_scaler_frozen.json\n","‚úÖ summary: so2_train -> /content/drive/MyDrive/Variables/so2_train_20250912_093025_summary.json\n","‚úÖ summary: so2_val -> /content/drive/MyDrive/Variables/so2_val_20250912_093025_summary.json\n","‚úÖ summary: so2_test -> /content/drive/MyDrive/Variables/so2_test_20250912_093025_summary.json\n","‚úÖ summary: no2_train -> /content/drive/MyDrive/Variables/no2_train_20250912_095739_summary.json\n","‚úÖ summary: no2_val -> /content/drive/MyDrive/Variables/no2_val_20250912_095739_summary.json\n","‚úÖ summary: no2_test -> /content/drive/MyDrive/Variables/no2_test_20250912_095739_summary.json\n"]}]}]}